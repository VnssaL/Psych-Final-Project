{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VnssaL/Psych-Final-Project/blob/main/examples/COLAB/COLAB_YOURDATA_TrainNetwork_VideoAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK255E7YoEIt"
      },
      "source": [
        "# DeepLabCut Toolbox - Colab for standard (single animal) projects!\n",
        "https://github.com/DeepLabCut/DeepLabCut\n",
        "\n",
        "This notebook illustrates how to use the cloud to:\n",
        "- create a training set\n",
        "- train a network\n",
        "- evaluate a network\n",
        "- create simple quality check plots\n",
        "- analyze novel videos!\n",
        "\n",
        "###This notebook assumes you already have a project folder with labeled data! \n",
        "\n",
        "This notebook demonstrates the necessary steps to use DeepLabCut for your own project.\n",
        "\n",
        "This shows the most simple code to do so, but many of the functions have additional features, so please check out the overview & the protocol paper!\n",
        "\n",
        "Nath\\*, Mathis\\* et al.: Using DeepLabCut for markerless pose estimation during behavior across species. Nature Protocols, 2019.\n",
        "\n",
        "\n",
        "Paper: https://www.nature.com/articles/s41596-019-0176-0\n",
        "\n",
        "Pre-print: https://www.biorxiv.org/content/biorxiv/early/2018/11/24/476531.full.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txoddlM8hLKm"
      },
      "source": [
        "## First, go to \"Runtime\" ->\"change runtime type\"->select \"Python3\", and then select \"GPU\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q23BzhA6CXxu",
        "outputId": "4bda2254-d48a-48cc-aeb7-7f3e7a9d9b46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deeplabcut\n",
            "  Downloading deeplabcut-2.3.4-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image>=0.17 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (0.19.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (1.2.2)\n",
            "Requirement already satisfied: statsmodels>=0.11 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (0.13.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (4.65.0)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (1.10.1)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (0.56.4)\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (8.4.0)\n",
            "Requirement already satisfied: pandas!=1.5.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (1.5.3)\n",
            "Collecting dlclibrary\n",
            "  Downloading dlclibrary-0.0.3-py3-none-any.whl (14 kB)\n",
            "Collecting ruamel.yaml>=0.15.0\n",
            "  Downloading ruamel.yaml-0.17.22-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (6.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (1.22.4)\n",
            "Collecting tensorpack>=0.11\n",
            "  Downloading tensorpack-0.11-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.3/296.3 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tables>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (3.8.0)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (1.1.0)\n",
            "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (0.4.0)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (3.7.1)\n",
            "Collecting filterpy>=1.4.4\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (0.4.8)\n",
            "Requirement already satisfied: networkx>=2.6 in /usr/local/lib/python3.10/dist-packages (from deeplabcut) (3.1)\n",
            "Collecting torch<=1.12\n",
            "  Downloading torch-1.12.0-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->deeplabcut) (2.25.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->deeplabcut) (1.16.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->deeplabcut) (4.7.0.72)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug>=0.4.0->deeplabcut) (2.0.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut) (4.39.3)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut) (0.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut) (23.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut) (1.4.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut) (1.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->deeplabcut) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.54->deeplabcut) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.54->deeplabcut) (67.7.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.5.0,>=1.0.1->deeplabcut) (2022.7.1)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (485 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.17->deeplabcut) (1.4.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.17->deeplabcut) (2023.4.12)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->deeplabcut) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->deeplabcut) (3.1.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.11->deeplabcut) (0.5.3)\n",
            "Requirement already satisfied: cython>=0.29.21 in /usr/local/lib/python3.10/dist-packages (from tables>=3.7.0->deeplabcut) (0.29.34)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.10/dist-packages (from tables>=3.7.0->deeplabcut) (2.8.4)\n",
            "Requirement already satisfied: blosc2~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from tables>=3.7.0->deeplabcut) (2.0.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from tables>=3.7.0->deeplabcut) (9.0.0)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=16 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut) (23.2.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut) (2.3.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut) (0.8.10)\n",
            "Collecting msgpack-numpy>=0.4.4.2\n",
            "  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from tensorpack>=0.11->deeplabcut) (1.0.5)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tf-slim>=1.1.0->deeplabcut) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<=1.12->deeplabcut) (4.5.0)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->dlclibrary->deeplabcut) (3.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->dlclibrary->deeplabcut) (2.27.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->dlclibrary->deeplabcut) (2023.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->dlclibrary->deeplabcut) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->dlclibrary->deeplabcut) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->dlclibrary->deeplabcut) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->dlclibrary->deeplabcut) (2.0.12)\n",
            "Building wheels for collected packages: filterpy\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110473 sha256=6c8f51e9d996f6287ec92573306b2cbcb19aaa9cbfe6afc33969eb90a2cb244b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/0c/ea/218f266af4ad626897562199fbbcba521b8497303200186102\n",
            "Successfully built filterpy\n",
            "Installing collected packages: torch, ruamel.yaml.clib, msgpack-numpy, tensorpack, ruamel.yaml, huggingface-hub, filterpy, dlclibrary, deeplabcut\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.12.0 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.12.0 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.12.0 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed deeplabcut-2.3.4 dlclibrary-0.0.3 filterpy-1.4.5 huggingface-hub-0.14.1 msgpack-numpy-0.4.8 ruamel.yaml-0.17.22 ruamel.yaml.clib-0.2.7 tensorpack-0.11 torch-1.12.0\n"
          ]
        }
      ],
      "source": [
        "#(this will take a few minutes to install all the dependences!)\n",
        "!pip install deeplabcut"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25wSj6TlVclR"
      },
      "source": [
        "**(Be sure to click \"RESTART RUNTIME\" if it is displayed above before moving on !)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ-nlTkri4HZ"
      },
      "source": [
        "## Link your Google Drive (with your labeled data, or the demo data):\n",
        "\n",
        "### First, place your project folder into you google drive! \"i.e. move the folder named \"Project-YourName-TheDate\" into google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS4Q4UkR9rgG",
        "outputId": "39551051-10e3-4018-a419-535b8d2e2e09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Now, let's link to your GoogleDrive. Run this cell and follow the authorization instructions:\n",
        "#(We recommend putting a copy of the github repo in your google drive if you are using the demo \"examples\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frnj1RVDyEqs"
      },
      "source": [
        "YOU WILL NEED TO EDIT THE PROJECT PATH **in the config.yaml file** TO BE SET TO YOUR GOOGLE DRIVE LINK!\n",
        "\n",
        "Typically, this will be: /content/drive/My Drive/yourProjectFolderName\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhENAlQnFENJ",
        "outputId": "975e7be4-035d-4356-ac3a-0634556e191c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/group35-Grelby-2023-05-04/videos/']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#Setup your project variables:\n",
        "# PLEASE EDIT THESE:\n",
        "  \n",
        "ProjectFolderName = 'group35-Grelby-2023-05-04'\n",
        "VideoType = 'mp4' \n",
        "\n",
        "#don't edit these:\n",
        "videofile_path = ['/content/drive/My Drive/'+ProjectFolderName+'/videos/'] #Enter the list of videos or folder to analyze.\n",
        "videofile_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K9Ndy1beyfG",
        "outputId": "2738b41a-1544-49a7-c620-511e188b86d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading DLC 2.3.4...\n",
            "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"
          ]
        }
      ],
      "source": [
        "import deeplabcut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "o4orkg9QTHKK",
        "outputId": "980f6122-a3d0-4d47-b490-71abfac793a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3.4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "deeplabcut.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Z7ZlDr3wV4D1",
        "outputId": "f18ffdef-faaa-455b-eb8d-e6b4904d75ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/group35-Grelby-2023-05-04/config.yaml'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#This creates a path variable that links to your google drive copy\n",
        "#No need to edit this, as you set it up before: \n",
        "path_config_file = '/content/drive/My Drive/'+ProjectFolderName+'/config.yaml'\n",
        "path_config_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNi9s1dboEJN"
      },
      "source": [
        "## Create a training dataset:\n",
        "### You must do this step inside of Colab:\n",
        "After running this script the training dataset is created and saved in the project directory under the subdirectory **'training-datasets'**\n",
        "\n",
        "This function also creates new subdirectories under **dlc-models** and appends the project config.yaml file with the correct path to the training and testing pose configuration file. These files hold the parameters for training the network. Such an example file is provided with the toolbox and named as **pose_cfg.yaml**.\n",
        "\n",
        "Now it is the time to start training the network!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMeUwgxPoEJP",
        "scrolled": true,
        "outputId": "e20e89e8-d9e0-4c5d-efb1-a6635e4c404d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/group35-Grelby-2023-05-04/labeled-data/IMG_3180 (1)/CollectedData_Grelby.h5  not found (perhaps not annotated).\n",
            "/content/drive/My Drive/group35-Grelby-2023-05-04/labeled-data/IMG_3192/CollectedData_Grelby.h5  not found (perhaps not annotated).\n",
            "Downloading a ImageNet-pretrained model from http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz....\n",
            "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.95,\n",
              "  1,\n",
              "  (array([132, 309, 341, 196, 246,  60, 155, 261, 141, 214,  37, 134, 113,\n",
              "          348,  12,  59, 293, 140, 206, 199, 176, 268, 124, 344, 175, 313,\n",
              "           78,  15, 286, 102, 170, 303, 334, 225,  65,  76,  90, 173, 179,\n",
              "          399, 100, 322,   6,   1, 297,  54, 374, 255, 158, 233, 247, 144,\n",
              "          294, 171, 280, 318,  52,  10,  26,  45,  74, 272,   4, 354, 191,\n",
              "          371, 145,   5, 154, 310,   7, 260,  68,  20, 107,  14, 363, 304,\n",
              "          361, 329, 336,  64,  55, 106, 300, 229, 122, 373, 395, 325, 380,\n",
              "          253,  56,   8, 190, 146, 135, 390, 264, 364, 250,  63, 312, 159,\n",
              "          283, 340,  81, 349, 153, 295,  96,  89, 296,  21, 365, 216, 259,\n",
              "           49, 238, 343,  92, 223, 234, 232, 377, 142,  22, 252, 350, 168,\n",
              "          150, 393,  66, 240, 218, 101, 311, 194, 326,  17, 164, 186,  30,\n",
              "          114, 263, 103, 358, 245, 235, 116, 330, 120, 289, 112, 215, 136,\n",
              "          275, 126, 198, 299, 281, 133,  33, 378, 162,  34, 231,  97,  85,\n",
              "           61, 167, 282, 200, 391, 230, 287, 108,  46, 320, 396, 224,  73,\n",
              "          137, 381, 220, 210,  29, 181, 360, 271,  51, 328, 352,  27,   2,\n",
              "          217, 156, 212, 376, 221, 138, 236, 219, 274, 278, 307, 239,  35,\n",
              "          204, 392,  67,  24, 332,  44, 241, 129,  93, 111, 166, 389, 383,\n",
              "          342,  40,  18, 284,  79, 249, 394,  71,  13, 367, 213, 385, 388,\n",
              "          228, 160, 104, 161,  83, 189, 397, 118, 254, 188, 208, 375, 110,\n",
              "          149, 157, 152,  16, 269,  75, 109, 327, 205, 315, 139, 237, 319,\n",
              "          248, 308,  19, 226, 306,   3, 276, 125,  77, 184, 301, 379, 346,\n",
              "          182, 356,  80, 258,  11, 298,  86, 266,  36, 382,  58,  41, 270,\n",
              "           50, 209, 317, 316, 331, 123, 222,  62, 302, 130, 187,  23,  43,\n",
              "            0, 201, 339,  98, 387, 178, 256,  94, 369,  95, 351, 169,  69,\n",
              "          305,  48, 207, 279, 227, 148, 143, 180, 131, 357, 398, 262, 324,\n",
              "          203,  84, 121, 345, 366,  91,  82, 267, 119, 291,  57, 321, 257,\n",
              "          355,  42, 105, 368, 273, 353,  38,  53, 347, 128, 290,  28, 183,\n",
              "          163, 151, 244, 202,  31,  32, 127, 185, 372, 288, 362, 147, 285,\n",
              "          370, 177,  99, 338, 335, 197, 243, 115, 265,  72, 333,  25, 165,\n",
              "          337, 384, 174]),\n",
              "   array([386,  39, 193, 314,  88,  70,  87, 292, 242, 277, 211,   9, 359,\n",
              "          195, 251, 323, 192, 117,  47, 172])))]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Note: if you are using the demo data (i.e. examples/Reaching-Mackenzie-2018-08-30/), first delete the folder called dlc-models! \n",
        "#Then, run this cell. There are many more functions you can set here, including which netowkr to use!\n",
        "#check the docstring for full options you can do!\n",
        "deeplabcut.create_training_dataset(path_config_file, net_type='resnet_50', augmenter_type='imgaug')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4FczXGDoEJU"
      },
      "source": [
        "## Start training:\n",
        "This function trains the network for a specific shuffle of the training dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_pOvDq_2oEJW",
        "outputId": "3a3ea112-ccfc-49de-88bf-d678cc621602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Config:\n",
            "{'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7]],\n",
            " 'all_joints_names': ['Bell',\n",
            "                      'tentacles',\n",
            "                      'muscles',\n",
            "                      'eyespot',\n",
            "                      'oral arms',\n",
            "                      'canals',\n",
            "                      'gut',\n",
            "                      'gonads'],\n",
            " 'alpha_r': 0.02,\n",
            " 'apply_prob': 0.5,\n",
            " 'batch_size': 1,\n",
            " 'contrast': {'clahe': True,\n",
            "              'claheratio': 0.1,\n",
            "              'histeq': True,\n",
            "              'histeqratio': 0.1},\n",
            " 'convolution': {'edge': False,\n",
            "                 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]},\n",
            "                 'embossratio': 0.1,\n",
            "                 'sharpen': False,\n",
            "                 'sharpenratio': 0.3},\n",
            " 'crop_pad': 0,\n",
            " 'cropratio': 0.4,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_group '\n",
            "            '35May4/group 35_Grelby95shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'decay_steps': 30000,\n",
            " 'deterministic': False,\n",
            " 'display_iters': 1000,\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'init_weights': '/usr/local/lib/python3.10/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 0.05,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'lr_init': 0.0005,\n",
            " 'max_input_size': 1500,\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_group '\n",
            "                '35May4/Documentation_data-group 35_95shuffle1.pickle',\n",
            " 'min_input_size': 64,\n",
            " 'mirror': False,\n",
            " 'multi_stage': False,\n",
            " 'multi_step': [[0.005, 10000],\n",
            "                [0.02, 430000],\n",
            "                [0.002, 730000],\n",
            "                [0.001, 1030000]],\n",
            " 'net_type': 'resnet_50',\n",
            " 'num_joints': 8,\n",
            " 'optimizer': 'sgd',\n",
            " 'pairwise_huber_loss': False,\n",
            " 'pairwise_predict': False,\n",
            " 'partaffinityfield_predict': False,\n",
            " 'pos_dist_thresh': 17,\n",
            " 'project_path': '/content/drive/My Drive/group35-Grelby-2023-05-04',\n",
            " 'regularize': False,\n",
            " 'rotation': 25,\n",
            " 'rotratio': 0.4,\n",
            " 'save_iters': 50000,\n",
            " 'scale_jitter_lo': 0.5,\n",
            " 'scale_jitter_up': 1.25,\n",
            " 'scoremap_dir': 'test',\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/My '\n",
            "                    'Drive/group35-Grelby-2023-05-04/dlc-models/iteration-0/group '\n",
            "                    '35May4-trainset95shuffle1/train/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting single-animal trainer\n",
            "Batch Size is 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ImageNet-pretrained resnet_50\n",
            "Display_iters overwritten as 10\n",
            "Save_iters overwritten as 500\n",
            "Training parameter:\n",
            "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/content/drive/My Drive/group35-Grelby-2023-05-04/dlc-models/iteration-0/group 35May4-trainset95shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7]], 'all_joints_names': ['Bell', 'tentacles', 'muscles', 'eyespot', 'oral arms', 'canals', 'gut', 'gonads'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'contrast': {'clahe': True, 'claheratio': 0.1, 'histeq': True, 'histeqratio': 0.1, 'gamma': False, 'sigmoid': False, 'log': False, 'linear': False}, 'convolution': {'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'strength': [0.5, 1.5]}, 'embossratio': 0.1, 'sharpen': False, 'sharpenratio': 0.3}, 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_group 35May4/group 35_Grelby95shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'init_weights': '/usr/local/lib/python3.10/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_group 35May4/Documentation_data-group 35_95shuffle1.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 8, 'pos_dist_thresh': 17, 'project_path': '/content/drive/My Drive/group35-Grelby-2023-05-04', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n",
            "Starting training....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "iteration: 18920 loss: 0.0148 lr: 0.02\n",
            "iteration: 18930 loss: 0.0191 lr: 0.02\n",
            "iteration: 18940 loss: 0.0207 lr: 0.02\n",
            "iteration: 18950 loss: 0.0145 lr: 0.02\n",
            "iteration: 18960 loss: 0.0258 lr: 0.02\n",
            "iteration: 18970 loss: 0.0138 lr: 0.02\n",
            "iteration: 18980 loss: 0.0183 lr: 0.02\n",
            "iteration: 18990 loss: 0.0129 lr: 0.02\n",
            "iteration: 19000 loss: 0.0140 lr: 0.02\n",
            "iteration: 19010 loss: 0.0245 lr: 0.02\n",
            "iteration: 19020 loss: 0.0140 lr: 0.02\n",
            "iteration: 19030 loss: 0.0319 lr: 0.02\n",
            "iteration: 19040 loss: 0.0147 lr: 0.02\n",
            "iteration: 19050 loss: 0.0152 lr: 0.02\n",
            "iteration: 19060 loss: 0.0151 lr: 0.02\n",
            "iteration: 19070 loss: 0.0150 lr: 0.02\n",
            "iteration: 19080 loss: 0.0107 lr: 0.02\n",
            "iteration: 19090 loss: 0.0276 lr: 0.02\n",
            "iteration: 19100 loss: 0.0195 lr: 0.02\n",
            "iteration: 19110 loss: 0.0228 lr: 0.02\n",
            "iteration: 19120 loss: 0.0357 lr: 0.02\n",
            "iteration: 19130 loss: 0.0371 lr: 0.02\n",
            "iteration: 19140 loss: 0.0153 lr: 0.02\n",
            "iteration: 19150 loss: 0.0165 lr: 0.02\n",
            "iteration: 19160 loss: 0.0171 lr: 0.02\n",
            "iteration: 19170 loss: 0.0123 lr: 0.02\n",
            "iteration: 19180 loss: 0.0187 lr: 0.02\n",
            "iteration: 19190 loss: 0.0133 lr: 0.02\n",
            "iteration: 19200 loss: 0.0227 lr: 0.02\n",
            "iteration: 19210 loss: 0.0228 lr: 0.02\n",
            "iteration: 19220 loss: 0.0203 lr: 0.02\n",
            "iteration: 19230 loss: 0.0147 lr: 0.02\n",
            "iteration: 19240 loss: 0.0208 lr: 0.02\n",
            "iteration: 19250 loss: 0.0143 lr: 0.02\n",
            "iteration: 19260 loss: 0.0144 lr: 0.02\n",
            "iteration: 19270 loss: 0.0169 lr: 0.02\n",
            "iteration: 19280 loss: 0.0170 lr: 0.02\n",
            "iteration: 19290 loss: 0.0208 lr: 0.02\n",
            "iteration: 19300 loss: 0.0154 lr: 0.02\n",
            "iteration: 19310 loss: 0.0228 lr: 0.02\n",
            "iteration: 19320 loss: 0.0271 lr: 0.02\n",
            "iteration: 19330 loss: 0.0226 lr: 0.02\n",
            "iteration: 19340 loss: 0.0313 lr: 0.02\n",
            "iteration: 19350 loss: 0.0195 lr: 0.02\n",
            "iteration: 19360 loss: 0.0148 lr: 0.02\n",
            "iteration: 19370 loss: 0.0197 lr: 0.02\n",
            "iteration: 19380 loss: 0.0226 lr: 0.02\n",
            "iteration: 19390 loss: 0.0131 lr: 0.02\n",
            "iteration: 19400 loss: 0.0208 lr: 0.02\n",
            "iteration: 19410 loss: 0.0114 lr: 0.02\n",
            "iteration: 19420 loss: 0.0131 lr: 0.02\n",
            "iteration: 19430 loss: 0.0130 lr: 0.02\n",
            "iteration: 19440 loss: 0.0197 lr: 0.02\n",
            "iteration: 19450 loss: 0.0309 lr: 0.02\n",
            "iteration: 19460 loss: 0.0201 lr: 0.02\n",
            "iteration: 19470 loss: 0.0208 lr: 0.02\n",
            "iteration: 19480 loss: 0.0174 lr: 0.02\n",
            "iteration: 19490 loss: 0.0150 lr: 0.02\n",
            "iteration: 19500 loss: 0.0176 lr: 0.02\n",
            "iteration: 19510 loss: 0.0098 lr: 0.02\n",
            "iteration: 19520 loss: 0.0126 lr: 0.02\n",
            "iteration: 19530 loss: 0.0164 lr: 0.02\n",
            "iteration: 19540 loss: 0.0123 lr: 0.02\n",
            "iteration: 19550 loss: 0.0160 lr: 0.02\n",
            "iteration: 19560 loss: 0.0196 lr: 0.02\n",
            "iteration: 19570 loss: 0.0202 lr: 0.02\n",
            "iteration: 19580 loss: 0.0153 lr: 0.02\n",
            "iteration: 19590 loss: 0.0251 lr: 0.02\n",
            "iteration: 19600 loss: 0.0122 lr: 0.02\n",
            "iteration: 19610 loss: 0.0204 lr: 0.02\n",
            "iteration: 19620 loss: 0.0201 lr: 0.02\n",
            "iteration: 19630 loss: 0.0145 lr: 0.02\n",
            "iteration: 19640 loss: 0.0156 lr: 0.02\n",
            "iteration: 19650 loss: 0.0159 lr: 0.02\n",
            "iteration: 19660 loss: 0.0123 lr: 0.02\n",
            "iteration: 19670 loss: 0.0150 lr: 0.02\n",
            "iteration: 19680 loss: 0.0221 lr: 0.02\n",
            "iteration: 19690 loss: 0.0297 lr: 0.02\n",
            "iteration: 19700 loss: 0.0261 lr: 0.02\n",
            "iteration: 19710 loss: 0.0150 lr: 0.02\n",
            "iteration: 19720 loss: 0.0304 lr: 0.02\n",
            "iteration: 19730 loss: 0.0133 lr: 0.02\n",
            "iteration: 19740 loss: 0.0212 lr: 0.02\n",
            "iteration: 19750 loss: 0.0168 lr: 0.02\n",
            "iteration: 19760 loss: 0.0181 lr: 0.02\n",
            "iteration: 19770 loss: 0.0170 lr: 0.02\n",
            "iteration: 19780 loss: 0.0174 lr: 0.02\n",
            "iteration: 19790 loss: 0.0314 lr: 0.02\n",
            "iteration: 19800 loss: 0.0220 lr: 0.02\n",
            "iteration: 19810 loss: 0.0209 lr: 0.02\n",
            "iteration: 19820 loss: 0.0163 lr: 0.02\n",
            "iteration: 19830 loss: 0.0200 lr: 0.02\n",
            "iteration: 19840 loss: 0.0155 lr: 0.02\n",
            "iteration: 19850 loss: 0.0263 lr: 0.02\n",
            "iteration: 19860 loss: 0.0148 lr: 0.02\n",
            "iteration: 19870 loss: 0.0132 lr: 0.02\n",
            "iteration: 19880 loss: 0.0189 lr: 0.02\n",
            "iteration: 19890 loss: 0.0162 lr: 0.02\n",
            "iteration: 19900 loss: 0.0182 lr: 0.02\n",
            "iteration: 19910 loss: 0.0341 lr: 0.02\n",
            "iteration: 19920 loss: 0.0175 lr: 0.02\n",
            "iteration: 19930 loss: 0.0141 lr: 0.02\n",
            "iteration: 19940 loss: 0.0207 lr: 0.02\n",
            "iteration: 19950 loss: 0.0210 lr: 0.02\n",
            "iteration: 19960 loss: 0.0152 lr: 0.02\n",
            "iteration: 19970 loss: 0.0118 lr: 0.02\n",
            "iteration: 19980 loss: 0.0159 lr: 0.02\n",
            "iteration: 19990 loss: 0.0297 lr: 0.02\n",
            "iteration: 20000 loss: 0.0200 lr: 0.02\n",
            "iteration: 20010 loss: 0.0181 lr: 0.02\n",
            "iteration: 20020 loss: 0.0149 lr: 0.02\n",
            "iteration: 20030 loss: 0.0316 lr: 0.02\n",
            "iteration: 20040 loss: 0.0217 lr: 0.02\n",
            "iteration: 20050 loss: 0.0282 lr: 0.02\n",
            "iteration: 20060 loss: 0.0163 lr: 0.02\n",
            "iteration: 20070 loss: 0.0183 lr: 0.02\n",
            "iteration: 20080 loss: 0.0147 lr: 0.02\n",
            "iteration: 20090 loss: 0.0223 lr: 0.02\n",
            "iteration: 20100 loss: 0.0140 lr: 0.02\n",
            "iteration: 20110 loss: 0.0175 lr: 0.02\n",
            "iteration: 20120 loss: 0.0153 lr: 0.02\n",
            "iteration: 20130 loss: 0.0183 lr: 0.02\n",
            "iteration: 20140 loss: 0.0149 lr: 0.02\n",
            "iteration: 20150 loss: 0.0158 lr: 0.02\n",
            "iteration: 20160 loss: 0.0222 lr: 0.02\n",
            "iteration: 20170 loss: 0.0148 lr: 0.02\n",
            "iteration: 20180 loss: 0.0206 lr: 0.02\n",
            "iteration: 20190 loss: 0.0214 lr: 0.02\n",
            "iteration: 20200 loss: 0.0179 lr: 0.02\n",
            "iteration: 20210 loss: 0.0212 lr: 0.02\n",
            "iteration: 20220 loss: 0.0134 lr: 0.02\n",
            "iteration: 20230 loss: 0.0173 lr: 0.02\n",
            "iteration: 20240 loss: 0.0105 lr: 0.02\n",
            "iteration: 20250 loss: 0.0228 lr: 0.02\n",
            "iteration: 20260 loss: 0.0187 lr: 0.02\n",
            "iteration: 20270 loss: 0.0215 lr: 0.02\n",
            "iteration: 20280 loss: 0.0146 lr: 0.02\n",
            "iteration: 20290 loss: 0.0170 lr: 0.02\n",
            "iteration: 20300 loss: 0.0126 lr: 0.02\n",
            "iteration: 20310 loss: 0.0156 lr: 0.02\n",
            "iteration: 20320 loss: 0.0187 lr: 0.02\n",
            "iteration: 20330 loss: 0.0206 lr: 0.02\n",
            "iteration: 20340 loss: 0.0143 lr: 0.02\n",
            "iteration: 20350 loss: 0.0216 lr: 0.02\n",
            "iteration: 20360 loss: 0.0137 lr: 0.02\n",
            "iteration: 20370 loss: 0.0224 lr: 0.02\n",
            "iteration: 20380 loss: 0.0319 lr: 0.02\n",
            "iteration: 20390 loss: 0.0187 lr: 0.02\n",
            "iteration: 20400 loss: 0.0185 lr: 0.02\n",
            "iteration: 20410 loss: 0.0158 lr: 0.02\n",
            "iteration: 20420 loss: 0.0287 lr: 0.02\n",
            "iteration: 20430 loss: 0.0272 lr: 0.02\n",
            "iteration: 20440 loss: 0.0165 lr: 0.02\n",
            "iteration: 20450 loss: 0.0203 lr: 0.02\n",
            "iteration: 20460 loss: 0.0217 lr: 0.02\n",
            "iteration: 20470 loss: 0.0155 lr: 0.02\n",
            "iteration: 20480 loss: 0.0226 lr: 0.02\n",
            "iteration: 20490 loss: 0.0116 lr: 0.02\n",
            "iteration: 20500 loss: 0.0165 lr: 0.02\n",
            "iteration: 20510 loss: 0.0110 lr: 0.02\n",
            "iteration: 20520 loss: 0.0270 lr: 0.02\n",
            "iteration: 20530 loss: 0.0154 lr: 0.02\n",
            "iteration: 20540 loss: 0.0184 lr: 0.02\n",
            "iteration: 20550 loss: 0.0150 lr: 0.02\n",
            "iteration: 20560 loss: 0.0141 lr: 0.02\n",
            "iteration: 20570 loss: 0.0183 lr: 0.02\n",
            "iteration: 20580 loss: 0.0098 lr: 0.02\n",
            "iteration: 20590 loss: 0.0173 lr: 0.02\n",
            "iteration: 20600 loss: 0.0141 lr: 0.02\n",
            "iteration: 20610 loss: 0.0211 lr: 0.02\n",
            "iteration: 20620 loss: 0.0230 lr: 0.02\n",
            "iteration: 20630 loss: 0.0277 lr: 0.02\n",
            "iteration: 20640 loss: 0.0203 lr: 0.02\n",
            "iteration: 20650 loss: 0.0243 lr: 0.02\n",
            "iteration: 20660 loss: 0.0189 lr: 0.02\n",
            "iteration: 20670 loss: 0.0261 lr: 0.02\n",
            "iteration: 20680 loss: 0.0251 lr: 0.02\n",
            "iteration: 20690 loss: 0.0422 lr: 0.02\n",
            "iteration: 20700 loss: 0.0285 lr: 0.02\n",
            "iteration: 20710 loss: 0.0172 lr: 0.02\n",
            "iteration: 20720 loss: 0.0210 lr: 0.02\n",
            "iteration: 20730 loss: 0.0150 lr: 0.02\n",
            "iteration: 20740 loss: 0.0139 lr: 0.02\n",
            "iteration: 20750 loss: 0.0155 lr: 0.02\n",
            "iteration: 20760 loss: 0.0185 lr: 0.02\n",
            "iteration: 20770 loss: 0.0230 lr: 0.02\n",
            "iteration: 20780 loss: 0.0159 lr: 0.02\n",
            "iteration: 20790 loss: 0.0345 lr: 0.02\n",
            "iteration: 20800 loss: 0.0192 lr: 0.02\n",
            "iteration: 20810 loss: 0.0177 lr: 0.02\n",
            "iteration: 20820 loss: 0.0227 lr: 0.02\n",
            "iteration: 20830 loss: 0.0254 lr: 0.02\n",
            "iteration: 20840 loss: 0.0239 lr: 0.02\n",
            "iteration: 20850 loss: 0.0235 lr: 0.02\n",
            "iteration: 20860 loss: 0.0151 lr: 0.02\n",
            "iteration: 20870 loss: 0.0270 lr: 0.02\n",
            "iteration: 20880 loss: 0.0188 lr: 0.02\n",
            "iteration: 20890 loss: 0.0215 lr: 0.02\n",
            "iteration: 20900 loss: 0.0146 lr: 0.02\n",
            "iteration: 20910 loss: 0.0167 lr: 0.02\n",
            "iteration: 20920 loss: 0.0226 lr: 0.02\n",
            "iteration: 20930 loss: 0.0191 lr: 0.02\n",
            "iteration: 20940 loss: 0.0180 lr: 0.02\n",
            "iteration: 20950 loss: 0.0135 lr: 0.02\n",
            "iteration: 20960 loss: 0.0177 lr: 0.02\n",
            "iteration: 20970 loss: 0.0354 lr: 0.02\n",
            "iteration: 20980 loss: 0.0231 lr: 0.02\n",
            "iteration: 20990 loss: 0.0114 lr: 0.02\n",
            "iteration: 21000 loss: 0.0192 lr: 0.02\n",
            "iteration: 21010 loss: 0.0170 lr: 0.02\n",
            "iteration: 21020 loss: 0.0169 lr: 0.02\n",
            "iteration: 21030 loss: 0.0182 lr: 0.02\n",
            "iteration: 21040 loss: 0.0135 lr: 0.02\n",
            "iteration: 21050 loss: 0.0184 lr: 0.02\n",
            "iteration: 21060 loss: 0.0195 lr: 0.02\n",
            "iteration: 21070 loss: 0.0183 lr: 0.02\n",
            "iteration: 21080 loss: 0.0138 lr: 0.02\n",
            "iteration: 21090 loss: 0.0208 lr: 0.02\n",
            "iteration: 21100 loss: 0.0139 lr: 0.02\n",
            "iteration: 21110 loss: 0.0172 lr: 0.02\n",
            "iteration: 21120 loss: 0.0143 lr: 0.02\n",
            "iteration: 21130 loss: 0.0140 lr: 0.02\n",
            "iteration: 21140 loss: 0.0186 lr: 0.02\n",
            "iteration: 21150 loss: 0.0189 lr: 0.02\n",
            "iteration: 21160 loss: 0.0214 lr: 0.02\n",
            "iteration: 21170 loss: 0.0225 lr: 0.02\n",
            "iteration: 21180 loss: 0.0291 lr: 0.02\n",
            "iteration: 21190 loss: 0.0218 lr: 0.02\n",
            "iteration: 21200 loss: 0.0219 lr: 0.02\n",
            "iteration: 21210 loss: 0.0186 lr: 0.02\n",
            "iteration: 21220 loss: 0.0135 lr: 0.02\n",
            "iteration: 21230 loss: 0.0218 lr: 0.02\n",
            "iteration: 21240 loss: 0.0152 lr: 0.02\n",
            "iteration: 21250 loss: 0.0213 lr: 0.02\n",
            "iteration: 21260 loss: 0.0148 lr: 0.02\n",
            "iteration: 21270 loss: 0.0167 lr: 0.02\n",
            "iteration: 21280 loss: 0.0116 lr: 0.02\n",
            "iteration: 21290 loss: 0.0222 lr: 0.02\n",
            "iteration: 21300 loss: 0.0167 lr: 0.02\n",
            "iteration: 21310 loss: 0.0358 lr: 0.02\n",
            "iteration: 21320 loss: 0.0223 lr: 0.02\n",
            "iteration: 21330 loss: 0.0190 lr: 0.02\n",
            "iteration: 21340 loss: 0.0182 lr: 0.02\n",
            "iteration: 21350 loss: 0.0272 lr: 0.02\n",
            "iteration: 21360 loss: 0.0156 lr: 0.02\n",
            "iteration: 21370 loss: 0.0306 lr: 0.02\n",
            "iteration: 21380 loss: 0.0233 lr: 0.02\n",
            "iteration: 21390 loss: 0.0189 lr: 0.02\n",
            "iteration: 21400 loss: 0.0188 lr: 0.02\n",
            "iteration: 21410 loss: 0.0257 lr: 0.02\n",
            "iteration: 21420 loss: 0.0198 lr: 0.02\n",
            "iteration: 21430 loss: 0.0155 lr: 0.02\n",
            "iteration: 21440 loss: 0.0229 lr: 0.02\n",
            "iteration: 21450 loss: 0.0200 lr: 0.02\n",
            "iteration: 21460 loss: 0.0133 lr: 0.02\n",
            "iteration: 21470 loss: 0.0199 lr: 0.02\n",
            "iteration: 21480 loss: 0.0147 lr: 0.02\n",
            "iteration: 21490 loss: 0.0137 lr: 0.02\n",
            "iteration: 21500 loss: 0.0125 lr: 0.02\n",
            "iteration: 21510 loss: 0.0123 lr: 0.02\n",
            "iteration: 21520 loss: 0.0151 lr: 0.02\n",
            "iteration: 21530 loss: 0.0214 lr: 0.02\n",
            "iteration: 21540 loss: 0.0287 lr: 0.02\n",
            "iteration: 21550 loss: 0.0196 lr: 0.02\n",
            "iteration: 21560 loss: 0.0172 lr: 0.02\n",
            "iteration: 21570 loss: 0.0180 lr: 0.02\n",
            "iteration: 21580 loss: 0.0120 lr: 0.02\n",
            "iteration: 21590 loss: 0.0427 lr: 0.02\n",
            "iteration: 21600 loss: 0.0123 lr: 0.02\n",
            "iteration: 21610 loss: 0.0286 lr: 0.02\n",
            "iteration: 21620 loss: 0.0162 lr: 0.02\n",
            "iteration: 21630 loss: 0.0234 lr: 0.02\n",
            "iteration: 21640 loss: 0.0123 lr: 0.02\n",
            "iteration: 21650 loss: 0.0195 lr: 0.02\n",
            "iteration: 21660 loss: 0.0196 lr: 0.02\n",
            "iteration: 21670 loss: 0.0286 lr: 0.02\n",
            "iteration: 21680 loss: 0.0188 lr: 0.02\n",
            "iteration: 21690 loss: 0.0138 lr: 0.02\n",
            "iteration: 21700 loss: 0.0172 lr: 0.02\n",
            "iteration: 21710 loss: 0.0120 lr: 0.02\n",
            "iteration: 21720 loss: 0.0208 lr: 0.02\n",
            "iteration: 21730 loss: 0.0204 lr: 0.02\n",
            "iteration: 21740 loss: 0.0126 lr: 0.02\n",
            "iteration: 21750 loss: 0.0254 lr: 0.02\n",
            "iteration: 21760 loss: 0.0194 lr: 0.02\n",
            "iteration: 21770 loss: 0.0240 lr: 0.02\n",
            "iteration: 21780 loss: 0.0150 lr: 0.02\n",
            "iteration: 21790 loss: 0.0121 lr: 0.02\n",
            "iteration: 21800 loss: 0.0113 lr: 0.02\n",
            "iteration: 21810 loss: 0.0251 lr: 0.02\n",
            "iteration: 21820 loss: 0.0189 lr: 0.02\n",
            "iteration: 21830 loss: 0.0153 lr: 0.02\n",
            "iteration: 21840 loss: 0.0143 lr: 0.02\n",
            "iteration: 21850 loss: 0.0169 lr: 0.02\n",
            "iteration: 21860 loss: 0.0157 lr: 0.02\n",
            "iteration: 21870 loss: 0.0163 lr: 0.02\n",
            "iteration: 21880 loss: 0.0239 lr: 0.02\n",
            "iteration: 21890 loss: 0.0272 lr: 0.02\n",
            "iteration: 21900 loss: 0.0132 lr: 0.02\n",
            "iteration: 21910 loss: 0.0187 lr: 0.02\n",
            "iteration: 21920 loss: 0.0138 lr: 0.02\n",
            "iteration: 21930 loss: 0.0207 lr: 0.02\n",
            "iteration: 21940 loss: 0.0172 lr: 0.02\n",
            "iteration: 21950 loss: 0.0130 lr: 0.02\n",
            "iteration: 21960 loss: 0.0152 lr: 0.02\n",
            "iteration: 21970 loss: 0.0126 lr: 0.02\n",
            "iteration: 21980 loss: 0.0300 lr: 0.02\n",
            "iteration: 21990 loss: 0.0172 lr: 0.02\n",
            "iteration: 22000 loss: 0.0203 lr: 0.02\n",
            "iteration: 22010 loss: 0.0121 lr: 0.02\n",
            "iteration: 22020 loss: 0.0168 lr: 0.02\n",
            "iteration: 22030 loss: 0.0264 lr: 0.02\n",
            "iteration: 22040 loss: 0.0171 lr: 0.02\n",
            "iteration: 22050 loss: 0.0158 lr: 0.02\n",
            "iteration: 22060 loss: 0.0173 lr: 0.02\n",
            "iteration: 22070 loss: 0.0137 lr: 0.02\n",
            "iteration: 22080 loss: 0.0165 lr: 0.02\n",
            "iteration: 22090 loss: 0.0281 lr: 0.02\n",
            "iteration: 22100 loss: 0.0238 lr: 0.02\n",
            "iteration: 22110 loss: 0.0223 lr: 0.02\n",
            "iteration: 22120 loss: 0.0170 lr: 0.02\n",
            "iteration: 22130 loss: 0.0237 lr: 0.02\n",
            "iteration: 22140 loss: 0.0184 lr: 0.02\n",
            "iteration: 22150 loss: 0.0137 lr: 0.02\n",
            "iteration: 22160 loss: 0.0193 lr: 0.02\n",
            "iteration: 22170 loss: 0.0152 lr: 0.02\n",
            "iteration: 22180 loss: 0.0233 lr: 0.02\n",
            "iteration: 22190 loss: 0.0149 lr: 0.02\n",
            "iteration: 22200 loss: 0.0171 lr: 0.02\n",
            "iteration: 22210 loss: 0.0140 lr: 0.02\n",
            "iteration: 22220 loss: 0.0189 lr: 0.02\n",
            "iteration: 22230 loss: 0.0129 lr: 0.02\n",
            "iteration: 22240 loss: 0.0253 lr: 0.02\n",
            "iteration: 22250 loss: 0.0150 lr: 0.02\n",
            "iteration: 22260 loss: 0.0228 lr: 0.02\n",
            "iteration: 22270 loss: 0.0304 lr: 0.02\n",
            "iteration: 22280 loss: 0.0154 lr: 0.02\n",
            "iteration: 22290 loss: 0.0126 lr: 0.02\n",
            "iteration: 22300 loss: 0.0162 lr: 0.02\n",
            "iteration: 22310 loss: 0.0158 lr: 0.02\n",
            "iteration: 22320 loss: 0.0214 lr: 0.02\n",
            "iteration: 22330 loss: 0.0132 lr: 0.02\n",
            "iteration: 22340 loss: 0.0136 lr: 0.02\n",
            "iteration: 22350 loss: 0.0210 lr: 0.02\n",
            "iteration: 22360 loss: 0.0214 lr: 0.02\n",
            "iteration: 22370 loss: 0.0183 lr: 0.02\n",
            "iteration: 22380 loss: 0.0165 lr: 0.02\n",
            "iteration: 22390 loss: 0.0135 lr: 0.02\n",
            "iteration: 22400 loss: 0.0198 lr: 0.02\n",
            "iteration: 22410 loss: 0.0193 lr: 0.02\n",
            "iteration: 22420 loss: 0.0282 lr: 0.02\n",
            "iteration: 22430 loss: 0.0165 lr: 0.02\n",
            "iteration: 22440 loss: 0.0189 lr: 0.02\n",
            "iteration: 22450 loss: 0.0222 lr: 0.02\n",
            "iteration: 22460 loss: 0.0129 lr: 0.02\n",
            "iteration: 22470 loss: 0.0129 lr: 0.02\n",
            "iteration: 22480 loss: 0.0161 lr: 0.02\n",
            "iteration: 22490 loss: 0.0211 lr: 0.02\n",
            "iteration: 22500 loss: 0.0138 lr: 0.02\n",
            "iteration: 22510 loss: 0.0378 lr: 0.02\n",
            "iteration: 22520 loss: 0.0287 lr: 0.02\n",
            "iteration: 22530 loss: 0.0147 lr: 0.02\n",
            "iteration: 22540 loss: 0.0156 lr: 0.02\n",
            "iteration: 22550 loss: 0.0243 lr: 0.02\n",
            "iteration: 22560 loss: 0.0110 lr: 0.02\n",
            "iteration: 22570 loss: 0.0239 lr: 0.02\n",
            "iteration: 22580 loss: 0.0210 lr: 0.02\n",
            "iteration: 22590 loss: 0.0235 lr: 0.02\n",
            "iteration: 22600 loss: 0.0311 lr: 0.02\n",
            "iteration: 22610 loss: 0.0140 lr: 0.02\n",
            "iteration: 22620 loss: 0.0168 lr: 0.02\n",
            "iteration: 22630 loss: 0.0180 lr: 0.02\n",
            "iteration: 22640 loss: 0.0216 lr: 0.02\n",
            "iteration: 22650 loss: 0.0155 lr: 0.02\n",
            "iteration: 22660 loss: 0.0205 lr: 0.02\n",
            "iteration: 22670 loss: 0.0198 lr: 0.02\n",
            "iteration: 22680 loss: 0.0173 lr: 0.02\n",
            "iteration: 22690 loss: 0.0149 lr: 0.02\n",
            "iteration: 22700 loss: 0.0175 lr: 0.02\n",
            "iteration: 22710 loss: 0.0166 lr: 0.02\n",
            "iteration: 22720 loss: 0.0164 lr: 0.02\n",
            "iteration: 22730 loss: 0.0286 lr: 0.02\n",
            "iteration: 22740 loss: 0.0173 lr: 0.02\n",
            "iteration: 22750 loss: 0.0185 lr: 0.02\n",
            "iteration: 22760 loss: 0.0186 lr: 0.02\n",
            "iteration: 22770 loss: 0.0166 lr: 0.02\n",
            "iteration: 22780 loss: 0.0104 lr: 0.02\n",
            "iteration: 22790 loss: 0.0140 lr: 0.02\n",
            "iteration: 22800 loss: 0.0157 lr: 0.02\n",
            "iteration: 22810 loss: 0.0172 lr: 0.02\n",
            "iteration: 22820 loss: 0.0174 lr: 0.02\n",
            "iteration: 22830 loss: 0.0231 lr: 0.02\n",
            "iteration: 22840 loss: 0.0147 lr: 0.02\n",
            "iteration: 22850 loss: 0.0196 lr: 0.02\n",
            "iteration: 22860 loss: 0.0155 lr: 0.02\n",
            "iteration: 22870 loss: 0.0136 lr: 0.02\n",
            "iteration: 22880 loss: 0.0189 lr: 0.02\n",
            "iteration: 22890 loss: 0.0163 lr: 0.02\n",
            "iteration: 22900 loss: 0.0160 lr: 0.02\n",
            "iteration: 22910 loss: 0.0224 lr: 0.02\n",
            "iteration: 22920 loss: 0.0180 lr: 0.02\n",
            "iteration: 22930 loss: 0.0167 lr: 0.02\n",
            "iteration: 22940 loss: 0.0214 lr: 0.02\n",
            "iteration: 22950 loss: 0.0205 lr: 0.02\n",
            "iteration: 22960 loss: 0.0191 lr: 0.02\n",
            "iteration: 22970 loss: 0.0247 lr: 0.02\n",
            "iteration: 22980 loss: 0.0168 lr: 0.02\n",
            "iteration: 22990 loss: 0.0192 lr: 0.02\n",
            "iteration: 23000 loss: 0.0241 lr: 0.02\n",
            "iteration: 23010 loss: 0.0274 lr: 0.02\n",
            "iteration: 23020 loss: 0.0160 lr: 0.02\n",
            "iteration: 23030 loss: 0.0254 lr: 0.02\n",
            "iteration: 23040 loss: 0.0129 lr: 0.02\n",
            "iteration: 23050 loss: 0.0193 lr: 0.02\n",
            "iteration: 23060 loss: 0.0199 lr: 0.02\n",
            "iteration: 23070 loss: 0.0138 lr: 0.02\n",
            "iteration: 23080 loss: 0.0238 lr: 0.02\n",
            "iteration: 23090 loss: 0.0168 lr: 0.02\n",
            "iteration: 23100 loss: 0.0279 lr: 0.02\n",
            "iteration: 23110 loss: 0.0185 lr: 0.02\n",
            "iteration: 23120 loss: 0.0151 lr: 0.02\n",
            "iteration: 23130 loss: 0.0253 lr: 0.02\n",
            "iteration: 23140 loss: 0.0232 lr: 0.02\n",
            "iteration: 23150 loss: 0.0179 lr: 0.02\n",
            "iteration: 23160 loss: 0.0181 lr: 0.02\n",
            "iteration: 23170 loss: 0.0190 lr: 0.02\n",
            "iteration: 23180 loss: 0.0210 lr: 0.02\n",
            "iteration: 23190 loss: 0.0197 lr: 0.02\n",
            "iteration: 23200 loss: 0.0227 lr: 0.02\n",
            "iteration: 23210 loss: 0.0127 lr: 0.02\n",
            "iteration: 23220 loss: 0.0315 lr: 0.02\n",
            "iteration: 23230 loss: 0.0240 lr: 0.02\n",
            "iteration: 23240 loss: 0.0304 lr: 0.02\n",
            "iteration: 23250 loss: 0.0163 lr: 0.02\n",
            "iteration: 23260 loss: 0.0244 lr: 0.02\n",
            "iteration: 23270 loss: 0.0169 lr: 0.02\n",
            "iteration: 23280 loss: 0.0233 lr: 0.02\n",
            "iteration: 23290 loss: 0.0135 lr: 0.02\n",
            "iteration: 23300 loss: 0.0145 lr: 0.02\n",
            "iteration: 23310 loss: 0.0183 lr: 0.02\n",
            "iteration: 23320 loss: 0.0247 lr: 0.02\n",
            "iteration: 23330 loss: 0.0213 lr: 0.02\n",
            "iteration: 23340 loss: 0.0169 lr: 0.02\n",
            "iteration: 23350 loss: 0.0198 lr: 0.02\n",
            "iteration: 23360 loss: 0.0136 lr: 0.02\n",
            "iteration: 23370 loss: 0.0296 lr: 0.02\n",
            "iteration: 23380 loss: 0.0248 lr: 0.02\n",
            "iteration: 23390 loss: 0.0133 lr: 0.02\n",
            "iteration: 23400 loss: 0.0196 lr: 0.02\n",
            "iteration: 23410 loss: 0.0305 lr: 0.02\n",
            "iteration: 23420 loss: 0.0189 lr: 0.02\n",
            "iteration: 23430 loss: 0.0233 lr: 0.02\n",
            "iteration: 23440 loss: 0.0306 lr: 0.02\n",
            "iteration: 23450 loss: 0.0138 lr: 0.02\n",
            "iteration: 23460 loss: 0.0178 lr: 0.02\n",
            "iteration: 23470 loss: 0.0203 lr: 0.02\n",
            "iteration: 23480 loss: 0.0205 lr: 0.02\n",
            "iteration: 23490 loss: 0.0114 lr: 0.02\n",
            "iteration: 23500 loss: 0.0166 lr: 0.02\n",
            "iteration: 23510 loss: 0.0188 lr: 0.02\n",
            "iteration: 23520 loss: 0.0199 lr: 0.02\n",
            "iteration: 23530 loss: 0.0183 lr: 0.02\n",
            "iteration: 23540 loss: 0.0212 lr: 0.02\n",
            "iteration: 23550 loss: 0.0212 lr: 0.02\n",
            "iteration: 23560 loss: 0.0135 lr: 0.02\n",
            "iteration: 23570 loss: 0.0257 lr: 0.02\n",
            "iteration: 23580 loss: 0.0189 lr: 0.02\n",
            "iteration: 23590 loss: 0.0214 lr: 0.02\n",
            "iteration: 23600 loss: 0.0165 lr: 0.02\n",
            "iteration: 23610 loss: 0.0167 lr: 0.02\n",
            "iteration: 23620 loss: 0.0179 lr: 0.02\n",
            "iteration: 23630 loss: 0.0149 lr: 0.02\n",
            "iteration: 23640 loss: 0.0385 lr: 0.02\n",
            "iteration: 23650 loss: 0.0231 lr: 0.02\n",
            "iteration: 23660 loss: 0.0229 lr: 0.02\n",
            "iteration: 23670 loss: 0.0178 lr: 0.02\n",
            "iteration: 23680 loss: 0.0140 lr: 0.02\n",
            "iteration: 23690 loss: 0.0268 lr: 0.02\n",
            "iteration: 23700 loss: 0.0212 lr: 0.02\n",
            "iteration: 23710 loss: 0.0295 lr: 0.02\n",
            "iteration: 23720 loss: 0.0185 lr: 0.02\n",
            "iteration: 23730 loss: 0.0176 lr: 0.02\n",
            "iteration: 23740 loss: 0.0268 lr: 0.02\n",
            "iteration: 23750 loss: 0.0161 lr: 0.02\n",
            "iteration: 23760 loss: 0.0165 lr: 0.02\n",
            "iteration: 23770 loss: 0.0287 lr: 0.02\n",
            "iteration: 23780 loss: 0.0209 lr: 0.02\n",
            "iteration: 23790 loss: 0.0164 lr: 0.02\n",
            "iteration: 23800 loss: 0.0157 lr: 0.02\n",
            "iteration: 23810 loss: 0.0190 lr: 0.02\n",
            "iteration: 23820 loss: 0.0170 lr: 0.02\n",
            "iteration: 23830 loss: 0.0230 lr: 0.02\n",
            "iteration: 23840 loss: 0.0180 lr: 0.02\n",
            "iteration: 23850 loss: 0.0178 lr: 0.02\n",
            "iteration: 23860 loss: 0.0223 lr: 0.02\n",
            "iteration: 23870 loss: 0.0195 lr: 0.02\n",
            "iteration: 23880 loss: 0.0146 lr: 0.02\n",
            "iteration: 23890 loss: 0.0283 lr: 0.02\n",
            "iteration: 23900 loss: 0.0186 lr: 0.02\n",
            "iteration: 23910 loss: 0.0152 lr: 0.02\n",
            "iteration: 23920 loss: 0.0146 lr: 0.02\n",
            "iteration: 23930 loss: 0.0213 lr: 0.02\n",
            "iteration: 23940 loss: 0.0190 lr: 0.02\n",
            "iteration: 23950 loss: 0.0213 lr: 0.02\n",
            "iteration: 23960 loss: 0.0194 lr: 0.02\n",
            "iteration: 23970 loss: 0.0284 lr: 0.02\n",
            "iteration: 23980 loss: 0.0150 lr: 0.02\n",
            "iteration: 23990 loss: 0.0253 lr: 0.02\n",
            "iteration: 24000 loss: 0.0124 lr: 0.02\n",
            "iteration: 24010 loss: 0.0156 lr: 0.02\n",
            "iteration: 24020 loss: 0.0255 lr: 0.02\n",
            "iteration: 24030 loss: 0.0258 lr: 0.02\n",
            "iteration: 24040 loss: 0.0208 lr: 0.02\n",
            "iteration: 24050 loss: 0.0192 lr: 0.02\n",
            "iteration: 24060 loss: 0.0317 lr: 0.02\n",
            "iteration: 24070 loss: 0.0138 lr: 0.02\n",
            "iteration: 24080 loss: 0.0139 lr: 0.02\n",
            "iteration: 24090 loss: 0.0162 lr: 0.02\n",
            "iteration: 24100 loss: 0.0165 lr: 0.02\n",
            "iteration: 24110 loss: 0.0156 lr: 0.02\n",
            "iteration: 24120 loss: 0.0125 lr: 0.02\n",
            "iteration: 24130 loss: 0.0205 lr: 0.02\n",
            "iteration: 24140 loss: 0.0128 lr: 0.02\n",
            "iteration: 24150 loss: 0.0160 lr: 0.02\n",
            "iteration: 24160 loss: 0.0225 lr: 0.02\n",
            "iteration: 24170 loss: 0.0187 lr: 0.02\n",
            "iteration: 24180 loss: 0.0207 lr: 0.02\n",
            "iteration: 24190 loss: 0.0269 lr: 0.02\n",
            "iteration: 24200 loss: 0.0190 lr: 0.02\n",
            "iteration: 24210 loss: 0.0171 lr: 0.02\n",
            "iteration: 24220 loss: 0.0136 lr: 0.02\n",
            "iteration: 24230 loss: 0.0260 lr: 0.02\n",
            "iteration: 24240 loss: 0.0174 lr: 0.02\n",
            "iteration: 24250 loss: 0.0170 lr: 0.02\n",
            "iteration: 24260 loss: 0.0160 lr: 0.02\n",
            "iteration: 24270 loss: 0.0196 lr: 0.02\n",
            "iteration: 24280 loss: 0.0198 lr: 0.02\n",
            "iteration: 24290 loss: 0.0279 lr: 0.02\n",
            "iteration: 24300 loss: 0.0103 lr: 0.02\n",
            "iteration: 24310 loss: 0.0134 lr: 0.02\n",
            "iteration: 24320 loss: 0.0238 lr: 0.02\n",
            "iteration: 24330 loss: 0.0202 lr: 0.02\n",
            "iteration: 24340 loss: 0.0195 lr: 0.02\n",
            "iteration: 24350 loss: 0.0145 lr: 0.02\n",
            "iteration: 24360 loss: 0.0177 lr: 0.02\n",
            "iteration: 24370 loss: 0.0219 lr: 0.02\n",
            "iteration: 24380 loss: 0.0217 lr: 0.02\n",
            "iteration: 24390 loss: 0.0215 lr: 0.02\n",
            "iteration: 24400 loss: 0.0136 lr: 0.02\n",
            "iteration: 24410 loss: 0.0154 lr: 0.02\n",
            "iteration: 24420 loss: 0.0249 lr: 0.02\n",
            "iteration: 24430 loss: 0.0372 lr: 0.02\n",
            "iteration: 24440 loss: 0.0150 lr: 0.02\n",
            "iteration: 24450 loss: 0.0233 lr: 0.02\n",
            "iteration: 24460 loss: 0.0144 lr: 0.02\n",
            "iteration: 24470 loss: 0.0147 lr: 0.02\n",
            "iteration: 24480 loss: 0.0143 lr: 0.02\n",
            "iteration: 24490 loss: 0.0170 lr: 0.02\n",
            "iteration: 24500 loss: 0.0223 lr: 0.02\n",
            "iteration: 24510 loss: 0.0209 lr: 0.02\n",
            "iteration: 24520 loss: 0.0148 lr: 0.02\n",
            "iteration: 24530 loss: 0.0143 lr: 0.02\n",
            "iteration: 24540 loss: 0.0281 lr: 0.02\n",
            "iteration: 24550 loss: 0.0197 lr: 0.02\n",
            "iteration: 24560 loss: 0.0179 lr: 0.02\n",
            "iteration: 24570 loss: 0.0188 lr: 0.02\n",
            "iteration: 24580 loss: 0.0279 lr: 0.02\n",
            "iteration: 24590 loss: 0.0227 lr: 0.02\n",
            "iteration: 24600 loss: 0.0206 lr: 0.02\n",
            "iteration: 24610 loss: 0.0158 lr: 0.02\n",
            "iteration: 24620 loss: 0.0157 lr: 0.02\n",
            "iteration: 24630 loss: 0.0108 lr: 0.02\n",
            "iteration: 24640 loss: 0.0214 lr: 0.02\n",
            "iteration: 24650 loss: 0.0131 lr: 0.02\n",
            "iteration: 24660 loss: 0.0129 lr: 0.02\n",
            "iteration: 24670 loss: 0.0227 lr: 0.02\n",
            "iteration: 24680 loss: 0.0166 lr: 0.02\n",
            "iteration: 24690 loss: 0.0098 lr: 0.02\n",
            "iteration: 24700 loss: 0.0162 lr: 0.02\n",
            "iteration: 24710 loss: 0.0113 lr: 0.02\n",
            "iteration: 24720 loss: 0.0213 lr: 0.02\n",
            "iteration: 24730 loss: 0.0183 lr: 0.02\n",
            "iteration: 24740 loss: 0.0187 lr: 0.02\n",
            "iteration: 24750 loss: 0.0175 lr: 0.02\n",
            "iteration: 24760 loss: 0.0260 lr: 0.02\n",
            "iteration: 24770 loss: 0.0120 lr: 0.02\n",
            "iteration: 24780 loss: 0.0213 lr: 0.02\n",
            "iteration: 24790 loss: 0.0129 lr: 0.02\n",
            "iteration: 24800 loss: 0.0239 lr: 0.02\n",
            "iteration: 24810 loss: 0.0131 lr: 0.02\n",
            "iteration: 24820 loss: 0.0199 lr: 0.02\n",
            "iteration: 24830 loss: 0.0137 lr: 0.02\n",
            "iteration: 24840 loss: 0.0204 lr: 0.02\n",
            "iteration: 24850 loss: 0.0157 lr: 0.02\n",
            "iteration: 24860 loss: 0.0163 lr: 0.02\n",
            "iteration: 24870 loss: 0.0162 lr: 0.02\n",
            "iteration: 24880 loss: 0.0268 lr: 0.02\n",
            "iteration: 24890 loss: 0.0212 lr: 0.02\n",
            "iteration: 24900 loss: 0.0197 lr: 0.02\n",
            "iteration: 24910 loss: 0.0235 lr: 0.02\n",
            "iteration: 24920 loss: 0.0180 lr: 0.02\n",
            "iteration: 24930 loss: 0.0209 lr: 0.02\n",
            "iteration: 24940 loss: 0.0253 lr: 0.02\n",
            "iteration: 24950 loss: 0.0385 lr: 0.02\n",
            "iteration: 24960 loss: 0.0156 lr: 0.02\n",
            "iteration: 24970 loss: 0.0143 lr: 0.02\n",
            "iteration: 24980 loss: 0.0191 lr: 0.02\n",
            "iteration: 24990 loss: 0.0365 lr: 0.02\n",
            "iteration: 25000 loss: 0.0231 lr: 0.02\n",
            "iteration: 25010 loss: 0.0212 lr: 0.02\n",
            "iteration: 25020 loss: 0.0163 lr: 0.02\n",
            "iteration: 25030 loss: 0.0206 lr: 0.02\n",
            "iteration: 25040 loss: 0.0188 lr: 0.02\n",
            "iteration: 25050 loss: 0.0233 lr: 0.02\n",
            "iteration: 25060 loss: 0.0187 lr: 0.02\n",
            "iteration: 25070 loss: 0.0182 lr: 0.02\n",
            "iteration: 25080 loss: 0.0260 lr: 0.02\n",
            "iteration: 25090 loss: 0.0203 lr: 0.02\n",
            "iteration: 25100 loss: 0.0156 lr: 0.02\n",
            "iteration: 25110 loss: 0.0191 lr: 0.02\n",
            "iteration: 25120 loss: 0.0199 lr: 0.02\n",
            "iteration: 25130 loss: 0.0190 lr: 0.02\n",
            "iteration: 25140 loss: 0.0128 lr: 0.02\n",
            "iteration: 25150 loss: 0.0163 lr: 0.02\n",
            "iteration: 25160 loss: 0.0152 lr: 0.02\n",
            "iteration: 25170 loss: 0.0172 lr: 0.02\n",
            "iteration: 25180 loss: 0.0188 lr: 0.02\n",
            "iteration: 25190 loss: 0.0197 lr: 0.02\n",
            "iteration: 25200 loss: 0.0124 lr: 0.02\n",
            "iteration: 25210 loss: 0.0216 lr: 0.02\n",
            "iteration: 25220 loss: 0.0178 lr: 0.02\n",
            "iteration: 25230 loss: 0.0157 lr: 0.02\n",
            "iteration: 25240 loss: 0.0202 lr: 0.02\n",
            "iteration: 25250 loss: 0.0213 lr: 0.02\n",
            "iteration: 25260 loss: 0.0182 lr: 0.02\n",
            "iteration: 25270 loss: 0.0162 lr: 0.02\n",
            "iteration: 25280 loss: 0.0117 lr: 0.02\n",
            "iteration: 25290 loss: 0.0141 lr: 0.02\n",
            "iteration: 25300 loss: 0.0210 lr: 0.02\n",
            "iteration: 25310 loss: 0.0213 lr: 0.02\n",
            "iteration: 25320 loss: 0.0152 lr: 0.02\n",
            "iteration: 25330 loss: 0.0179 lr: 0.02\n",
            "iteration: 25340 loss: 0.0235 lr: 0.02\n",
            "iteration: 25350 loss: 0.0221 lr: 0.02\n",
            "iteration: 25360 loss: 0.0115 lr: 0.02\n",
            "iteration: 25370 loss: 0.0205 lr: 0.02\n",
            "iteration: 25380 loss: 0.0261 lr: 0.02\n",
            "iteration: 25390 loss: 0.0250 lr: 0.02\n",
            "iteration: 25400 loss: 0.0193 lr: 0.02\n",
            "iteration: 25410 loss: 0.0194 lr: 0.02\n",
            "iteration: 25420 loss: 0.0195 lr: 0.02\n",
            "iteration: 25430 loss: 0.0172 lr: 0.02\n",
            "iteration: 25440 loss: 0.0233 lr: 0.02\n",
            "iteration: 25450 loss: 0.0136 lr: 0.02\n",
            "iteration: 25460 loss: 0.0149 lr: 0.02\n",
            "iteration: 25470 loss: 0.0126 lr: 0.02\n",
            "iteration: 25480 loss: 0.0176 lr: 0.02\n",
            "iteration: 25490 loss: 0.0276 lr: 0.02\n",
            "iteration: 25500 loss: 0.0170 lr: 0.02\n",
            "iteration: 25510 loss: 0.0171 lr: 0.02\n",
            "iteration: 25520 loss: 0.0211 lr: 0.02\n",
            "iteration: 25530 loss: 0.0119 lr: 0.02\n",
            "iteration: 25540 loss: 0.0256 lr: 0.02\n",
            "iteration: 25550 loss: 0.0136 lr: 0.02\n",
            "iteration: 25560 loss: 0.0214 lr: 0.02\n",
            "iteration: 25570 loss: 0.0233 lr: 0.02\n",
            "iteration: 25580 loss: 0.0186 lr: 0.02\n",
            "iteration: 25590 loss: 0.0173 lr: 0.02\n",
            "iteration: 25600 loss: 0.0119 lr: 0.02\n",
            "iteration: 25610 loss: 0.0183 lr: 0.02\n",
            "iteration: 25620 loss: 0.0184 lr: 0.02\n",
            "iteration: 25630 loss: 0.0113 lr: 0.02\n",
            "iteration: 25640 loss: 0.0153 lr: 0.02\n",
            "iteration: 25650 loss: 0.0125 lr: 0.02\n",
            "iteration: 25660 loss: 0.0176 lr: 0.02\n",
            "iteration: 25670 loss: 0.0158 lr: 0.02\n",
            "iteration: 25680 loss: 0.0174 lr: 0.02\n",
            "iteration: 25690 loss: 0.0177 lr: 0.02\n",
            "iteration: 25700 loss: 0.0155 lr: 0.02\n",
            "iteration: 25710 loss: 0.0141 lr: 0.02\n",
            "iteration: 25720 loss: 0.0169 lr: 0.02\n",
            "iteration: 25730 loss: 0.0166 lr: 0.02\n",
            "iteration: 25740 loss: 0.0142 lr: 0.02\n",
            "iteration: 25750 loss: 0.0169 lr: 0.02\n",
            "iteration: 25760 loss: 0.0127 lr: 0.02\n",
            "iteration: 25770 loss: 0.0165 lr: 0.02\n",
            "iteration: 25780 loss: 0.0177 lr: 0.02\n",
            "iteration: 25790 loss: 0.0152 lr: 0.02\n",
            "iteration: 25800 loss: 0.0118 lr: 0.02\n",
            "iteration: 25810 loss: 0.0130 lr: 0.02\n",
            "iteration: 25820 loss: 0.0180 lr: 0.02\n",
            "iteration: 25830 loss: 0.0209 lr: 0.02\n",
            "iteration: 25840 loss: 0.0159 lr: 0.02\n",
            "iteration: 25850 loss: 0.0110 lr: 0.02\n",
            "iteration: 25860 loss: 0.0166 lr: 0.02\n",
            "iteration: 25870 loss: 0.0155 lr: 0.02\n",
            "iteration: 25880 loss: 0.0168 lr: 0.02\n",
            "iteration: 25890 loss: 0.0188 lr: 0.02\n",
            "iteration: 25900 loss: 0.0187 lr: 0.02\n",
            "iteration: 25910 loss: 0.0132 lr: 0.02\n",
            "iteration: 25920 loss: 0.0156 lr: 0.02\n",
            "iteration: 25930 loss: 0.0209 lr: 0.02\n",
            "iteration: 25940 loss: 0.0186 lr: 0.02\n",
            "iteration: 25950 loss: 0.0223 lr: 0.02\n",
            "iteration: 25960 loss: 0.0125 lr: 0.02\n",
            "iteration: 25970 loss: 0.0182 lr: 0.02\n",
            "iteration: 25980 loss: 0.0256 lr: 0.02\n",
            "iteration: 25990 loss: 0.0150 lr: 0.02\n",
            "iteration: 26000 loss: 0.0134 lr: 0.02\n",
            "iteration: 26010 loss: 0.0186 lr: 0.02\n",
            "iteration: 26020 loss: 0.0126 lr: 0.02\n",
            "iteration: 26030 loss: 0.0224 lr: 0.02\n",
            "iteration: 26040 loss: 0.0191 lr: 0.02\n",
            "iteration: 26050 loss: 0.0134 lr: 0.02\n",
            "iteration: 26060 loss: 0.0157 lr: 0.02\n",
            "iteration: 26070 loss: 0.0155 lr: 0.02\n",
            "iteration: 26080 loss: 0.0224 lr: 0.02\n",
            "iteration: 26090 loss: 0.0154 lr: 0.02\n",
            "iteration: 26100 loss: 0.0124 lr: 0.02\n",
            "iteration: 26110 loss: 0.0173 lr: 0.02\n",
            "iteration: 26120 loss: 0.0128 lr: 0.02\n",
            "iteration: 26130 loss: 0.0167 lr: 0.02\n",
            "iteration: 26140 loss: 0.0138 lr: 0.02\n",
            "iteration: 26150 loss: 0.0204 lr: 0.02\n",
            "iteration: 26160 loss: 0.0110 lr: 0.02\n",
            "iteration: 26170 loss: 0.0183 lr: 0.02\n",
            "iteration: 26180 loss: 0.0281 lr: 0.02\n",
            "iteration: 26190 loss: 0.0172 lr: 0.02\n",
            "iteration: 26200 loss: 0.0157 lr: 0.02\n",
            "iteration: 26210 loss: 0.0164 lr: 0.02\n",
            "iteration: 26220 loss: 0.0178 lr: 0.02\n",
            "iteration: 26230 loss: 0.0265 lr: 0.02\n",
            "iteration: 26240 loss: 0.0209 lr: 0.02\n",
            "iteration: 26250 loss: 0.0168 lr: 0.02\n",
            "iteration: 26260 loss: 0.0130 lr: 0.02\n",
            "iteration: 26270 loss: 0.0165 lr: 0.02\n",
            "iteration: 26280 loss: 0.0163 lr: 0.02\n",
            "iteration: 26290 loss: 0.0190 lr: 0.02\n",
            "iteration: 26300 loss: 0.0157 lr: 0.02\n",
            "iteration: 26310 loss: 0.0125 lr: 0.02\n",
            "iteration: 26320 loss: 0.0095 lr: 0.02\n",
            "iteration: 26330 loss: 0.0192 lr: 0.02\n",
            "iteration: 26340 loss: 0.0152 lr: 0.02\n",
            "iteration: 26350 loss: 0.0266 lr: 0.02\n",
            "iteration: 26360 loss: 0.0248 lr: 0.02\n",
            "iteration: 26370 loss: 0.0119 lr: 0.02\n",
            "iteration: 26380 loss: 0.0271 lr: 0.02\n",
            "iteration: 26390 loss: 0.0224 lr: 0.02\n",
            "iteration: 26400 loss: 0.0235 lr: 0.02\n",
            "iteration: 26410 loss: 0.0175 lr: 0.02\n",
            "iteration: 26420 loss: 0.0236 lr: 0.02\n",
            "iteration: 26430 loss: 0.0168 lr: 0.02\n",
            "iteration: 26440 loss: 0.0192 lr: 0.02\n",
            "iteration: 26450 loss: 0.0153 lr: 0.02\n",
            "iteration: 26460 loss: 0.0088 lr: 0.02\n",
            "iteration: 26470 loss: 0.0334 lr: 0.02\n",
            "iteration: 26480 loss: 0.0114 lr: 0.02\n",
            "iteration: 26490 loss: 0.0235 lr: 0.02\n",
            "iteration: 26500 loss: 0.0193 lr: 0.02\n",
            "iteration: 26510 loss: 0.0226 lr: 0.02\n",
            "iteration: 26520 loss: 0.0159 lr: 0.02\n",
            "iteration: 26530 loss: 0.0203 lr: 0.02\n",
            "iteration: 26540 loss: 0.0183 lr: 0.02\n",
            "iteration: 26550 loss: 0.0184 lr: 0.02\n",
            "iteration: 26560 loss: 0.0155 lr: 0.02\n",
            "iteration: 26570 loss: 0.0128 lr: 0.02\n",
            "iteration: 26580 loss: 0.0190 lr: 0.02\n",
            "iteration: 26590 loss: 0.0177 lr: 0.02\n",
            "iteration: 26600 loss: 0.0133 lr: 0.02\n",
            "iteration: 26610 loss: 0.0205 lr: 0.02\n",
            "iteration: 26620 loss: 0.0191 lr: 0.02\n",
            "iteration: 26630 loss: 0.0118 lr: 0.02\n",
            "iteration: 26640 loss: 0.0274 lr: 0.02\n",
            "iteration: 26650 loss: 0.0223 lr: 0.02\n",
            "iteration: 26660 loss: 0.0281 lr: 0.02\n",
            "iteration: 26670 loss: 0.0293 lr: 0.02\n",
            "iteration: 26680 loss: 0.0173 lr: 0.02\n",
            "iteration: 26690 loss: 0.0181 lr: 0.02\n",
            "iteration: 26700 loss: 0.0203 lr: 0.02\n",
            "iteration: 26710 loss: 0.0180 lr: 0.02\n",
            "iteration: 26720 loss: 0.0292 lr: 0.02\n",
            "iteration: 26730 loss: 0.0227 lr: 0.02\n",
            "iteration: 26740 loss: 0.0164 lr: 0.02\n",
            "iteration: 26750 loss: 0.0253 lr: 0.02\n",
            "iteration: 26760 loss: 0.0143 lr: 0.02\n",
            "iteration: 26770 loss: 0.0256 lr: 0.02\n",
            "iteration: 26780 loss: 0.0135 lr: 0.02\n",
            "iteration: 26790 loss: 0.0179 lr: 0.02\n",
            "iteration: 26800 loss: 0.0202 lr: 0.02\n",
            "iteration: 26810 loss: 0.0292 lr: 0.02\n",
            "iteration: 26820 loss: 0.0234 lr: 0.02\n",
            "iteration: 26830 loss: 0.0119 lr: 0.02\n",
            "iteration: 26840 loss: 0.0178 lr: 0.02\n",
            "iteration: 26850 loss: 0.0189 lr: 0.02\n",
            "iteration: 26860 loss: 0.0220 lr: 0.02\n",
            "iteration: 26870 loss: 0.0198 lr: 0.02\n",
            "iteration: 26880 loss: 0.0152 lr: 0.02\n",
            "iteration: 26890 loss: 0.0139 lr: 0.02\n",
            "iteration: 26900 loss: 0.0310 lr: 0.02\n",
            "iteration: 26910 loss: 0.0203 lr: 0.02\n",
            "iteration: 26920 loss: 0.0263 lr: 0.02\n",
            "iteration: 26930 loss: 0.0133 lr: 0.02\n",
            "iteration: 26940 loss: 0.0160 lr: 0.02\n",
            "iteration: 26950 loss: 0.0251 lr: 0.02\n",
            "iteration: 26960 loss: 0.0199 lr: 0.02\n",
            "iteration: 26970 loss: 0.0258 lr: 0.02\n",
            "iteration: 26980 loss: 0.0139 lr: 0.02\n",
            "iteration: 26990 loss: 0.0198 lr: 0.02\n",
            "iteration: 27000 loss: 0.0221 lr: 0.02\n",
            "iteration: 27010 loss: 0.0127 lr: 0.02\n",
            "iteration: 27020 loss: 0.0243 lr: 0.02\n",
            "iteration: 27030 loss: 0.0198 lr: 0.02\n",
            "iteration: 27040 loss: 0.0163 lr: 0.02\n",
            "iteration: 27050 loss: 0.0179 lr: 0.02\n",
            "iteration: 27060 loss: 0.0238 lr: 0.02\n",
            "iteration: 27070 loss: 0.0190 lr: 0.02\n",
            "iteration: 27080 loss: 0.0151 lr: 0.02\n",
            "iteration: 27090 loss: 0.0130 lr: 0.02\n",
            "iteration: 27100 loss: 0.0153 lr: 0.02\n",
            "iteration: 27110 loss: 0.0182 lr: 0.02\n",
            "iteration: 27120 loss: 0.0222 lr: 0.02\n",
            "iteration: 27130 loss: 0.0169 lr: 0.02\n",
            "iteration: 27140 loss: 0.0170 lr: 0.02\n",
            "iteration: 27150 loss: 0.0192 lr: 0.02\n",
            "iteration: 27160 loss: 0.0131 lr: 0.02\n",
            "iteration: 27170 loss: 0.0213 lr: 0.02\n",
            "iteration: 27180 loss: 0.0181 lr: 0.02\n",
            "iteration: 27190 loss: 0.0182 lr: 0.02\n",
            "iteration: 27200 loss: 0.0188 lr: 0.02\n",
            "iteration: 27210 loss: 0.0160 lr: 0.02\n",
            "iteration: 27220 loss: 0.0248 lr: 0.02\n",
            "iteration: 27230 loss: 0.0119 lr: 0.02\n",
            "iteration: 27240 loss: 0.0283 lr: 0.02\n",
            "iteration: 27250 loss: 0.0174 lr: 0.02\n",
            "iteration: 27260 loss: 0.0181 lr: 0.02\n",
            "iteration: 27270 loss: 0.0116 lr: 0.02\n",
            "iteration: 27280 loss: 0.0170 lr: 0.02\n",
            "iteration: 27290 loss: 0.0213 lr: 0.02\n",
            "iteration: 27300 loss: 0.0285 lr: 0.02\n",
            "iteration: 27310 loss: 0.0210 lr: 0.02\n",
            "iteration: 27320 loss: 0.0139 lr: 0.02\n",
            "iteration: 27330 loss: 0.0111 lr: 0.02\n",
            "iteration: 27340 loss: 0.0156 lr: 0.02\n",
            "iteration: 27350 loss: 0.0214 lr: 0.02\n",
            "iteration: 27360 loss: 0.0206 lr: 0.02\n",
            "iteration: 27370 loss: 0.0277 lr: 0.02\n",
            "iteration: 27380 loss: 0.0232 lr: 0.02\n",
            "iteration: 27390 loss: 0.0157 lr: 0.02\n",
            "iteration: 27400 loss: 0.0180 lr: 0.02\n",
            "iteration: 27410 loss: 0.0183 lr: 0.02\n",
            "iteration: 27420 loss: 0.0273 lr: 0.02\n",
            "iteration: 27430 loss: 0.0211 lr: 0.02\n",
            "iteration: 27440 loss: 0.0241 lr: 0.02\n",
            "iteration: 27450 loss: 0.0263 lr: 0.02\n",
            "iteration: 27460 loss: 0.0165 lr: 0.02\n",
            "iteration: 27470 loss: 0.0183 lr: 0.02\n",
            "iteration: 27480 loss: 0.0095 lr: 0.02\n",
            "iteration: 27490 loss: 0.0140 lr: 0.02\n",
            "iteration: 27500 loss: 0.0280 lr: 0.02\n",
            "iteration: 27510 loss: 0.0187 lr: 0.02\n",
            "iteration: 27520 loss: 0.0174 lr: 0.02\n",
            "iteration: 27530 loss: 0.0208 lr: 0.02\n",
            "iteration: 27540 loss: 0.0167 lr: 0.02\n",
            "iteration: 27550 loss: 0.0129 lr: 0.02\n",
            "iteration: 27560 loss: 0.0173 lr: 0.02\n",
            "iteration: 27570 loss: 0.0151 lr: 0.02\n",
            "iteration: 27580 loss: 0.0161 lr: 0.02\n",
            "iteration: 27590 loss: 0.0135 lr: 0.02\n",
            "iteration: 27600 loss: 0.0198 lr: 0.02\n",
            "iteration: 27610 loss: 0.0146 lr: 0.02\n",
            "iteration: 27620 loss: 0.0102 lr: 0.02\n",
            "iteration: 27630 loss: 0.0149 lr: 0.02\n",
            "iteration: 27640 loss: 0.0140 lr: 0.02\n",
            "iteration: 27650 loss: 0.0138 lr: 0.02\n",
            "iteration: 27660 loss: 0.0156 lr: 0.02\n",
            "iteration: 27670 loss: 0.0120 lr: 0.02\n",
            "iteration: 27680 loss: 0.0198 lr: 0.02\n",
            "iteration: 27690 loss: 0.0166 lr: 0.02\n",
            "iteration: 27700 loss: 0.0109 lr: 0.02\n",
            "iteration: 27710 loss: 0.0160 lr: 0.02\n",
            "iteration: 27720 loss: 0.0273 lr: 0.02\n",
            "iteration: 27730 loss: 0.0130 lr: 0.02\n",
            "iteration: 27740 loss: 0.0131 lr: 0.02\n",
            "iteration: 27750 loss: 0.0163 lr: 0.02\n",
            "iteration: 27760 loss: 0.0180 lr: 0.02\n",
            "iteration: 27770 loss: 0.0183 lr: 0.02\n",
            "iteration: 27780 loss: 0.0307 lr: 0.02\n",
            "iteration: 27790 loss: 0.0152 lr: 0.02\n",
            "iteration: 27800 loss: 0.0182 lr: 0.02\n",
            "iteration: 27810 loss: 0.0209 lr: 0.02\n",
            "iteration: 27820 loss: 0.0331 lr: 0.02\n",
            "iteration: 27830 loss: 0.0224 lr: 0.02\n",
            "iteration: 27840 loss: 0.0154 lr: 0.02\n",
            "iteration: 27850 loss: 0.0268 lr: 0.02\n",
            "iteration: 27860 loss: 0.0146 lr: 0.02\n",
            "iteration: 27870 loss: 0.0252 lr: 0.02\n",
            "iteration: 27880 loss: 0.0147 lr: 0.02\n",
            "iteration: 27890 loss: 0.0200 lr: 0.02\n",
            "iteration: 27900 loss: 0.0140 lr: 0.02\n",
            "iteration: 27910 loss: 0.0249 lr: 0.02\n",
            "iteration: 27920 loss: 0.0239 lr: 0.02\n",
            "iteration: 27930 loss: 0.0189 lr: 0.02\n",
            "iteration: 27940 loss: 0.0083 lr: 0.02\n",
            "iteration: 27950 loss: 0.0176 lr: 0.02\n",
            "iteration: 27960 loss: 0.0205 lr: 0.02\n",
            "iteration: 27970 loss: 0.0208 lr: 0.02\n",
            "iteration: 27980 loss: 0.0131 lr: 0.02\n",
            "iteration: 27990 loss: 0.0203 lr: 0.02\n",
            "iteration: 28000 loss: 0.0137 lr: 0.02\n",
            "iteration: 28010 loss: 0.0240 lr: 0.02\n",
            "iteration: 28020 loss: 0.0174 lr: 0.02\n",
            "iteration: 28030 loss: 0.0278 lr: 0.02\n",
            "iteration: 28040 loss: 0.0181 lr: 0.02\n",
            "iteration: 28050 loss: 0.0162 lr: 0.02\n",
            "iteration: 28060 loss: 0.0126 lr: 0.02\n",
            "iteration: 28070 loss: 0.0171 lr: 0.02\n",
            "iteration: 28080 loss: 0.0125 lr: 0.02\n",
            "iteration: 28090 loss: 0.0144 lr: 0.02\n",
            "iteration: 28100 loss: 0.0241 lr: 0.02\n",
            "iteration: 28110 loss: 0.0308 lr: 0.02\n",
            "iteration: 28120 loss: 0.0149 lr: 0.02\n",
            "iteration: 28130 loss: 0.0103 lr: 0.02\n",
            "iteration: 28140 loss: 0.0158 lr: 0.02\n",
            "iteration: 28150 loss: 0.0196 lr: 0.02\n",
            "iteration: 28160 loss: 0.0251 lr: 0.02\n",
            "iteration: 28170 loss: 0.0184 lr: 0.02\n",
            "iteration: 28180 loss: 0.0190 lr: 0.02\n",
            "iteration: 28190 loss: 0.0115 lr: 0.02\n",
            "iteration: 28200 loss: 0.0204 lr: 0.02\n",
            "iteration: 28210 loss: 0.0158 lr: 0.02\n",
            "iteration: 28220 loss: 0.0163 lr: 0.02\n",
            "iteration: 28230 loss: 0.0135 lr: 0.02\n",
            "iteration: 28240 loss: 0.0146 lr: 0.02\n",
            "iteration: 28250 loss: 0.0137 lr: 0.02\n",
            "iteration: 28260 loss: 0.0208 lr: 0.02\n",
            "iteration: 28270 loss: 0.0164 lr: 0.02\n",
            "iteration: 28280 loss: 0.0159 lr: 0.02\n",
            "iteration: 28290 loss: 0.0224 lr: 0.02\n",
            "iteration: 28300 loss: 0.0148 lr: 0.02\n",
            "iteration: 28310 loss: 0.0288 lr: 0.02\n",
            "iteration: 28320 loss: 0.0205 lr: 0.02\n",
            "iteration: 28330 loss: 0.0156 lr: 0.02\n",
            "iteration: 28340 loss: 0.0178 lr: 0.02\n",
            "iteration: 28350 loss: 0.0188 lr: 0.02\n",
            "iteration: 28360 loss: 0.0215 lr: 0.02\n",
            "iteration: 28370 loss: 0.0185 lr: 0.02\n",
            "iteration: 28380 loss: 0.0142 lr: 0.02\n",
            "iteration: 28390 loss: 0.0146 lr: 0.02\n",
            "iteration: 28400 loss: 0.0140 lr: 0.02\n",
            "iteration: 28410 loss: 0.0151 lr: 0.02\n",
            "iteration: 28420 loss: 0.0194 lr: 0.02\n",
            "iteration: 28430 loss: 0.0237 lr: 0.02\n",
            "iteration: 28440 loss: 0.0235 lr: 0.02\n",
            "iteration: 28450 loss: 0.0165 lr: 0.02\n",
            "iteration: 28460 loss: 0.0189 lr: 0.02\n",
            "iteration: 28470 loss: 0.0190 lr: 0.02\n",
            "iteration: 28480 loss: 0.0214 lr: 0.02\n",
            "iteration: 28490 loss: 0.0188 lr: 0.02\n",
            "iteration: 28500 loss: 0.0200 lr: 0.02\n",
            "iteration: 28510 loss: 0.0326 lr: 0.02\n",
            "iteration: 28520 loss: 0.0211 lr: 0.02\n",
            "iteration: 28530 loss: 0.0139 lr: 0.02\n",
            "iteration: 28540 loss: 0.0178 lr: 0.02\n",
            "iteration: 28550 loss: 0.0177 lr: 0.02\n",
            "iteration: 28560 loss: 0.0183 lr: 0.02\n",
            "iteration: 28570 loss: 0.0236 lr: 0.02\n",
            "iteration: 28580 loss: 0.0140 lr: 0.02\n",
            "iteration: 28590 loss: 0.0138 lr: 0.02\n",
            "iteration: 28600 loss: 0.0301 lr: 0.02\n",
            "iteration: 28610 loss: 0.0182 lr: 0.02\n",
            "iteration: 28620 loss: 0.0290 lr: 0.02\n",
            "iteration: 28630 loss: 0.0175 lr: 0.02\n",
            "iteration: 28640 loss: 0.0206 lr: 0.02\n",
            "iteration: 28650 loss: 0.0146 lr: 0.02\n",
            "iteration: 28660 loss: 0.0213 lr: 0.02\n",
            "iteration: 28670 loss: 0.0132 lr: 0.02\n",
            "iteration: 28680 loss: 0.0196 lr: 0.02\n",
            "iteration: 28690 loss: 0.0148 lr: 0.02\n",
            "iteration: 28700 loss: 0.0134 lr: 0.02\n",
            "iteration: 28710 loss: 0.0273 lr: 0.02\n",
            "iteration: 28720 loss: 0.0229 lr: 0.02\n",
            "iteration: 28730 loss: 0.0202 lr: 0.02\n",
            "iteration: 28740 loss: 0.0145 lr: 0.02\n",
            "iteration: 28750 loss: 0.0171 lr: 0.02\n",
            "iteration: 28760 loss: 0.0160 lr: 0.02\n",
            "iteration: 28770 loss: 0.0135 lr: 0.02\n",
            "iteration: 28780 loss: 0.0179 lr: 0.02\n",
            "iteration: 28790 loss: 0.0124 lr: 0.02\n",
            "iteration: 28800 loss: 0.0166 lr: 0.02\n",
            "iteration: 28810 loss: 0.0142 lr: 0.02\n",
            "iteration: 28820 loss: 0.0106 lr: 0.02\n",
            "iteration: 28830 loss: 0.0183 lr: 0.02\n",
            "iteration: 28840 loss: 0.0172 lr: 0.02\n",
            "iteration: 28850 loss: 0.0137 lr: 0.02\n",
            "iteration: 28860 loss: 0.0269 lr: 0.02\n",
            "iteration: 28870 loss: 0.0137 lr: 0.02\n",
            "iteration: 28880 loss: 0.0209 lr: 0.02\n",
            "iteration: 28890 loss: 0.0193 lr: 0.02\n",
            "iteration: 28900 loss: 0.0122 lr: 0.02\n",
            "iteration: 28910 loss: 0.0155 lr: 0.02\n",
            "iteration: 28920 loss: 0.0203 lr: 0.02\n",
            "iteration: 28930 loss: 0.0165 lr: 0.02\n",
            "iteration: 28940 loss: 0.0175 lr: 0.02\n",
            "iteration: 28950 loss: 0.0254 lr: 0.02\n",
            "iteration: 28960 loss: 0.0150 lr: 0.02\n",
            "iteration: 28970 loss: 0.0110 lr: 0.02\n",
            "iteration: 28980 loss: 0.0132 lr: 0.02\n",
            "iteration: 28990 loss: 0.0194 lr: 0.02\n",
            "iteration: 29000 loss: 0.0152 lr: 0.02\n",
            "iteration: 29010 loss: 0.0131 lr: 0.02\n",
            "iteration: 29020 loss: 0.0275 lr: 0.02\n",
            "iteration: 29030 loss: 0.0236 lr: 0.02\n",
            "iteration: 29040 loss: 0.0131 lr: 0.02\n",
            "iteration: 29050 loss: 0.0159 lr: 0.02\n",
            "iteration: 29060 loss: 0.0220 lr: 0.02\n",
            "iteration: 29070 loss: 0.0181 lr: 0.02\n",
            "iteration: 29080 loss: 0.0101 lr: 0.02\n",
            "iteration: 29090 loss: 0.0159 lr: 0.02\n",
            "iteration: 29100 loss: 0.0179 lr: 0.02\n",
            "iteration: 29110 loss: 0.0133 lr: 0.02\n",
            "iteration: 29120 loss: 0.0171 lr: 0.02\n",
            "iteration: 29130 loss: 0.0155 lr: 0.02\n",
            "iteration: 29140 loss: 0.0132 lr: 0.02\n",
            "iteration: 29150 loss: 0.0155 lr: 0.02\n",
            "iteration: 29160 loss: 0.0121 lr: 0.02\n",
            "iteration: 29170 loss: 0.0156 lr: 0.02\n",
            "iteration: 29180 loss: 0.0163 lr: 0.02\n",
            "iteration: 29190 loss: 0.0216 lr: 0.02\n",
            "iteration: 29200 loss: 0.0146 lr: 0.02\n",
            "iteration: 29210 loss: 0.0171 lr: 0.02\n",
            "iteration: 29220 loss: 0.0197 lr: 0.02\n",
            "iteration: 29230 loss: 0.0177 lr: 0.02\n",
            "iteration: 29240 loss: 0.0171 lr: 0.02\n",
            "iteration: 29250 loss: 0.0172 lr: 0.02\n",
            "iteration: 29260 loss: 0.0156 lr: 0.02\n",
            "iteration: 29270 loss: 0.0168 lr: 0.02\n",
            "iteration: 29280 loss: 0.0194 lr: 0.02\n",
            "iteration: 29290 loss: 0.0147 lr: 0.02\n",
            "iteration: 29300 loss: 0.0197 lr: 0.02\n",
            "iteration: 29310 loss: 0.0185 lr: 0.02\n",
            "iteration: 29320 loss: 0.0249 lr: 0.02\n",
            "iteration: 29330 loss: 0.0139 lr: 0.02\n",
            "iteration: 29340 loss: 0.0209 lr: 0.02\n",
            "iteration: 29350 loss: 0.0162 lr: 0.02\n",
            "iteration: 29360 loss: 0.0169 lr: 0.02\n",
            "iteration: 29370 loss: 0.0154 lr: 0.02\n",
            "iteration: 29380 loss: 0.0187 lr: 0.02\n",
            "iteration: 29390 loss: 0.0172 lr: 0.02\n",
            "iteration: 29400 loss: 0.0150 lr: 0.02\n",
            "iteration: 29410 loss: 0.0171 lr: 0.02\n",
            "iteration: 29420 loss: 0.0111 lr: 0.02\n",
            "iteration: 29430 loss: 0.0203 lr: 0.02\n",
            "iteration: 29440 loss: 0.0236 lr: 0.02\n",
            "iteration: 29450 loss: 0.0280 lr: 0.02\n",
            "iteration: 29460 loss: 0.0169 lr: 0.02\n",
            "iteration: 29470 loss: 0.0157 lr: 0.02\n",
            "iteration: 29480 loss: 0.0158 lr: 0.02\n",
            "iteration: 29490 loss: 0.0186 lr: 0.02\n",
            "iteration: 29500 loss: 0.0190 lr: 0.02\n",
            "iteration: 29510 loss: 0.0133 lr: 0.02\n",
            "iteration: 29520 loss: 0.0165 lr: 0.02\n",
            "iteration: 29530 loss: 0.0161 lr: 0.02\n",
            "iteration: 29540 loss: 0.0221 lr: 0.02\n",
            "iteration: 29550 loss: 0.0140 lr: 0.02\n",
            "iteration: 29560 loss: 0.0137 lr: 0.02\n",
            "iteration: 29570 loss: 0.0204 lr: 0.02\n",
            "iteration: 29580 loss: 0.0152 lr: 0.02\n",
            "iteration: 29590 loss: 0.0141 lr: 0.02\n",
            "iteration: 29600 loss: 0.0169 lr: 0.02\n",
            "iteration: 29610 loss: 0.0182 lr: 0.02\n",
            "iteration: 29620 loss: 0.0121 lr: 0.02\n",
            "iteration: 29630 loss: 0.0131 lr: 0.02\n",
            "iteration: 29640 loss: 0.0143 lr: 0.02\n",
            "iteration: 29650 loss: 0.0179 lr: 0.02\n",
            "iteration: 29660 loss: 0.0243 lr: 0.02\n",
            "iteration: 29670 loss: 0.0126 lr: 0.02\n",
            "iteration: 29680 loss: 0.0229 lr: 0.02\n",
            "iteration: 29690 loss: 0.0094 lr: 0.02\n",
            "iteration: 29700 loss: 0.0184 lr: 0.02\n",
            "iteration: 29710 loss: 0.0209 lr: 0.02\n",
            "iteration: 29720 loss: 0.0125 lr: 0.02\n",
            "iteration: 29730 loss: 0.0151 lr: 0.02\n",
            "iteration: 29740 loss: 0.0105 lr: 0.02\n",
            "iteration: 29750 loss: 0.0184 lr: 0.02\n",
            "iteration: 29760 loss: 0.0137 lr: 0.02\n",
            "iteration: 29770 loss: 0.0160 lr: 0.02\n",
            "iteration: 29780 loss: 0.0133 lr: 0.02\n",
            "iteration: 29790 loss: 0.0121 lr: 0.02\n",
            "iteration: 29800 loss: 0.0205 lr: 0.02\n",
            "iteration: 29810 loss: 0.0232 lr: 0.02\n",
            "iteration: 29820 loss: 0.0170 lr: 0.02\n",
            "iteration: 29830 loss: 0.0167 lr: 0.02\n",
            "iteration: 29840 loss: 0.0191 lr: 0.02\n",
            "iteration: 29850 loss: 0.0176 lr: 0.02\n",
            "iteration: 29860 loss: 0.0256 lr: 0.02\n",
            "iteration: 29870 loss: 0.0174 lr: 0.02\n",
            "iteration: 29880 loss: 0.0197 lr: 0.02\n",
            "iteration: 29890 loss: 0.0146 lr: 0.02\n",
            "iteration: 29900 loss: 0.0103 lr: 0.02\n",
            "iteration: 29910 loss: 0.0176 lr: 0.02\n",
            "iteration: 29920 loss: 0.0177 lr: 0.02\n",
            "iteration: 29930 loss: 0.0269 lr: 0.02\n",
            "iteration: 29940 loss: 0.0185 lr: 0.02\n",
            "iteration: 29950 loss: 0.0125 lr: 0.02\n",
            "iteration: 29960 loss: 0.0244 lr: 0.02\n",
            "iteration: 29970 loss: 0.0115 lr: 0.02\n",
            "iteration: 29980 loss: 0.0199 lr: 0.02\n",
            "iteration: 29990 loss: 0.0156 lr: 0.02\n",
            "iteration: 30000 loss: 0.0117 lr: 0.02\n",
            "iteration: 30010 loss: 0.0142 lr: 0.02\n",
            "iteration: 30020 loss: 0.0155 lr: 0.02\n",
            "iteration: 30030 loss: 0.0285 lr: 0.02\n",
            "iteration: 30040 loss: 0.0134 lr: 0.02\n",
            "iteration: 30050 loss: 0.0182 lr: 0.02\n",
            "iteration: 30060 loss: 0.0211 lr: 0.02\n",
            "iteration: 30070 loss: 0.0132 lr: 0.02\n",
            "iteration: 30080 loss: 0.0157 lr: 0.02\n",
            "iteration: 30090 loss: 0.0123 lr: 0.02\n",
            "iteration: 30100 loss: 0.0108 lr: 0.02\n",
            "iteration: 30110 loss: 0.0204 lr: 0.02\n",
            "iteration: 30120 loss: 0.0133 lr: 0.02\n",
            "iteration: 30130 loss: 0.0153 lr: 0.02\n",
            "iteration: 30140 loss: 0.0134 lr: 0.02\n",
            "iteration: 30150 loss: 0.0170 lr: 0.02\n",
            "iteration: 30160 loss: 0.0173 lr: 0.02\n",
            "iteration: 30170 loss: 0.0189 lr: 0.02\n",
            "iteration: 30180 loss: 0.0152 lr: 0.02\n",
            "iteration: 30190 loss: 0.0170 lr: 0.02\n",
            "iteration: 30200 loss: 0.0249 lr: 0.02\n",
            "iteration: 30210 loss: 0.0155 lr: 0.02\n",
            "iteration: 30220 loss: 0.0166 lr: 0.02\n",
            "iteration: 30230 loss: 0.0254 lr: 0.02\n",
            "iteration: 30240 loss: 0.0149 lr: 0.02\n",
            "iteration: 30250 loss: 0.0184 lr: 0.02\n",
            "iteration: 30260 loss: 0.0189 lr: 0.02\n",
            "iteration: 30270 loss: 0.0249 lr: 0.02\n",
            "iteration: 30280 loss: 0.0204 lr: 0.02\n",
            "iteration: 30290 loss: 0.0182 lr: 0.02\n",
            "iteration: 30300 loss: 0.0167 lr: 0.02\n",
            "iteration: 30310 loss: 0.0177 lr: 0.02\n",
            "iteration: 30320 loss: 0.0104 lr: 0.02\n",
            "iteration: 30330 loss: 0.0109 lr: 0.02\n",
            "iteration: 30340 loss: 0.0216 lr: 0.02\n",
            "iteration: 30350 loss: 0.0218 lr: 0.02\n",
            "iteration: 30360 loss: 0.0147 lr: 0.02\n",
            "iteration: 30370 loss: 0.0168 lr: 0.02\n",
            "iteration: 30380 loss: 0.0158 lr: 0.02\n",
            "iteration: 30390 loss: 0.0081 lr: 0.02\n",
            "iteration: 30400 loss: 0.0189 lr: 0.02\n",
            "iteration: 30410 loss: 0.0159 lr: 0.02\n",
            "iteration: 30420 loss: 0.0318 lr: 0.02\n",
            "iteration: 30430 loss: 0.0176 lr: 0.02\n",
            "iteration: 30440 loss: 0.0132 lr: 0.02\n",
            "iteration: 30450 loss: 0.0132 lr: 0.02\n",
            "iteration: 30460 loss: 0.0183 lr: 0.02\n",
            "iteration: 30470 loss: 0.0239 lr: 0.02\n",
            "iteration: 30480 loss: 0.0200 lr: 0.02\n",
            "iteration: 30490 loss: 0.0151 lr: 0.02\n",
            "iteration: 30500 loss: 0.0197 lr: 0.02\n",
            "iteration: 30510 loss: 0.0155 lr: 0.02\n",
            "iteration: 30520 loss: 0.0122 lr: 0.02\n",
            "iteration: 30530 loss: 0.0164 lr: 0.02\n",
            "iteration: 30540 loss: 0.0252 lr: 0.02\n",
            "iteration: 30550 loss: 0.0229 lr: 0.02\n",
            "iteration: 30560 loss: 0.0143 lr: 0.02\n",
            "iteration: 30570 loss: 0.0131 lr: 0.02\n",
            "iteration: 30580 loss: 0.0144 lr: 0.02\n",
            "iteration: 30590 loss: 0.0163 lr: 0.02\n",
            "iteration: 30600 loss: 0.0164 lr: 0.02\n",
            "iteration: 30610 loss: 0.0154 lr: 0.02\n",
            "iteration: 30620 loss: 0.0198 lr: 0.02\n",
            "iteration: 30630 loss: 0.0167 lr: 0.02\n",
            "iteration: 30640 loss: 0.0203 lr: 0.02\n",
            "iteration: 30650 loss: 0.0129 lr: 0.02\n",
            "iteration: 30660 loss: 0.0171 lr: 0.02\n",
            "iteration: 30670 loss: 0.0110 lr: 0.02\n",
            "iteration: 30680 loss: 0.0198 lr: 0.02\n",
            "iteration: 30690 loss: 0.0200 lr: 0.02\n",
            "iteration: 30700 loss: 0.0302 lr: 0.02\n",
            "iteration: 30710 loss: 0.0118 lr: 0.02\n",
            "iteration: 30720 loss: 0.0100 lr: 0.02\n",
            "iteration: 30730 loss: 0.0141 lr: 0.02\n",
            "iteration: 30740 loss: 0.0140 lr: 0.02\n",
            "iteration: 30750 loss: 0.0219 lr: 0.02\n",
            "iteration: 30760 loss: 0.0224 lr: 0.02\n",
            "iteration: 30770 loss: 0.0136 lr: 0.02\n",
            "iteration: 30780 loss: 0.0132 lr: 0.02\n",
            "iteration: 30790 loss: 0.0133 lr: 0.02\n",
            "iteration: 30800 loss: 0.0143 lr: 0.02\n",
            "iteration: 30810 loss: 0.0205 lr: 0.02\n",
            "iteration: 30820 loss: 0.0236 lr: 0.02\n",
            "iteration: 30830 loss: 0.0130 lr: 0.02\n",
            "iteration: 30840 loss: 0.0184 lr: 0.02\n",
            "iteration: 30850 loss: 0.0145 lr: 0.02\n",
            "iteration: 30860 loss: 0.0129 lr: 0.02\n",
            "iteration: 30870 loss: 0.0162 lr: 0.02\n",
            "iteration: 30880 loss: 0.0154 lr: 0.02\n",
            "iteration: 30890 loss: 0.0158 lr: 0.02\n",
            "iteration: 30900 loss: 0.0274 lr: 0.02\n",
            "iteration: 30910 loss: 0.0244 lr: 0.02\n",
            "iteration: 30920 loss: 0.0124 lr: 0.02\n",
            "iteration: 30930 loss: 0.0193 lr: 0.02\n",
            "iteration: 30940 loss: 0.0150 lr: 0.02\n",
            "iteration: 30950 loss: 0.0133 lr: 0.02\n",
            "iteration: 30960 loss: 0.0117 lr: 0.02\n",
            "iteration: 30970 loss: 0.0241 lr: 0.02\n",
            "iteration: 30980 loss: 0.0162 lr: 0.02\n",
            "iteration: 30990 loss: 0.0123 lr: 0.02\n",
            "iteration: 31000 loss: 0.0229 lr: 0.02\n",
            "iteration: 31010 loss: 0.0150 lr: 0.02\n",
            "iteration: 31020 loss: 0.0121 lr: 0.02\n",
            "iteration: 31030 loss: 0.0119 lr: 0.02\n",
            "iteration: 31040 loss: 0.0170 lr: 0.02\n",
            "iteration: 31050 loss: 0.0125 lr: 0.02\n",
            "iteration: 31060 loss: 0.0200 lr: 0.02\n",
            "iteration: 31070 loss: 0.0153 lr: 0.02\n",
            "iteration: 31080 loss: 0.0137 lr: 0.02\n",
            "iteration: 31090 loss: 0.0155 lr: 0.02\n",
            "iteration: 31100 loss: 0.0166 lr: 0.02\n",
            "iteration: 31110 loss: 0.0158 lr: 0.02\n",
            "iteration: 31120 loss: 0.0169 lr: 0.02\n",
            "iteration: 31130 loss: 0.0118 lr: 0.02\n",
            "iteration: 31140 loss: 0.0147 lr: 0.02\n",
            "iteration: 31150 loss: 0.0200 lr: 0.02\n",
            "iteration: 31160 loss: 0.0235 lr: 0.02\n",
            "iteration: 31170 loss: 0.0148 lr: 0.02\n",
            "iteration: 31180 loss: 0.0153 lr: 0.02\n",
            "iteration: 31190 loss: 0.0140 lr: 0.02\n",
            "iteration: 31200 loss: 0.0150 lr: 0.02\n",
            "iteration: 31210 loss: 0.0174 lr: 0.02\n",
            "iteration: 31220 loss: 0.0152 lr: 0.02\n",
            "iteration: 31230 loss: 0.0162 lr: 0.02\n",
            "iteration: 31240 loss: 0.0260 lr: 0.02\n",
            "iteration: 31250 loss: 0.0153 lr: 0.02\n",
            "iteration: 31260 loss: 0.0174 lr: 0.02\n",
            "iteration: 31270 loss: 0.0142 lr: 0.02\n",
            "iteration: 31280 loss: 0.0169 lr: 0.02\n",
            "iteration: 31290 loss: 0.0246 lr: 0.02\n",
            "iteration: 31300 loss: 0.0155 lr: 0.02\n",
            "iteration: 31310 loss: 0.0139 lr: 0.02\n",
            "iteration: 31320 loss: 0.0166 lr: 0.02\n",
            "iteration: 31330 loss: 0.0119 lr: 0.02\n",
            "iteration: 31340 loss: 0.0195 lr: 0.02\n",
            "iteration: 31350 loss: 0.0148 lr: 0.02\n",
            "iteration: 31360 loss: 0.0112 lr: 0.02\n",
            "iteration: 31370 loss: 0.0181 lr: 0.02\n",
            "iteration: 31380 loss: 0.0205 lr: 0.02\n",
            "iteration: 31390 loss: 0.0133 lr: 0.02\n",
            "iteration: 31400 loss: 0.0152 lr: 0.02\n",
            "iteration: 31410 loss: 0.0134 lr: 0.02\n",
            "iteration: 31420 loss: 0.0126 lr: 0.02\n",
            "iteration: 31430 loss: 0.0078 lr: 0.02\n",
            "iteration: 31440 loss: 0.0178 lr: 0.02\n",
            "iteration: 31450 loss: 0.0113 lr: 0.02\n",
            "iteration: 31460 loss: 0.0187 lr: 0.02\n",
            "iteration: 31470 loss: 0.0165 lr: 0.02\n",
            "iteration: 31480 loss: 0.0203 lr: 0.02\n",
            "iteration: 31490 loss: 0.0178 lr: 0.02\n",
            "iteration: 31500 loss: 0.0188 lr: 0.02\n",
            "iteration: 31510 loss: 0.0140 lr: 0.02\n",
            "iteration: 31520 loss: 0.0173 lr: 0.02\n",
            "iteration: 31530 loss: 0.0271 lr: 0.02\n",
            "iteration: 31540 loss: 0.0211 lr: 0.02\n",
            "iteration: 31550 loss: 0.0160 lr: 0.02\n",
            "iteration: 31560 loss: 0.0176 lr: 0.02\n",
            "iteration: 31570 loss: 0.0143 lr: 0.02\n",
            "iteration: 31580 loss: 0.0145 lr: 0.02\n",
            "iteration: 31590 loss: 0.0183 lr: 0.02\n",
            "iteration: 31600 loss: 0.0322 lr: 0.02\n",
            "iteration: 31610 loss: 0.0152 lr: 0.02\n",
            "iteration: 31620 loss: 0.0137 lr: 0.02\n",
            "iteration: 31630 loss: 0.0206 lr: 0.02\n",
            "iteration: 31640 loss: 0.0216 lr: 0.02\n",
            "iteration: 31650 loss: 0.0127 lr: 0.02\n",
            "iteration: 31660 loss: 0.0182 lr: 0.02\n",
            "iteration: 31670 loss: 0.0136 lr: 0.02\n",
            "iteration: 31680 loss: 0.0151 lr: 0.02\n",
            "iteration: 31690 loss: 0.0114 lr: 0.02\n",
            "iteration: 31700 loss: 0.0242 lr: 0.02\n",
            "iteration: 31710 loss: 0.0248 lr: 0.02\n",
            "iteration: 31720 loss: 0.0277 lr: 0.02\n",
            "iteration: 31730 loss: 0.0166 lr: 0.02\n",
            "iteration: 31740 loss: 0.0211 lr: 0.02\n",
            "iteration: 31750 loss: 0.0232 lr: 0.02\n",
            "iteration: 31760 loss: 0.0155 lr: 0.02\n",
            "iteration: 31770 loss: 0.0207 lr: 0.02\n",
            "iteration: 31780 loss: 0.0168 lr: 0.02\n",
            "iteration: 31790 loss: 0.0199 lr: 0.02\n",
            "iteration: 31800 loss: 0.0147 lr: 0.02\n",
            "iteration: 31810 loss: 0.0159 lr: 0.02\n",
            "iteration: 31820 loss: 0.0111 lr: 0.02\n",
            "iteration: 31830 loss: 0.0189 lr: 0.02\n",
            "iteration: 31840 loss: 0.0226 lr: 0.02\n",
            "iteration: 31850 loss: 0.0196 lr: 0.02\n",
            "iteration: 31860 loss: 0.0195 lr: 0.02\n",
            "iteration: 31870 loss: 0.0237 lr: 0.02\n",
            "iteration: 31880 loss: 0.0149 lr: 0.02\n",
            "iteration: 31890 loss: 0.0145 lr: 0.02\n",
            "iteration: 31900 loss: 0.0220 lr: 0.02\n",
            "iteration: 31910 loss: 0.0149 lr: 0.02\n",
            "iteration: 31920 loss: 0.0234 lr: 0.02\n",
            "iteration: 31930 loss: 0.0176 lr: 0.02\n",
            "iteration: 31940 loss: 0.0196 lr: 0.02\n",
            "iteration: 31950 loss: 0.0189 lr: 0.02\n",
            "iteration: 31960 loss: 0.0163 lr: 0.02\n",
            "iteration: 31970 loss: 0.0167 lr: 0.02\n",
            "iteration: 31980 loss: 0.0138 lr: 0.02\n",
            "iteration: 31990 loss: 0.0176 lr: 0.02\n",
            "iteration: 32000 loss: 0.0175 lr: 0.02\n",
            "iteration: 32010 loss: 0.0249 lr: 0.02\n",
            "iteration: 32020 loss: 0.0109 lr: 0.02\n",
            "iteration: 32030 loss: 0.0248 lr: 0.02\n",
            "iteration: 32040 loss: 0.0238 lr: 0.02\n",
            "iteration: 32050 loss: 0.0172 lr: 0.02\n",
            "iteration: 32060 loss: 0.0220 lr: 0.02\n",
            "iteration: 32070 loss: 0.0252 lr: 0.02\n",
            "iteration: 32080 loss: 0.0113 lr: 0.02\n",
            "iteration: 32090 loss: 0.0118 lr: 0.02\n",
            "iteration: 32100 loss: 0.0249 lr: 0.02\n",
            "iteration: 32110 loss: 0.0199 lr: 0.02\n",
            "iteration: 32120 loss: 0.0148 lr: 0.02\n",
            "iteration: 32130 loss: 0.0230 lr: 0.02\n",
            "iteration: 32140 loss: 0.0261 lr: 0.02\n",
            "iteration: 32150 loss: 0.0269 lr: 0.02\n",
            "iteration: 32160 loss: 0.0165 lr: 0.02\n",
            "iteration: 32170 loss: 0.0159 lr: 0.02\n",
            "iteration: 32180 loss: 0.0162 lr: 0.02\n",
            "iteration: 32190 loss: 0.0153 lr: 0.02\n",
            "iteration: 32200 loss: 0.0139 lr: 0.02\n",
            "iteration: 32210 loss: 0.0169 lr: 0.02\n",
            "iteration: 32220 loss: 0.0168 lr: 0.02\n",
            "iteration: 32230 loss: 0.0135 lr: 0.02\n",
            "iteration: 32240 loss: 0.0219 lr: 0.02\n",
            "iteration: 32250 loss: 0.0164 lr: 0.02\n",
            "iteration: 32260 loss: 0.0185 lr: 0.02\n",
            "iteration: 32270 loss: 0.0197 lr: 0.02\n",
            "iteration: 32280 loss: 0.0246 lr: 0.02\n",
            "iteration: 32290 loss: 0.0241 lr: 0.02\n",
            "iteration: 32300 loss: 0.0183 lr: 0.02\n",
            "iteration: 32310 loss: 0.0182 lr: 0.02\n",
            "iteration: 32320 loss: 0.0186 lr: 0.02\n",
            "iteration: 32330 loss: 0.0128 lr: 0.02\n",
            "iteration: 32340 loss: 0.0120 lr: 0.02\n",
            "iteration: 32350 loss: 0.0178 lr: 0.02\n",
            "iteration: 32360 loss: 0.0100 lr: 0.02\n",
            "iteration: 32370 loss: 0.0145 lr: 0.02\n",
            "iteration: 32380 loss: 0.0145 lr: 0.02\n",
            "iteration: 32390 loss: 0.0114 lr: 0.02\n",
            "iteration: 32400 loss: 0.0145 lr: 0.02\n",
            "iteration: 32410 loss: 0.0142 lr: 0.02\n",
            "iteration: 32420 loss: 0.0194 lr: 0.02\n",
            "iteration: 32430 loss: 0.0139 lr: 0.02\n",
            "iteration: 32440 loss: 0.0153 lr: 0.02\n",
            "iteration: 32450 loss: 0.0278 lr: 0.02\n",
            "iteration: 32460 loss: 0.0254 lr: 0.02\n",
            "iteration: 32470 loss: 0.0174 lr: 0.02\n",
            "iteration: 32480 loss: 0.0144 lr: 0.02\n",
            "iteration: 32490 loss: 0.0132 lr: 0.02\n",
            "iteration: 32500 loss: 0.0181 lr: 0.02\n",
            "iteration: 32510 loss: 0.0137 lr: 0.02\n",
            "iteration: 32520 loss: 0.0162 lr: 0.02\n",
            "iteration: 32530 loss: 0.0162 lr: 0.02\n",
            "iteration: 32540 loss: 0.0128 lr: 0.02\n",
            "iteration: 32550 loss: 0.0169 lr: 0.02\n",
            "iteration: 32560 loss: 0.0171 lr: 0.02\n",
            "iteration: 32570 loss: 0.0231 lr: 0.02\n",
            "iteration: 32580 loss: 0.0165 lr: 0.02\n",
            "iteration: 32590 loss: 0.0173 lr: 0.02\n",
            "iteration: 32600 loss: 0.0166 lr: 0.02\n",
            "iteration: 32610 loss: 0.0155 lr: 0.02\n",
            "iteration: 32620 loss: 0.0198 lr: 0.02\n",
            "iteration: 32630 loss: 0.0157 lr: 0.02\n",
            "iteration: 32640 loss: 0.0209 lr: 0.02\n",
            "iteration: 32650 loss: 0.0206 lr: 0.02\n",
            "iteration: 32660 loss: 0.0173 lr: 0.02\n",
            "iteration: 32670 loss: 0.0168 lr: 0.02\n",
            "iteration: 32680 loss: 0.0257 lr: 0.02\n",
            "iteration: 32690 loss: 0.0139 lr: 0.02\n",
            "iteration: 32700 loss: 0.0118 lr: 0.02\n",
            "iteration: 32710 loss: 0.0147 lr: 0.02\n",
            "iteration: 32720 loss: 0.0140 lr: 0.02\n",
            "iteration: 32730 loss: 0.0188 lr: 0.02\n",
            "iteration: 32740 loss: 0.0128 lr: 0.02\n",
            "iteration: 32750 loss: 0.0157 lr: 0.02\n",
            "iteration: 32760 loss: 0.0167 lr: 0.02\n",
            "iteration: 32770 loss: 0.0215 lr: 0.02\n",
            "iteration: 32780 loss: 0.0195 lr: 0.02\n",
            "iteration: 32790 loss: 0.0168 lr: 0.02\n",
            "iteration: 32800 loss: 0.0172 lr: 0.02\n",
            "iteration: 32810 loss: 0.0241 lr: 0.02\n",
            "iteration: 32820 loss: 0.0139 lr: 0.02\n",
            "iteration: 32830 loss: 0.0156 lr: 0.02\n",
            "iteration: 32840 loss: 0.0388 lr: 0.02\n",
            "iteration: 32850 loss: 0.0168 lr: 0.02\n",
            "iteration: 32860 loss: 0.0157 lr: 0.02\n",
            "iteration: 32870 loss: 0.0178 lr: 0.02\n",
            "iteration: 32880 loss: 0.0178 lr: 0.02\n",
            "iteration: 32890 loss: 0.0150 lr: 0.02\n",
            "iteration: 32900 loss: 0.0167 lr: 0.02\n",
            "iteration: 32910 loss: 0.0200 lr: 0.02\n",
            "iteration: 32920 loss: 0.0186 lr: 0.02\n",
            "iteration: 32930 loss: 0.0142 lr: 0.02\n",
            "iteration: 32940 loss: 0.0186 lr: 0.02\n",
            "iteration: 32950 loss: 0.0181 lr: 0.02\n",
            "iteration: 32960 loss: 0.0172 lr: 0.02\n",
            "iteration: 32970 loss: 0.0127 lr: 0.02\n",
            "iteration: 32980 loss: 0.0199 lr: 0.02\n",
            "iteration: 32990 loss: 0.0190 lr: 0.02\n",
            "iteration: 33000 loss: 0.0126 lr: 0.02\n",
            "iteration: 33010 loss: 0.0210 lr: 0.02\n",
            "iteration: 33020 loss: 0.0228 lr: 0.02\n",
            "iteration: 33030 loss: 0.0169 lr: 0.02\n",
            "iteration: 33040 loss: 0.0139 lr: 0.02\n",
            "iteration: 33050 loss: 0.0179 lr: 0.02\n",
            "iteration: 33060 loss: 0.0153 lr: 0.02\n",
            "iteration: 33070 loss: 0.0175 lr: 0.02\n",
            "iteration: 33080 loss: 0.0115 lr: 0.02\n",
            "iteration: 33090 loss: 0.0222 lr: 0.02\n",
            "iteration: 33100 loss: 0.0182 lr: 0.02\n",
            "iteration: 33110 loss: 0.0182 lr: 0.02\n",
            "iteration: 33120 loss: 0.0167 lr: 0.02\n",
            "iteration: 33130 loss: 0.0230 lr: 0.02\n",
            "iteration: 33140 loss: 0.0167 lr: 0.02\n",
            "iteration: 33150 loss: 0.0197 lr: 0.02\n",
            "iteration: 33160 loss: 0.0195 lr: 0.02\n",
            "iteration: 33170 loss: 0.0172 lr: 0.02\n",
            "iteration: 33180 loss: 0.0135 lr: 0.02\n",
            "iteration: 33190 loss: 0.0118 lr: 0.02\n",
            "iteration: 33200 loss: 0.0191 lr: 0.02\n",
            "iteration: 33210 loss: 0.0183 lr: 0.02\n",
            "iteration: 33220 loss: 0.0128 lr: 0.02\n",
            "iteration: 33230 loss: 0.0146 lr: 0.02\n",
            "iteration: 33240 loss: 0.0138 lr: 0.02\n",
            "iteration: 33250 loss: 0.0115 lr: 0.02\n",
            "iteration: 33260 loss: 0.0171 lr: 0.02\n",
            "iteration: 33270 loss: 0.0281 lr: 0.02\n",
            "iteration: 33280 loss: 0.0160 lr: 0.02\n",
            "iteration: 33290 loss: 0.0151 lr: 0.02\n",
            "iteration: 33300 loss: 0.0138 lr: 0.02\n",
            "iteration: 33310 loss: 0.0186 lr: 0.02\n",
            "iteration: 33320 loss: 0.0158 lr: 0.02\n",
            "iteration: 33330 loss: 0.0139 lr: 0.02\n",
            "iteration: 33340 loss: 0.0150 lr: 0.02\n",
            "iteration: 33350 loss: 0.0133 lr: 0.02\n",
            "iteration: 33360 loss: 0.0231 lr: 0.02\n",
            "iteration: 33370 loss: 0.0181 lr: 0.02\n",
            "iteration: 33380 loss: 0.0171 lr: 0.02\n",
            "iteration: 33390 loss: 0.0202 lr: 0.02\n",
            "iteration: 33400 loss: 0.0153 lr: 0.02\n",
            "iteration: 33410 loss: 0.0166 lr: 0.02\n",
            "iteration: 33420 loss: 0.0174 lr: 0.02\n",
            "iteration: 33430 loss: 0.0156 lr: 0.02\n",
            "iteration: 33440 loss: 0.0180 lr: 0.02\n",
            "iteration: 33450 loss: 0.0140 lr: 0.02\n",
            "iteration: 33460 loss: 0.0291 lr: 0.02\n",
            "iteration: 33470 loss: 0.0130 lr: 0.02\n",
            "iteration: 33480 loss: 0.0173 lr: 0.02\n",
            "iteration: 33490 loss: 0.0227 lr: 0.02\n",
            "iteration: 33500 loss: 0.0129 lr: 0.02\n",
            "iteration: 33510 loss: 0.0156 lr: 0.02\n",
            "iteration: 33520 loss: 0.0242 lr: 0.02\n",
            "iteration: 33530 loss: 0.0199 lr: 0.02\n",
            "iteration: 33540 loss: 0.0183 lr: 0.02\n",
            "iteration: 33550 loss: 0.0130 lr: 0.02\n",
            "iteration: 33560 loss: 0.0218 lr: 0.02\n",
            "iteration: 33570 loss: 0.0201 lr: 0.02\n",
            "iteration: 33580 loss: 0.0136 lr: 0.02\n",
            "iteration: 33590 loss: 0.0134 lr: 0.02\n",
            "iteration: 33600 loss: 0.0142 lr: 0.02\n",
            "iteration: 33610 loss: 0.0194 lr: 0.02\n",
            "iteration: 33620 loss: 0.0198 lr: 0.02\n",
            "iteration: 33630 loss: 0.0241 lr: 0.02\n",
            "iteration: 33640 loss: 0.0132 lr: 0.02\n",
            "iteration: 33650 loss: 0.0114 lr: 0.02\n",
            "iteration: 33660 loss: 0.0194 lr: 0.02\n",
            "iteration: 33670 loss: 0.0221 lr: 0.02\n",
            "iteration: 33680 loss: 0.0158 lr: 0.02\n",
            "iteration: 33690 loss: 0.0161 lr: 0.02\n",
            "iteration: 33700 loss: 0.0141 lr: 0.02\n",
            "iteration: 33710 loss: 0.0231 lr: 0.02\n",
            "iteration: 33720 loss: 0.0168 lr: 0.02\n",
            "iteration: 33730 loss: 0.0173 lr: 0.02\n",
            "iteration: 33740 loss: 0.0181 lr: 0.02\n",
            "iteration: 33750 loss: 0.0123 lr: 0.02\n",
            "iteration: 33760 loss: 0.0133 lr: 0.02\n",
            "iteration: 33770 loss: 0.0240 lr: 0.02\n",
            "iteration: 33780 loss: 0.0186 lr: 0.02\n",
            "iteration: 33790 loss: 0.0167 lr: 0.02\n",
            "iteration: 33800 loss: 0.0126 lr: 0.02\n",
            "iteration: 33810 loss: 0.0140 lr: 0.02\n",
            "iteration: 33820 loss: 0.0109 lr: 0.02\n",
            "iteration: 33830 loss: 0.0190 lr: 0.02\n",
            "iteration: 33840 loss: 0.0165 lr: 0.02\n",
            "iteration: 33850 loss: 0.0149 lr: 0.02\n",
            "iteration: 33860 loss: 0.0108 lr: 0.02\n",
            "iteration: 33870 loss: 0.0218 lr: 0.02\n",
            "iteration: 33880 loss: 0.0132 lr: 0.02\n",
            "iteration: 33890 loss: 0.0159 lr: 0.02\n",
            "iteration: 33900 loss: 0.0231 lr: 0.02\n",
            "iteration: 33910 loss: 0.0212 lr: 0.02\n",
            "iteration: 33920 loss: 0.0222 lr: 0.02\n",
            "iteration: 33930 loss: 0.0103 lr: 0.02\n",
            "iteration: 33940 loss: 0.0197 lr: 0.02\n",
            "iteration: 33950 loss: 0.0110 lr: 0.02\n",
            "iteration: 33960 loss: 0.0175 lr: 0.02\n",
            "iteration: 33970 loss: 0.0185 lr: 0.02\n",
            "iteration: 33980 loss: 0.0166 lr: 0.02\n",
            "iteration: 33990 loss: 0.0140 lr: 0.02\n",
            "iteration: 34000 loss: 0.0177 lr: 0.02\n",
            "iteration: 34010 loss: 0.0132 lr: 0.02\n",
            "iteration: 34020 loss: 0.0149 lr: 0.02\n",
            "iteration: 34030 loss: 0.0189 lr: 0.02\n",
            "iteration: 34040 loss: 0.0194 lr: 0.02\n",
            "iteration: 34050 loss: 0.0222 lr: 0.02\n",
            "iteration: 34060 loss: 0.0147 lr: 0.02\n",
            "iteration: 34070 loss: 0.0161 lr: 0.02\n",
            "iteration: 34080 loss: 0.0138 lr: 0.02\n",
            "iteration: 34090 loss: 0.0214 lr: 0.02\n",
            "iteration: 34100 loss: 0.0158 lr: 0.02\n",
            "iteration: 34110 loss: 0.0175 lr: 0.02\n",
            "iteration: 34120 loss: 0.0178 lr: 0.02\n",
            "iteration: 34130 loss: 0.0212 lr: 0.02\n",
            "iteration: 34140 loss: 0.0175 lr: 0.02\n",
            "iteration: 34150 loss: 0.0141 lr: 0.02\n",
            "iteration: 34160 loss: 0.0142 lr: 0.02\n",
            "iteration: 34170 loss: 0.0120 lr: 0.02\n",
            "iteration: 34180 loss: 0.0220 lr: 0.02\n",
            "iteration: 34190 loss: 0.0166 lr: 0.02\n",
            "iteration: 34200 loss: 0.0137 lr: 0.02\n",
            "iteration: 34210 loss: 0.0152 lr: 0.02\n",
            "iteration: 34220 loss: 0.0138 lr: 0.02\n",
            "iteration: 34230 loss: 0.0153 lr: 0.02\n",
            "iteration: 34240 loss: 0.0206 lr: 0.02\n",
            "iteration: 34250 loss: 0.0163 lr: 0.02\n",
            "iteration: 34260 loss: 0.0104 lr: 0.02\n",
            "iteration: 34270 loss: 0.0146 lr: 0.02\n",
            "iteration: 34280 loss: 0.0153 lr: 0.02\n",
            "iteration: 34290 loss: 0.0156 lr: 0.02\n",
            "iteration: 34300 loss: 0.0185 lr: 0.02\n",
            "iteration: 34310 loss: 0.0200 lr: 0.02\n",
            "iteration: 34320 loss: 0.0254 lr: 0.02\n",
            "iteration: 34330 loss: 0.0134 lr: 0.02\n",
            "iteration: 34340 loss: 0.0140 lr: 0.02\n",
            "iteration: 34350 loss: 0.0250 lr: 0.02\n",
            "iteration: 34360 loss: 0.0124 lr: 0.02\n",
            "iteration: 34370 loss: 0.0143 lr: 0.02\n",
            "iteration: 34380 loss: 0.0147 lr: 0.02\n",
            "iteration: 34390 loss: 0.0157 lr: 0.02\n",
            "iteration: 34400 loss: 0.0148 lr: 0.02\n",
            "iteration: 34410 loss: 0.0161 lr: 0.02\n",
            "iteration: 34420 loss: 0.0176 lr: 0.02\n",
            "iteration: 34430 loss: 0.0161 lr: 0.02\n",
            "iteration: 34440 loss: 0.0175 lr: 0.02\n",
            "iteration: 34450 loss: 0.0103 lr: 0.02\n",
            "iteration: 34460 loss: 0.0134 lr: 0.02\n",
            "iteration: 34470 loss: 0.0179 lr: 0.02\n",
            "iteration: 34480 loss: 0.0126 lr: 0.02\n",
            "iteration: 34490 loss: 0.0114 lr: 0.02\n",
            "iteration: 34500 loss: 0.0129 lr: 0.02\n",
            "iteration: 34510 loss: 0.0168 lr: 0.02\n",
            "iteration: 34520 loss: 0.0140 lr: 0.02\n",
            "iteration: 34530 loss: 0.0176 lr: 0.02\n",
            "iteration: 34540 loss: 0.0154 lr: 0.02\n",
            "iteration: 34550 loss: 0.0137 lr: 0.02\n",
            "iteration: 34560 loss: 0.0135 lr: 0.02\n",
            "iteration: 34570 loss: 0.0153 lr: 0.02\n",
            "iteration: 34580 loss: 0.0184 lr: 0.02\n",
            "iteration: 34590 loss: 0.0151 lr: 0.02\n",
            "iteration: 34600 loss: 0.0219 lr: 0.02\n",
            "iteration: 34610 loss: 0.0166 lr: 0.02\n",
            "iteration: 34620 loss: 0.0164 lr: 0.02\n",
            "iteration: 34630 loss: 0.0210 lr: 0.02\n",
            "iteration: 34640 loss: 0.0308 lr: 0.02\n",
            "iteration: 34650 loss: 0.0103 lr: 0.02\n",
            "iteration: 34660 loss: 0.0112 lr: 0.02\n",
            "iteration: 34670 loss: 0.0118 lr: 0.02\n",
            "iteration: 34680 loss: 0.0153 lr: 0.02\n",
            "iteration: 34690 loss: 0.0148 lr: 0.02\n",
            "iteration: 34700 loss: 0.0131 lr: 0.02\n",
            "iteration: 34710 loss: 0.0137 lr: 0.02\n",
            "iteration: 34720 loss: 0.0119 lr: 0.02\n",
            "iteration: 34730 loss: 0.0187 lr: 0.02\n",
            "iteration: 34740 loss: 0.0199 lr: 0.02\n",
            "iteration: 34750 loss: 0.0167 lr: 0.02\n",
            "iteration: 34760 loss: 0.0123 lr: 0.02\n",
            "iteration: 34770 loss: 0.0171 lr: 0.02\n",
            "iteration: 34780 loss: 0.0194 lr: 0.02\n",
            "iteration: 34790 loss: 0.0150 lr: 0.02\n",
            "iteration: 34800 loss: 0.0151 lr: 0.02\n",
            "iteration: 34810 loss: 0.0188 lr: 0.02\n",
            "iteration: 34820 loss: 0.0286 lr: 0.02\n",
            "iteration: 34830 loss: 0.0114 lr: 0.02\n",
            "iteration: 34840 loss: 0.0123 lr: 0.02\n",
            "iteration: 34850 loss: 0.0166 lr: 0.02\n",
            "iteration: 34860 loss: 0.0145 lr: 0.02\n",
            "iteration: 34870 loss: 0.0237 lr: 0.02\n",
            "iteration: 34880 loss: 0.0174 lr: 0.02\n",
            "iteration: 34890 loss: 0.0108 lr: 0.02\n",
            "iteration: 34900 loss: 0.0170 lr: 0.02\n",
            "iteration: 34910 loss: 0.0141 lr: 0.02\n",
            "iteration: 34920 loss: 0.0190 lr: 0.02\n",
            "iteration: 34930 loss: 0.0145 lr: 0.02\n",
            "iteration: 34940 loss: 0.0180 lr: 0.02\n",
            "iteration: 34950 loss: 0.0150 lr: 0.02\n",
            "iteration: 34960 loss: 0.0184 lr: 0.02\n",
            "iteration: 34970 loss: 0.0262 lr: 0.02\n",
            "iteration: 34980 loss: 0.0240 lr: 0.02\n",
            "iteration: 34990 loss: 0.0175 lr: 0.02\n",
            "iteration: 35000 loss: 0.0229 lr: 0.02\n",
            "iteration: 35010 loss: 0.0196 lr: 0.02\n",
            "iteration: 35020 loss: 0.0199 lr: 0.02\n",
            "iteration: 35030 loss: 0.0102 lr: 0.02\n",
            "iteration: 35040 loss: 0.0156 lr: 0.02\n",
            "iteration: 35050 loss: 0.0139 lr: 0.02\n",
            "iteration: 35060 loss: 0.0160 lr: 0.02\n",
            "iteration: 35070 loss: 0.0134 lr: 0.02\n",
            "iteration: 35080 loss: 0.0193 lr: 0.02\n",
            "iteration: 35090 loss: 0.0147 lr: 0.02\n",
            "iteration: 35100 loss: 0.0163 lr: 0.02\n",
            "iteration: 35110 loss: 0.0166 lr: 0.02\n",
            "iteration: 35120 loss: 0.0120 lr: 0.02\n",
            "iteration: 35130 loss: 0.0170 lr: 0.02\n",
            "iteration: 35140 loss: 0.0142 lr: 0.02\n",
            "iteration: 35150 loss: 0.0142 lr: 0.02\n",
            "iteration: 35160 loss: 0.0126 lr: 0.02\n",
            "iteration: 35170 loss: 0.0155 lr: 0.02\n",
            "iteration: 35180 loss: 0.0144 lr: 0.02\n",
            "iteration: 35190 loss: 0.0194 lr: 0.02\n",
            "iteration: 35200 loss: 0.0176 lr: 0.02\n",
            "iteration: 35210 loss: 0.0157 lr: 0.02\n",
            "iteration: 35220 loss: 0.0131 lr: 0.02\n",
            "iteration: 35230 loss: 0.0129 lr: 0.02\n",
            "iteration: 35240 loss: 0.0164 lr: 0.02\n",
            "iteration: 35250 loss: 0.0175 lr: 0.02\n",
            "iteration: 35260 loss: 0.0219 lr: 0.02\n",
            "iteration: 35270 loss: 0.0146 lr: 0.02\n",
            "iteration: 35280 loss: 0.0112 lr: 0.02\n",
            "iteration: 35290 loss: 0.0183 lr: 0.02\n",
            "iteration: 35300 loss: 0.0167 lr: 0.02\n",
            "iteration: 35310 loss: 0.0140 lr: 0.02\n",
            "iteration: 35320 loss: 0.0177 lr: 0.02\n",
            "iteration: 35330 loss: 0.0151 lr: 0.02\n",
            "iteration: 35340 loss: 0.0160 lr: 0.02\n",
            "iteration: 35350 loss: 0.0122 lr: 0.02\n",
            "iteration: 35360 loss: 0.0148 lr: 0.02\n",
            "iteration: 35370 loss: 0.0117 lr: 0.02\n",
            "iteration: 35380 loss: 0.0312 lr: 0.02\n",
            "iteration: 35390 loss: 0.0110 lr: 0.02\n",
            "iteration: 35400 loss: 0.0175 lr: 0.02\n",
            "iteration: 35410 loss: 0.0216 lr: 0.02\n",
            "iteration: 35420 loss: 0.0191 lr: 0.02\n",
            "iteration: 35430 loss: 0.0110 lr: 0.02\n",
            "iteration: 35440 loss: 0.0164 lr: 0.02\n",
            "iteration: 35450 loss: 0.0167 lr: 0.02\n",
            "iteration: 35460 loss: 0.0140 lr: 0.02\n",
            "iteration: 35470 loss: 0.0211 lr: 0.02\n",
            "iteration: 35480 loss: 0.0162 lr: 0.02\n",
            "iteration: 35490 loss: 0.0124 lr: 0.02\n",
            "iteration: 35500 loss: 0.0185 lr: 0.02\n",
            "iteration: 35510 loss: 0.0142 lr: 0.02\n",
            "iteration: 35520 loss: 0.0164 lr: 0.02\n",
            "iteration: 35530 loss: 0.0158 lr: 0.02\n",
            "iteration: 35540 loss: 0.0127 lr: 0.02\n",
            "iteration: 35550 loss: 0.0181 lr: 0.02\n",
            "iteration: 35560 loss: 0.0184 lr: 0.02\n",
            "iteration: 35570 loss: 0.0161 lr: 0.02\n",
            "iteration: 35580 loss: 0.0258 lr: 0.02\n",
            "iteration: 35590 loss: 0.0124 lr: 0.02\n",
            "iteration: 35600 loss: 0.0289 lr: 0.02\n",
            "iteration: 35610 loss: 0.0145 lr: 0.02\n",
            "iteration: 35620 loss: 0.0202 lr: 0.02\n",
            "iteration: 35630 loss: 0.0112 lr: 0.02\n",
            "iteration: 35640 loss: 0.0165 lr: 0.02\n",
            "iteration: 35650 loss: 0.0139 lr: 0.02\n",
            "iteration: 35660 loss: 0.0116 lr: 0.02\n",
            "iteration: 35670 loss: 0.0146 lr: 0.02\n",
            "iteration: 35680 loss: 0.0155 lr: 0.02\n",
            "iteration: 35690 loss: 0.0250 lr: 0.02\n",
            "iteration: 35700 loss: 0.0200 lr: 0.02\n",
            "iteration: 35710 loss: 0.0135 lr: 0.02\n",
            "iteration: 35720 loss: 0.0153 lr: 0.02\n",
            "iteration: 35730 loss: 0.0150 lr: 0.02\n",
            "iteration: 35740 loss: 0.0116 lr: 0.02\n",
            "iteration: 35750 loss: 0.0189 lr: 0.02\n",
            "iteration: 35760 loss: 0.0145 lr: 0.02\n",
            "iteration: 35770 loss: 0.0182 lr: 0.02\n",
            "iteration: 35780 loss: 0.0145 lr: 0.02\n",
            "iteration: 35790 loss: 0.0104 lr: 0.02\n",
            "iteration: 35800 loss: 0.0121 lr: 0.02\n",
            "iteration: 35810 loss: 0.0146 lr: 0.02\n",
            "iteration: 35820 loss: 0.0264 lr: 0.02\n",
            "iteration: 35830 loss: 0.0147 lr: 0.02\n",
            "iteration: 35840 loss: 0.0128 lr: 0.02\n",
            "iteration: 35850 loss: 0.0168 lr: 0.02\n",
            "iteration: 35860 loss: 0.0119 lr: 0.02\n",
            "iteration: 35870 loss: 0.0112 lr: 0.02\n",
            "iteration: 35880 loss: 0.0222 lr: 0.02\n",
            "iteration: 35890 loss: 0.0171 lr: 0.02\n",
            "iteration: 35900 loss: 0.0151 lr: 0.02\n",
            "iteration: 35910 loss: 0.0116 lr: 0.02\n",
            "iteration: 35920 loss: 0.0178 lr: 0.02\n",
            "iteration: 35930 loss: 0.0115 lr: 0.02\n",
            "iteration: 35940 loss: 0.0159 lr: 0.02\n",
            "iteration: 35950 loss: 0.0124 lr: 0.02\n",
            "iteration: 35960 loss: 0.0240 lr: 0.02\n",
            "iteration: 35970 loss: 0.0121 lr: 0.02\n",
            "iteration: 35980 loss: 0.0115 lr: 0.02\n",
            "iteration: 35990 loss: 0.0169 lr: 0.02\n",
            "iteration: 36000 loss: 0.0148 lr: 0.02\n",
            "iteration: 36010 loss: 0.0174 lr: 0.02\n",
            "iteration: 36020 loss: 0.0142 lr: 0.02\n",
            "iteration: 36030 loss: 0.0131 lr: 0.02\n",
            "iteration: 36040 loss: 0.0166 lr: 0.02\n",
            "iteration: 36050 loss: 0.0158 lr: 0.02\n",
            "iteration: 36060 loss: 0.0200 lr: 0.02\n",
            "iteration: 36070 loss: 0.0137 lr: 0.02\n",
            "iteration: 36080 loss: 0.0153 lr: 0.02\n",
            "iteration: 36090 loss: 0.0185 lr: 0.02\n",
            "iteration: 36100 loss: 0.0122 lr: 0.02\n",
            "iteration: 36110 loss: 0.0184 lr: 0.02\n",
            "iteration: 36120 loss: 0.0199 lr: 0.02\n",
            "iteration: 36130 loss: 0.0170 lr: 0.02\n",
            "iteration: 36140 loss: 0.0177 lr: 0.02\n",
            "iteration: 36150 loss: 0.0162 lr: 0.02\n",
            "iteration: 36160 loss: 0.0148 lr: 0.02\n",
            "iteration: 36170 loss: 0.0176 lr: 0.02\n",
            "iteration: 36180 loss: 0.0149 lr: 0.02\n",
            "iteration: 36190 loss: 0.0163 lr: 0.02\n",
            "iteration: 36200 loss: 0.0160 lr: 0.02\n",
            "iteration: 36210 loss: 0.0164 lr: 0.02\n",
            "iteration: 36220 loss: 0.0134 lr: 0.02\n",
            "iteration: 36230 loss: 0.0243 lr: 0.02\n",
            "iteration: 36240 loss: 0.0110 lr: 0.02\n",
            "iteration: 36250 loss: 0.0148 lr: 0.02\n",
            "iteration: 36260 loss: 0.0180 lr: 0.02\n",
            "iteration: 36270 loss: 0.0173 lr: 0.02\n",
            "iteration: 36280 loss: 0.0180 lr: 0.02\n",
            "iteration: 36290 loss: 0.0271 lr: 0.02\n",
            "iteration: 36300 loss: 0.0185 lr: 0.02\n",
            "iteration: 36310 loss: 0.0138 lr: 0.02\n",
            "iteration: 36320 loss: 0.0193 lr: 0.02\n",
            "iteration: 36330 loss: 0.0171 lr: 0.02\n",
            "iteration: 36340 loss: 0.0171 lr: 0.02\n",
            "iteration: 36350 loss: 0.0225 lr: 0.02\n",
            "iteration: 36360 loss: 0.0081 lr: 0.02\n",
            "iteration: 36370 loss: 0.0110 lr: 0.02\n",
            "iteration: 36380 loss: 0.0145 lr: 0.02\n",
            "iteration: 36390 loss: 0.0132 lr: 0.02\n",
            "iteration: 36400 loss: 0.0198 lr: 0.02\n",
            "iteration: 36410 loss: 0.0223 lr: 0.02\n",
            "iteration: 36420 loss: 0.0162 lr: 0.02\n",
            "iteration: 36430 loss: 0.0268 lr: 0.02\n",
            "iteration: 36440 loss: 0.0146 lr: 0.02\n",
            "iteration: 36450 loss: 0.0159 lr: 0.02\n",
            "iteration: 36460 loss: 0.0138 lr: 0.02\n",
            "iteration: 36470 loss: 0.0189 lr: 0.02\n",
            "iteration: 36480 loss: 0.0137 lr: 0.02\n",
            "iteration: 36490 loss: 0.0170 lr: 0.02\n",
            "iteration: 36500 loss: 0.0166 lr: 0.02\n",
            "iteration: 36510 loss: 0.0188 lr: 0.02\n",
            "iteration: 36520 loss: 0.0149 lr: 0.02\n",
            "iteration: 36530 loss: 0.0132 lr: 0.02\n",
            "iteration: 36540 loss: 0.0107 lr: 0.02\n",
            "iteration: 36550 loss: 0.0166 lr: 0.02\n",
            "iteration: 36560 loss: 0.0137 lr: 0.02\n",
            "iteration: 36570 loss: 0.0131 lr: 0.02\n",
            "iteration: 36580 loss: 0.0136 lr: 0.02\n",
            "iteration: 36590 loss: 0.0172 lr: 0.02\n",
            "iteration: 36600 loss: 0.0186 lr: 0.02\n",
            "iteration: 36610 loss: 0.0129 lr: 0.02\n",
            "iteration: 36620 loss: 0.0149 lr: 0.02\n",
            "iteration: 36630 loss: 0.0132 lr: 0.02\n",
            "iteration: 36640 loss: 0.0145 lr: 0.02\n",
            "iteration: 36650 loss: 0.0178 lr: 0.02\n",
            "iteration: 36660 loss: 0.0152 lr: 0.02\n",
            "iteration: 36670 loss: 0.0168 lr: 0.02\n",
            "iteration: 36680 loss: 0.0190 lr: 0.02\n",
            "iteration: 36690 loss: 0.0151 lr: 0.02\n",
            "iteration: 36700 loss: 0.0121 lr: 0.02\n",
            "iteration: 36710 loss: 0.0141 lr: 0.02\n",
            "iteration: 36720 loss: 0.0128 lr: 0.02\n",
            "iteration: 36730 loss: 0.0137 lr: 0.02\n",
            "iteration: 36740 loss: 0.0139 lr: 0.02\n",
            "iteration: 36750 loss: 0.0144 lr: 0.02\n",
            "iteration: 36760 loss: 0.0160 lr: 0.02\n",
            "iteration: 36770 loss: 0.0133 lr: 0.02\n",
            "iteration: 36780 loss: 0.0131 lr: 0.02\n",
            "iteration: 36790 loss: 0.0188 lr: 0.02\n",
            "iteration: 36800 loss: 0.0200 lr: 0.02\n",
            "iteration: 36810 loss: 0.0164 lr: 0.02\n",
            "iteration: 36820 loss: 0.0132 lr: 0.02\n",
            "iteration: 36830 loss: 0.0101 lr: 0.02\n",
            "iteration: 36840 loss: 0.0109 lr: 0.02\n",
            "iteration: 36850 loss: 0.0131 lr: 0.02\n",
            "iteration: 36860 loss: 0.0121 lr: 0.02\n",
            "iteration: 36870 loss: 0.0188 lr: 0.02\n",
            "iteration: 36880 loss: 0.0110 lr: 0.02\n",
            "iteration: 36890 loss: 0.0121 lr: 0.02\n",
            "iteration: 36900 loss: 0.0115 lr: 0.02\n",
            "iteration: 36910 loss: 0.0118 lr: 0.02\n",
            "iteration: 36920 loss: 0.0150 lr: 0.02\n",
            "iteration: 36930 loss: 0.0178 lr: 0.02\n",
            "iteration: 36940 loss: 0.0142 lr: 0.02\n",
            "iteration: 36950 loss: 0.0192 lr: 0.02\n",
            "iteration: 36960 loss: 0.0246 lr: 0.02\n",
            "iteration: 36970 loss: 0.0106 lr: 0.02\n",
            "iteration: 36980 loss: 0.0224 lr: 0.02\n",
            "iteration: 36990 loss: 0.0196 lr: 0.02\n",
            "iteration: 37000 loss: 0.0185 lr: 0.02\n",
            "iteration: 37010 loss: 0.0183 lr: 0.02\n",
            "iteration: 37020 loss: 0.0145 lr: 0.02\n",
            "iteration: 37030 loss: 0.0157 lr: 0.02\n",
            "iteration: 37040 loss: 0.0179 lr: 0.02\n",
            "iteration: 37050 loss: 0.0204 lr: 0.02\n",
            "iteration: 37060 loss: 0.0122 lr: 0.02\n",
            "iteration: 37070 loss: 0.0150 lr: 0.02\n",
            "iteration: 37080 loss: 0.0138 lr: 0.02\n",
            "iteration: 37090 loss: 0.0151 lr: 0.02\n",
            "iteration: 37100 loss: 0.0122 lr: 0.02\n",
            "iteration: 37110 loss: 0.0170 lr: 0.02\n",
            "iteration: 37120 loss: 0.0118 lr: 0.02\n",
            "iteration: 37130 loss: 0.0139 lr: 0.02\n",
            "iteration: 37140 loss: 0.0172 lr: 0.02\n",
            "iteration: 37150 loss: 0.0124 lr: 0.02\n",
            "iteration: 37160 loss: 0.0090 lr: 0.02\n",
            "iteration: 37170 loss: 0.0137 lr: 0.02\n",
            "iteration: 37180 loss: 0.0122 lr: 0.02\n",
            "iteration: 37190 loss: 0.0113 lr: 0.02\n",
            "iteration: 37200 loss: 0.0139 lr: 0.02\n",
            "iteration: 37210 loss: 0.0278 lr: 0.02\n",
            "iteration: 37220 loss: 0.0239 lr: 0.02\n",
            "iteration: 37230 loss: 0.0158 lr: 0.02\n",
            "iteration: 37240 loss: 0.0132 lr: 0.02\n",
            "iteration: 37250 loss: 0.0130 lr: 0.02\n",
            "iteration: 37260 loss: 0.0110 lr: 0.02\n",
            "iteration: 37270 loss: 0.0203 lr: 0.02\n",
            "iteration: 37280 loss: 0.0117 lr: 0.02\n",
            "iteration: 37290 loss: 0.0162 lr: 0.02\n",
            "iteration: 37300 loss: 0.0134 lr: 0.02\n",
            "iteration: 37310 loss: 0.0177 lr: 0.02\n",
            "iteration: 37320 loss: 0.0145 lr: 0.02\n",
            "iteration: 37330 loss: 0.0148 lr: 0.02\n",
            "iteration: 37340 loss: 0.0102 lr: 0.02\n",
            "iteration: 37350 loss: 0.0190 lr: 0.02\n",
            "iteration: 37360 loss: 0.0180 lr: 0.02\n",
            "iteration: 37370 loss: 0.0162 lr: 0.02\n",
            "iteration: 37380 loss: 0.0189 lr: 0.02\n",
            "iteration: 37390 loss: 0.0140 lr: 0.02\n",
            "iteration: 37400 loss: 0.0166 lr: 0.02\n",
            "iteration: 37410 loss: 0.0146 lr: 0.02\n",
            "iteration: 37420 loss: 0.0151 lr: 0.02\n",
            "iteration: 37430 loss: 0.0294 lr: 0.02\n",
            "iteration: 37440 loss: 0.0219 lr: 0.02\n",
            "iteration: 37450 loss: 0.0160 lr: 0.02\n",
            "iteration: 37460 loss: 0.0120 lr: 0.02\n",
            "iteration: 37470 loss: 0.0213 lr: 0.02\n",
            "iteration: 37480 loss: 0.0240 lr: 0.02\n",
            "iteration: 37490 loss: 0.0154 lr: 0.02\n",
            "iteration: 37500 loss: 0.0149 lr: 0.02\n",
            "iteration: 37510 loss: 0.0228 lr: 0.02\n",
            "iteration: 37520 loss: 0.0195 lr: 0.02\n",
            "iteration: 37530 loss: 0.0110 lr: 0.02\n",
            "iteration: 37540 loss: 0.0270 lr: 0.02\n",
            "iteration: 37550 loss: 0.0153 lr: 0.02\n",
            "iteration: 37560 loss: 0.0211 lr: 0.02\n",
            "iteration: 37570 loss: 0.0180 lr: 0.02\n",
            "iteration: 37580 loss: 0.0182 lr: 0.02\n",
            "iteration: 37590 loss: 0.0146 lr: 0.02\n",
            "iteration: 37600 loss: 0.0117 lr: 0.02\n",
            "iteration: 37610 loss: 0.0184 lr: 0.02\n",
            "iteration: 37620 loss: 0.0151 lr: 0.02\n",
            "iteration: 37630 loss: 0.0149 lr: 0.02\n",
            "iteration: 37640 loss: 0.0148 lr: 0.02\n",
            "iteration: 37650 loss: 0.0157 lr: 0.02\n",
            "iteration: 37660 loss: 0.0143 lr: 0.02\n",
            "iteration: 37670 loss: 0.0163 lr: 0.02\n",
            "iteration: 37680 loss: 0.0137 lr: 0.02\n",
            "iteration: 37690 loss: 0.0197 lr: 0.02\n",
            "iteration: 37700 loss: 0.0216 lr: 0.02\n",
            "iteration: 37710 loss: 0.0157 lr: 0.02\n",
            "iteration: 37720 loss: 0.0126 lr: 0.02\n",
            "iteration: 37730 loss: 0.0207 lr: 0.02\n",
            "iteration: 37740 loss: 0.0109 lr: 0.02\n",
            "iteration: 37750 loss: 0.0210 lr: 0.02\n",
            "iteration: 37760 loss: 0.0137 lr: 0.02\n",
            "iteration: 37770 loss: 0.0110 lr: 0.02\n",
            "iteration: 37780 loss: 0.0170 lr: 0.02\n",
            "iteration: 37790 loss: 0.0220 lr: 0.02\n",
            "iteration: 37800 loss: 0.0157 lr: 0.02\n",
            "iteration: 37810 loss: 0.0116 lr: 0.02\n",
            "iteration: 37820 loss: 0.0149 lr: 0.02\n",
            "iteration: 37830 loss: 0.0147 lr: 0.02\n",
            "iteration: 37840 loss: 0.0182 lr: 0.02\n",
            "iteration: 37850 loss: 0.0084 lr: 0.02\n",
            "iteration: 37860 loss: 0.0128 lr: 0.02\n",
            "iteration: 37870 loss: 0.0148 lr: 0.02\n",
            "iteration: 37880 loss: 0.0148 lr: 0.02\n",
            "iteration: 37890 loss: 0.0153 lr: 0.02\n",
            "iteration: 37900 loss: 0.0098 lr: 0.02\n",
            "iteration: 37910 loss: 0.0146 lr: 0.02\n",
            "iteration: 37920 loss: 0.0153 lr: 0.02\n",
            "iteration: 37930 loss: 0.0250 lr: 0.02\n",
            "iteration: 37940 loss: 0.0149 lr: 0.02\n",
            "iteration: 37950 loss: 0.0155 lr: 0.02\n",
            "iteration: 37960 loss: 0.0152 lr: 0.02\n",
            "iteration: 37970 loss: 0.0180 lr: 0.02\n",
            "iteration: 37980 loss: 0.0140 lr: 0.02\n",
            "iteration: 37990 loss: 0.0129 lr: 0.02\n",
            "iteration: 38000 loss: 0.0144 lr: 0.02\n",
            "iteration: 38010 loss: 0.0154 lr: 0.02\n",
            "iteration: 38020 loss: 0.0160 lr: 0.02\n",
            "iteration: 38030 loss: 0.0214 lr: 0.02\n",
            "iteration: 38040 loss: 0.0145 lr: 0.02\n",
            "iteration: 38050 loss: 0.0097 lr: 0.02\n",
            "iteration: 38060 loss: 0.0135 lr: 0.02\n",
            "iteration: 38070 loss: 0.0131 lr: 0.02\n",
            "iteration: 38080 loss: 0.0144 lr: 0.02\n",
            "iteration: 38090 loss: 0.0118 lr: 0.02\n",
            "iteration: 38100 loss: 0.0173 lr: 0.02\n",
            "iteration: 38110 loss: 0.0194 lr: 0.02\n",
            "iteration: 38120 loss: 0.0162 lr: 0.02\n",
            "iteration: 38130 loss: 0.0159 lr: 0.02\n",
            "iteration: 38140 loss: 0.0124 lr: 0.02\n",
            "iteration: 38150 loss: 0.0183 lr: 0.02\n",
            "iteration: 38160 loss: 0.0193 lr: 0.02\n",
            "iteration: 38170 loss: 0.0138 lr: 0.02\n",
            "iteration: 38180 loss: 0.0126 lr: 0.02\n",
            "iteration: 38190 loss: 0.0114 lr: 0.02\n",
            "iteration: 38200 loss: 0.0111 lr: 0.02\n",
            "iteration: 38210 loss: 0.0139 lr: 0.02\n",
            "iteration: 38220 loss: 0.0138 lr: 0.02\n",
            "iteration: 38230 loss: 0.0276 lr: 0.02\n",
            "iteration: 38240 loss: 0.0170 lr: 0.02\n",
            "iteration: 38250 loss: 0.0286 lr: 0.02\n",
            "iteration: 38260 loss: 0.0137 lr: 0.02\n",
            "iteration: 38270 loss: 0.0129 lr: 0.02\n",
            "iteration: 38280 loss: 0.0159 lr: 0.02\n",
            "iteration: 38290 loss: 0.0137 lr: 0.02\n",
            "iteration: 38300 loss: 0.0175 lr: 0.02\n",
            "iteration: 38310 loss: 0.0123 lr: 0.02\n",
            "iteration: 38320 loss: 0.0132 lr: 0.02\n",
            "iteration: 38330 loss: 0.0122 lr: 0.02\n",
            "iteration: 38340 loss: 0.0127 lr: 0.02\n",
            "iteration: 38350 loss: 0.0153 lr: 0.02\n",
            "iteration: 38360 loss: 0.0157 lr: 0.02\n",
            "iteration: 38370 loss: 0.0086 lr: 0.02\n",
            "iteration: 38380 loss: 0.0140 lr: 0.02\n",
            "iteration: 38390 loss: 0.0195 lr: 0.02\n",
            "iteration: 38400 loss: 0.0130 lr: 0.02\n",
            "iteration: 38410 loss: 0.0082 lr: 0.02\n",
            "iteration: 38420 loss: 0.0125 lr: 0.02\n",
            "iteration: 38430 loss: 0.0168 lr: 0.02\n",
            "iteration: 38440 loss: 0.0146 lr: 0.02\n",
            "iteration: 38450 loss: 0.0188 lr: 0.02\n",
            "iteration: 38460 loss: 0.0146 lr: 0.02\n",
            "iteration: 38470 loss: 0.0191 lr: 0.02\n",
            "iteration: 38480 loss: 0.0112 lr: 0.02\n",
            "iteration: 38490 loss: 0.0207 lr: 0.02\n",
            "iteration: 38500 loss: 0.0143 lr: 0.02\n",
            "iteration: 38510 loss: 0.0142 lr: 0.02\n",
            "iteration: 38520 loss: 0.0196 lr: 0.02\n",
            "iteration: 38530 loss: 0.0174 lr: 0.02\n",
            "iteration: 38540 loss: 0.0147 lr: 0.02\n",
            "iteration: 38550 loss: 0.0174 lr: 0.02\n",
            "iteration: 38560 loss: 0.0210 lr: 0.02\n",
            "iteration: 38570 loss: 0.0145 lr: 0.02\n",
            "iteration: 38580 loss: 0.0142 lr: 0.02\n",
            "iteration: 38590 loss: 0.0133 lr: 0.02\n",
            "iteration: 38600 loss: 0.0198 lr: 0.02\n",
            "iteration: 38610 loss: 0.0130 lr: 0.02\n",
            "iteration: 38620 loss: 0.0240 lr: 0.02\n",
            "iteration: 38630 loss: 0.0157 lr: 0.02\n",
            "iteration: 38640 loss: 0.0157 lr: 0.02\n",
            "iteration: 38650 loss: 0.0228 lr: 0.02\n",
            "iteration: 38660 loss: 0.0196 lr: 0.02\n",
            "iteration: 38670 loss: 0.0160 lr: 0.02\n",
            "iteration: 38680 loss: 0.0201 lr: 0.02\n",
            "iteration: 38690 loss: 0.0128 lr: 0.02\n",
            "iteration: 38700 loss: 0.0213 lr: 0.02\n",
            "iteration: 38710 loss: 0.0168 lr: 0.02\n",
            "iteration: 38720 loss: 0.0128 lr: 0.02\n",
            "iteration: 38730 loss: 0.0189 lr: 0.02\n",
            "iteration: 38740 loss: 0.0187 lr: 0.02\n",
            "iteration: 38750 loss: 0.0137 lr: 0.02\n",
            "iteration: 38760 loss: 0.0135 lr: 0.02\n",
            "iteration: 38770 loss: 0.0187 lr: 0.02\n",
            "iteration: 38780 loss: 0.0164 lr: 0.02\n",
            "iteration: 38790 loss: 0.0084 lr: 0.02\n",
            "iteration: 38800 loss: 0.0222 lr: 0.02\n",
            "iteration: 38810 loss: 0.0137 lr: 0.02\n",
            "iteration: 38820 loss: 0.0198 lr: 0.02\n",
            "iteration: 38830 loss: 0.0152 lr: 0.02\n",
            "iteration: 38840 loss: 0.0120 lr: 0.02\n",
            "iteration: 38850 loss: 0.0138 lr: 0.02\n",
            "iteration: 38860 loss: 0.0181 lr: 0.02\n",
            "iteration: 38870 loss: 0.0243 lr: 0.02\n",
            "iteration: 38880 loss: 0.0116 lr: 0.02\n",
            "iteration: 38890 loss: 0.0154 lr: 0.02\n",
            "iteration: 38900 loss: 0.0160 lr: 0.02\n",
            "iteration: 38910 loss: 0.0186 lr: 0.02\n",
            "iteration: 38920 loss: 0.0260 lr: 0.02\n",
            "iteration: 38930 loss: 0.0107 lr: 0.02\n",
            "iteration: 38940 loss: 0.0157 lr: 0.02\n",
            "iteration: 38950 loss: 0.0142 lr: 0.02\n",
            "iteration: 38960 loss: 0.0111 lr: 0.02\n",
            "iteration: 38970 loss: 0.0198 lr: 0.02\n",
            "iteration: 38980 loss: 0.0103 lr: 0.02\n",
            "iteration: 38990 loss: 0.0136 lr: 0.02\n",
            "iteration: 39000 loss: 0.0186 lr: 0.02\n",
            "iteration: 39010 loss: 0.0120 lr: 0.02\n",
            "iteration: 39020 loss: 0.0159 lr: 0.02\n",
            "iteration: 39030 loss: 0.0159 lr: 0.02\n",
            "iteration: 39040 loss: 0.0169 lr: 0.02\n",
            "iteration: 39050 loss: 0.0143 lr: 0.02\n",
            "iteration: 39060 loss: 0.0134 lr: 0.02\n",
            "iteration: 39070 loss: 0.0157 lr: 0.02\n",
            "iteration: 39080 loss: 0.0155 lr: 0.02\n",
            "iteration: 39090 loss: 0.0139 lr: 0.02\n",
            "iteration: 39100 loss: 0.0157 lr: 0.02\n",
            "iteration: 39110 loss: 0.0183 lr: 0.02\n",
            "iteration: 39120 loss: 0.0207 lr: 0.02\n",
            "iteration: 39130 loss: 0.0128 lr: 0.02\n",
            "iteration: 39140 loss: 0.0173 lr: 0.02\n",
            "iteration: 39150 loss: 0.0123 lr: 0.02\n",
            "iteration: 39160 loss: 0.0196 lr: 0.02\n",
            "iteration: 39170 loss: 0.0140 lr: 0.02\n",
            "iteration: 39180 loss: 0.0163 lr: 0.02\n",
            "iteration: 39190 loss: 0.0108 lr: 0.02\n",
            "iteration: 39200 loss: 0.0176 lr: 0.02\n",
            "iteration: 39210 loss: 0.0139 lr: 0.02\n",
            "iteration: 39220 loss: 0.0187 lr: 0.02\n",
            "iteration: 39230 loss: 0.0175 lr: 0.02\n",
            "iteration: 39240 loss: 0.0262 lr: 0.02\n",
            "iteration: 39250 loss: 0.0138 lr: 0.02\n",
            "iteration: 39260 loss: 0.0123 lr: 0.02\n",
            "iteration: 39270 loss: 0.0193 lr: 0.02\n",
            "iteration: 39280 loss: 0.0172 lr: 0.02\n",
            "iteration: 39290 loss: 0.0228 lr: 0.02\n",
            "iteration: 39300 loss: 0.0165 lr: 0.02\n",
            "iteration: 39310 loss: 0.0119 lr: 0.02\n",
            "iteration: 39320 loss: 0.0181 lr: 0.02\n",
            "iteration: 39330 loss: 0.0157 lr: 0.02\n",
            "iteration: 39340 loss: 0.0134 lr: 0.02\n",
            "iteration: 39350 loss: 0.0194 lr: 0.02\n",
            "iteration: 39360 loss: 0.0158 lr: 0.02\n",
            "iteration: 39370 loss: 0.0150 lr: 0.02\n",
            "iteration: 39380 loss: 0.0132 lr: 0.02\n",
            "iteration: 39390 loss: 0.0188 lr: 0.02\n",
            "iteration: 39400 loss: 0.0156 lr: 0.02\n",
            "iteration: 39410 loss: 0.0254 lr: 0.02\n",
            "iteration: 39420 loss: 0.0228 lr: 0.02\n",
            "iteration: 39430 loss: 0.0136 lr: 0.02\n",
            "iteration: 39440 loss: 0.0213 lr: 0.02\n",
            "iteration: 39450 loss: 0.0149 lr: 0.02\n",
            "iteration: 39460 loss: 0.0142 lr: 0.02\n",
            "iteration: 39470 loss: 0.0209 lr: 0.02\n",
            "iteration: 39480 loss: 0.0127 lr: 0.02\n",
            "iteration: 39490 loss: 0.0244 lr: 0.02\n",
            "iteration: 39500 loss: 0.0177 lr: 0.02\n",
            "iteration: 39510 loss: 0.0210 lr: 0.02\n",
            "iteration: 39520 loss: 0.0182 lr: 0.02\n",
            "iteration: 39530 loss: 0.0212 lr: 0.02\n",
            "iteration: 39540 loss: 0.0140 lr: 0.02\n",
            "iteration: 39550 loss: 0.0137 lr: 0.02\n",
            "iteration: 39560 loss: 0.0160 lr: 0.02\n",
            "iteration: 39570 loss: 0.0142 lr: 0.02\n",
            "iteration: 39580 loss: 0.0164 lr: 0.02\n",
            "iteration: 39590 loss: 0.0135 lr: 0.02\n",
            "iteration: 39600 loss: 0.0131 lr: 0.02\n",
            "iteration: 39610 loss: 0.0107 lr: 0.02\n",
            "iteration: 39620 loss: 0.0131 lr: 0.02\n",
            "iteration: 39630 loss: 0.0193 lr: 0.02\n",
            "iteration: 39640 loss: 0.0195 lr: 0.02\n",
            "iteration: 39650 loss: 0.0153 lr: 0.02\n",
            "iteration: 39660 loss: 0.0127 lr: 0.02\n",
            "iteration: 39670 loss: 0.0124 lr: 0.02\n",
            "iteration: 39680 loss: 0.0171 lr: 0.02\n",
            "iteration: 39690 loss: 0.0169 lr: 0.02\n",
            "iteration: 39700 loss: 0.0139 lr: 0.02\n",
            "iteration: 39710 loss: 0.0140 lr: 0.02\n",
            "iteration: 39720 loss: 0.0183 lr: 0.02\n",
            "iteration: 39730 loss: 0.0149 lr: 0.02\n",
            "iteration: 39740 loss: 0.0143 lr: 0.02\n",
            "iteration: 39750 loss: 0.0143 lr: 0.02\n",
            "iteration: 39760 loss: 0.0130 lr: 0.02\n",
            "iteration: 39770 loss: 0.0136 lr: 0.02\n",
            "iteration: 39780 loss: 0.0169 lr: 0.02\n",
            "iteration: 39790 loss: 0.0162 lr: 0.02\n",
            "iteration: 39800 loss: 0.0150 lr: 0.02\n",
            "iteration: 39810 loss: 0.0157 lr: 0.02\n",
            "iteration: 39820 loss: 0.0162 lr: 0.02\n",
            "iteration: 39830 loss: 0.0118 lr: 0.02\n",
            "iteration: 39840 loss: 0.0172 lr: 0.02\n",
            "iteration: 39850 loss: 0.0103 lr: 0.02\n",
            "iteration: 39860 loss: 0.0144 lr: 0.02\n",
            "iteration: 39870 loss: 0.0147 lr: 0.02\n",
            "iteration: 39880 loss: 0.0163 lr: 0.02\n",
            "iteration: 39890 loss: 0.0129 lr: 0.02\n",
            "iteration: 39900 loss: 0.0136 lr: 0.02\n",
            "iteration: 39910 loss: 0.0147 lr: 0.02\n",
            "iteration: 39920 loss: 0.0128 lr: 0.02\n",
            "iteration: 39930 loss: 0.0171 lr: 0.02\n",
            "iteration: 39940 loss: 0.0151 lr: 0.02\n",
            "iteration: 39950 loss: 0.0160 lr: 0.02\n",
            "iteration: 39960 loss: 0.0130 lr: 0.02\n",
            "iteration: 39970 loss: 0.0139 lr: 0.02\n",
            "iteration: 39980 loss: 0.0296 lr: 0.02\n",
            "iteration: 39990 loss: 0.0112 lr: 0.02\n",
            "iteration: 40000 loss: 0.0188 lr: 0.02\n",
            "iteration: 40010 loss: 0.0225 lr: 0.02\n",
            "iteration: 40020 loss: 0.0222 lr: 0.02\n",
            "iteration: 40030 loss: 0.0223 lr: 0.02\n",
            "iteration: 40040 loss: 0.0138 lr: 0.02\n",
            "iteration: 40050 loss: 0.0130 lr: 0.02\n",
            "iteration: 40060 loss: 0.0173 lr: 0.02\n",
            "iteration: 40070 loss: 0.0166 lr: 0.02\n",
            "iteration: 40080 loss: 0.0223 lr: 0.02\n",
            "iteration: 40090 loss: 0.0153 lr: 0.02\n",
            "iteration: 40100 loss: 0.0163 lr: 0.02\n",
            "iteration: 40110 loss: 0.0149 lr: 0.02\n",
            "iteration: 40120 loss: 0.0170 lr: 0.02\n",
            "iteration: 40130 loss: 0.0185 lr: 0.02\n",
            "iteration: 40140 loss: 0.0150 lr: 0.02\n",
            "iteration: 40150 loss: 0.0115 lr: 0.02\n",
            "iteration: 40160 loss: 0.0163 lr: 0.02\n",
            "iteration: 40170 loss: 0.0151 lr: 0.02\n",
            "iteration: 40180 loss: 0.0180 lr: 0.02\n",
            "iteration: 40190 loss: 0.0130 lr: 0.02\n",
            "iteration: 40200 loss: 0.0121 lr: 0.02\n",
            "iteration: 40210 loss: 0.0166 lr: 0.02\n",
            "iteration: 40220 loss: 0.0163 lr: 0.02\n",
            "iteration: 40230 loss: 0.0157 lr: 0.02\n",
            "iteration: 40240 loss: 0.0215 lr: 0.02\n",
            "iteration: 40250 loss: 0.0184 lr: 0.02\n",
            "iteration: 40260 loss: 0.0144 lr: 0.02\n",
            "iteration: 40270 loss: 0.0149 lr: 0.02\n",
            "iteration: 40280 loss: 0.0143 lr: 0.02\n",
            "iteration: 40290 loss: 0.0255 lr: 0.02\n",
            "iteration: 40300 loss: 0.0150 lr: 0.02\n",
            "iteration: 40310 loss: 0.0214 lr: 0.02\n",
            "iteration: 40320 loss: 0.0114 lr: 0.02\n",
            "iteration: 40330 loss: 0.0126 lr: 0.02\n",
            "iteration: 40340 loss: 0.0151 lr: 0.02\n",
            "iteration: 40350 loss: 0.0167 lr: 0.02\n",
            "iteration: 40360 loss: 0.0099 lr: 0.02\n",
            "iteration: 40370 loss: 0.0193 lr: 0.02\n",
            "iteration: 40380 loss: 0.0166 lr: 0.02\n",
            "iteration: 40390 loss: 0.0138 lr: 0.02\n",
            "iteration: 40400 loss: 0.0082 lr: 0.02\n",
            "iteration: 40410 loss: 0.0143 lr: 0.02\n",
            "iteration: 40420 loss: 0.0135 lr: 0.02\n",
            "iteration: 40430 loss: 0.0182 lr: 0.02\n",
            "iteration: 40440 loss: 0.0157 lr: 0.02\n",
            "iteration: 40450 loss: 0.0128 lr: 0.02\n",
            "iteration: 40460 loss: 0.0149 lr: 0.02\n",
            "iteration: 40470 loss: 0.0125 lr: 0.02\n",
            "iteration: 40480 loss: 0.0178 lr: 0.02\n",
            "iteration: 40490 loss: 0.0185 lr: 0.02\n",
            "iteration: 40500 loss: 0.0177 lr: 0.02\n",
            "iteration: 40510 loss: 0.0158 lr: 0.02\n",
            "iteration: 40520 loss: 0.0091 lr: 0.02\n",
            "iteration: 40530 loss: 0.0177 lr: 0.02\n",
            "iteration: 40540 loss: 0.0106 lr: 0.02\n",
            "iteration: 40550 loss: 0.0206 lr: 0.02\n",
            "iteration: 40560 loss: 0.0164 lr: 0.02\n",
            "iteration: 40570 loss: 0.0150 lr: 0.02\n",
            "iteration: 40580 loss: 0.0117 lr: 0.02\n",
            "iteration: 40590 loss: 0.0198 lr: 0.02\n",
            "iteration: 40600 loss: 0.0169 lr: 0.02\n",
            "iteration: 40610 loss: 0.0184 lr: 0.02\n",
            "iteration: 40620 loss: 0.0166 lr: 0.02\n",
            "iteration: 40630 loss: 0.0290 lr: 0.02\n",
            "iteration: 40640 loss: 0.0150 lr: 0.02\n",
            "iteration: 40650 loss: 0.0134 lr: 0.02\n",
            "iteration: 40660 loss: 0.0146 lr: 0.02\n",
            "iteration: 40670 loss: 0.0181 lr: 0.02\n",
            "iteration: 40680 loss: 0.0116 lr: 0.02\n",
            "iteration: 40690 loss: 0.0149 lr: 0.02\n",
            "iteration: 40700 loss: 0.0110 lr: 0.02\n",
            "iteration: 40710 loss: 0.0161 lr: 0.02\n",
            "iteration: 40720 loss: 0.0163 lr: 0.02\n",
            "iteration: 40730 loss: 0.0170 lr: 0.02\n",
            "iteration: 40740 loss: 0.0194 lr: 0.02\n",
            "iteration: 40750 loss: 0.0174 lr: 0.02\n",
            "iteration: 40760 loss: 0.0135 lr: 0.02\n",
            "iteration: 40770 loss: 0.0190 lr: 0.02\n",
            "iteration: 40780 loss: 0.0115 lr: 0.02\n",
            "iteration: 40790 loss: 0.0211 lr: 0.02\n",
            "iteration: 40800 loss: 0.0155 lr: 0.02\n",
            "iteration: 40810 loss: 0.0216 lr: 0.02\n",
            "iteration: 40820 loss: 0.0149 lr: 0.02\n",
            "iteration: 40830 loss: 0.0253 lr: 0.02\n",
            "iteration: 40840 loss: 0.0189 lr: 0.02\n",
            "iteration: 40850 loss: 0.0123 lr: 0.02\n",
            "iteration: 40860 loss: 0.0253 lr: 0.02\n",
            "iteration: 40870 loss: 0.0148 lr: 0.02\n",
            "iteration: 40880 loss: 0.0136 lr: 0.02\n",
            "iteration: 40890 loss: 0.0247 lr: 0.02\n",
            "iteration: 40900 loss: 0.0126 lr: 0.02\n",
            "iteration: 40910 loss: 0.0184 lr: 0.02\n",
            "iteration: 40920 loss: 0.0271 lr: 0.02\n",
            "iteration: 40930 loss: 0.0135 lr: 0.02\n",
            "iteration: 40940 loss: 0.0176 lr: 0.02\n",
            "iteration: 40950 loss: 0.0153 lr: 0.02\n",
            "iteration: 40960 loss: 0.0101 lr: 0.02\n",
            "iteration: 40970 loss: 0.0164 lr: 0.02\n",
            "iteration: 40980 loss: 0.0224 lr: 0.02\n",
            "iteration: 40990 loss: 0.0208 lr: 0.02\n",
            "iteration: 41000 loss: 0.0136 lr: 0.02\n",
            "iteration: 41010 loss: 0.0120 lr: 0.02\n",
            "iteration: 41020 loss: 0.0128 lr: 0.02\n",
            "iteration: 41030 loss: 0.0119 lr: 0.02\n",
            "iteration: 41040 loss: 0.0170 lr: 0.02\n",
            "iteration: 41050 loss: 0.0154 lr: 0.02\n",
            "iteration: 41060 loss: 0.0188 lr: 0.02\n",
            "iteration: 41070 loss: 0.0121 lr: 0.02\n",
            "iteration: 41080 loss: 0.0178 lr: 0.02\n",
            "iteration: 41090 loss: 0.0136 lr: 0.02\n",
            "iteration: 41100 loss: 0.0157 lr: 0.02\n",
            "iteration: 41110 loss: 0.0129 lr: 0.02\n",
            "iteration: 41120 loss: 0.0260 lr: 0.02\n",
            "iteration: 41130 loss: 0.0144 lr: 0.02\n",
            "iteration: 41140 loss: 0.0169 lr: 0.02\n",
            "iteration: 41150 loss: 0.0128 lr: 0.02\n",
            "iteration: 41160 loss: 0.0192 lr: 0.02\n",
            "iteration: 41170 loss: 0.0194 lr: 0.02\n",
            "iteration: 41180 loss: 0.0240 lr: 0.02\n",
            "iteration: 41190 loss: 0.0181 lr: 0.02\n",
            "iteration: 41200 loss: 0.0105 lr: 0.02\n",
            "iteration: 41210 loss: 0.0128 lr: 0.02\n",
            "iteration: 41220 loss: 0.0120 lr: 0.02\n",
            "iteration: 41230 loss: 0.0178 lr: 0.02\n",
            "iteration: 41240 loss: 0.0137 lr: 0.02\n",
            "iteration: 41250 loss: 0.0148 lr: 0.02\n",
            "iteration: 41260 loss: 0.0130 lr: 0.02\n",
            "iteration: 41270 loss: 0.0179 lr: 0.02\n",
            "iteration: 41280 loss: 0.0151 lr: 0.02\n",
            "iteration: 41290 loss: 0.0140 lr: 0.02\n",
            "iteration: 41300 loss: 0.0133 lr: 0.02\n",
            "iteration: 41310 loss: 0.0137 lr: 0.02\n",
            "iteration: 41320 loss: 0.0130 lr: 0.02\n",
            "iteration: 41330 loss: 0.0242 lr: 0.02\n",
            "iteration: 41340 loss: 0.0163 lr: 0.02\n",
            "iteration: 41350 loss: 0.0137 lr: 0.02\n",
            "iteration: 41360 loss: 0.0102 lr: 0.02\n",
            "iteration: 41370 loss: 0.0137 lr: 0.02\n",
            "iteration: 41380 loss: 0.0104 lr: 0.02\n",
            "iteration: 41390 loss: 0.0161 lr: 0.02\n",
            "iteration: 41400 loss: 0.0153 lr: 0.02\n",
            "iteration: 41410 loss: 0.0173 lr: 0.02\n",
            "iteration: 41420 loss: 0.0270 lr: 0.02\n",
            "iteration: 41430 loss: 0.0170 lr: 0.02\n",
            "iteration: 41440 loss: 0.0167 lr: 0.02\n",
            "iteration: 41450 loss: 0.0216 lr: 0.02\n",
            "iteration: 41460 loss: 0.0294 lr: 0.02\n",
            "iteration: 41470 loss: 0.0227 lr: 0.02\n",
            "iteration: 41480 loss: 0.0145 lr: 0.02\n",
            "iteration: 41490 loss: 0.0106 lr: 0.02\n",
            "iteration: 41500 loss: 0.0223 lr: 0.02\n",
            "iteration: 41510 loss: 0.0177 lr: 0.02\n",
            "iteration: 41520 loss: 0.0109 lr: 0.02\n",
            "iteration: 41530 loss: 0.0129 lr: 0.02\n",
            "iteration: 41540 loss: 0.0196 lr: 0.02\n",
            "iteration: 41550 loss: 0.0117 lr: 0.02\n",
            "iteration: 41560 loss: 0.0165 lr: 0.02\n",
            "iteration: 41570 loss: 0.0154 lr: 0.02\n",
            "iteration: 41580 loss: 0.0121 lr: 0.02\n",
            "iteration: 41590 loss: 0.0202 lr: 0.02\n",
            "iteration: 41600 loss: 0.0150 lr: 0.02\n",
            "iteration: 41610 loss: 0.0123 lr: 0.02\n",
            "iteration: 41620 loss: 0.0151 lr: 0.02\n",
            "iteration: 41630 loss: 0.0134 lr: 0.02\n",
            "iteration: 41640 loss: 0.0162 lr: 0.02\n",
            "iteration: 41650 loss: 0.0143 lr: 0.02\n",
            "iteration: 41660 loss: 0.0141 lr: 0.02\n",
            "iteration: 41670 loss: 0.0139 lr: 0.02\n",
            "iteration: 41680 loss: 0.0159 lr: 0.02\n",
            "iteration: 41690 loss: 0.0177 lr: 0.02\n",
            "iteration: 41700 loss: 0.0220 lr: 0.02\n",
            "iteration: 41710 loss: 0.0195 lr: 0.02\n",
            "iteration: 41720 loss: 0.0123 lr: 0.02\n",
            "iteration: 41730 loss: 0.0154 lr: 0.02\n",
            "iteration: 41740 loss: 0.0111 lr: 0.02\n",
            "iteration: 41750 loss: 0.0130 lr: 0.02\n",
            "iteration: 41760 loss: 0.0145 lr: 0.02\n",
            "iteration: 41770 loss: 0.0154 lr: 0.02\n",
            "iteration: 41780 loss: 0.0211 lr: 0.02\n",
            "iteration: 41790 loss: 0.0162 lr: 0.02\n",
            "iteration: 41800 loss: 0.0211 lr: 0.02\n",
            "iteration: 41810 loss: 0.0146 lr: 0.02\n",
            "iteration: 41820 loss: 0.0105 lr: 0.02\n",
            "iteration: 41830 loss: 0.0170 lr: 0.02\n",
            "iteration: 41840 loss: 0.0188 lr: 0.02\n",
            "iteration: 41850 loss: 0.0146 lr: 0.02\n",
            "iteration: 41860 loss: 0.0178 lr: 0.02\n",
            "iteration: 41870 loss: 0.0193 lr: 0.02\n",
            "iteration: 41880 loss: 0.0236 lr: 0.02\n",
            "iteration: 41890 loss: 0.0214 lr: 0.02\n",
            "iteration: 41900 loss: 0.0214 lr: 0.02\n",
            "iteration: 41910 loss: 0.0181 lr: 0.02\n",
            "iteration: 41920 loss: 0.0140 lr: 0.02\n",
            "iteration: 41930 loss: 0.0140 lr: 0.02\n",
            "iteration: 41940 loss: 0.0135 lr: 0.02\n",
            "iteration: 41950 loss: 0.0115 lr: 0.02\n",
            "iteration: 41960 loss: 0.0148 lr: 0.02\n",
            "iteration: 41970 loss: 0.0137 lr: 0.02\n",
            "iteration: 41980 loss: 0.0143 lr: 0.02\n",
            "iteration: 41990 loss: 0.0193 lr: 0.02\n",
            "iteration: 42000 loss: 0.0201 lr: 0.02\n",
            "iteration: 42010 loss: 0.0171 lr: 0.02\n",
            "iteration: 42020 loss: 0.0145 lr: 0.02\n",
            "iteration: 42030 loss: 0.0248 lr: 0.02\n",
            "iteration: 42040 loss: 0.0137 lr: 0.02\n",
            "iteration: 42050 loss: 0.0154 lr: 0.02\n",
            "iteration: 42060 loss: 0.0176 lr: 0.02\n",
            "iteration: 42070 loss: 0.0145 lr: 0.02\n",
            "iteration: 42080 loss: 0.0100 lr: 0.02\n",
            "iteration: 42090 loss: 0.0159 lr: 0.02\n",
            "iteration: 42100 loss: 0.0149 lr: 0.02\n",
            "iteration: 42110 loss: 0.0153 lr: 0.02\n",
            "iteration: 42120 loss: 0.0200 lr: 0.02\n",
            "iteration: 42130 loss: 0.0152 lr: 0.02\n",
            "iteration: 42140 loss: 0.0161 lr: 0.02\n",
            "iteration: 42150 loss: 0.0148 lr: 0.02\n",
            "iteration: 42160 loss: 0.0201 lr: 0.02\n",
            "iteration: 42170 loss: 0.0171 lr: 0.02\n",
            "iteration: 42180 loss: 0.0124 lr: 0.02\n",
            "iteration: 42190 loss: 0.0118 lr: 0.02\n",
            "iteration: 42200 loss: 0.0143 lr: 0.02\n",
            "iteration: 42210 loss: 0.0141 lr: 0.02\n",
            "iteration: 42220 loss: 0.0136 lr: 0.02\n",
            "iteration: 42230 loss: 0.0109 lr: 0.02\n",
            "iteration: 42240 loss: 0.0069 lr: 0.02\n",
            "iteration: 42250 loss: 0.0140 lr: 0.02\n",
            "iteration: 42260 loss: 0.0159 lr: 0.02\n",
            "iteration: 42270 loss: 0.0099 lr: 0.02\n",
            "iteration: 42280 loss: 0.0214 lr: 0.02\n",
            "iteration: 42290 loss: 0.0191 lr: 0.02\n",
            "iteration: 42300 loss: 0.0144 lr: 0.02\n",
            "iteration: 42310 loss: 0.0091 lr: 0.02\n",
            "iteration: 42320 loss: 0.0169 lr: 0.02\n",
            "iteration: 42330 loss: 0.0130 lr: 0.02\n",
            "iteration: 42340 loss: 0.0150 lr: 0.02\n",
            "iteration: 42350 loss: 0.0168 lr: 0.02\n",
            "iteration: 42360 loss: 0.0120 lr: 0.02\n",
            "iteration: 42370 loss: 0.0241 lr: 0.02\n",
            "iteration: 42380 loss: 0.0125 lr: 0.02\n",
            "iteration: 42390 loss: 0.0141 lr: 0.02\n",
            "iteration: 42400 loss: 0.0172 lr: 0.02\n",
            "iteration: 42410 loss: 0.0203 lr: 0.02\n",
            "iteration: 42420 loss: 0.0141 lr: 0.02\n",
            "iteration: 42430 loss: 0.0170 lr: 0.02\n",
            "iteration: 42440 loss: 0.0173 lr: 0.02\n",
            "iteration: 42450 loss: 0.0144 lr: 0.02\n",
            "iteration: 42460 loss: 0.0137 lr: 0.02\n",
            "iteration: 42470 loss: 0.0157 lr: 0.02\n",
            "iteration: 42480 loss: 0.0120 lr: 0.02\n",
            "iteration: 42490 loss: 0.0188 lr: 0.02\n",
            "iteration: 42500 loss: 0.0147 lr: 0.02\n",
            "iteration: 42510 loss: 0.0149 lr: 0.02\n",
            "iteration: 42520 loss: 0.0140 lr: 0.02\n",
            "iteration: 42530 loss: 0.0244 lr: 0.02\n",
            "iteration: 42540 loss: 0.0158 lr: 0.02\n",
            "iteration: 42550 loss: 0.0162 lr: 0.02\n",
            "iteration: 42560 loss: 0.0130 lr: 0.02\n",
            "iteration: 42570 loss: 0.0136 lr: 0.02\n",
            "iteration: 42580 loss: 0.0178 lr: 0.02\n",
            "iteration: 42590 loss: 0.0179 lr: 0.02\n",
            "iteration: 42600 loss: 0.0167 lr: 0.02\n",
            "iteration: 42610 loss: 0.0120 lr: 0.02\n",
            "iteration: 42620 loss: 0.0130 lr: 0.02\n",
            "iteration: 42630 loss: 0.0152 lr: 0.02\n",
            "iteration: 42640 loss: 0.0130 lr: 0.02\n",
            "iteration: 42650 loss: 0.0123 lr: 0.02\n",
            "iteration: 42660 loss: 0.0171 lr: 0.02\n",
            "iteration: 42670 loss: 0.0164 lr: 0.02\n",
            "iteration: 42680 loss: 0.0130 lr: 0.02\n",
            "iteration: 42690 loss: 0.0206 lr: 0.02\n",
            "iteration: 42700 loss: 0.0158 lr: 0.02\n",
            "iteration: 42710 loss: 0.0149 lr: 0.02\n",
            "iteration: 42720 loss: 0.0170 lr: 0.02\n",
            "iteration: 42730 loss: 0.0150 lr: 0.02\n",
            "iteration: 42740 loss: 0.0131 lr: 0.02\n",
            "iteration: 42750 loss: 0.0165 lr: 0.02\n",
            "iteration: 42760 loss: 0.0170 lr: 0.02\n",
            "iteration: 42770 loss: 0.0139 lr: 0.02\n",
            "iteration: 42780 loss: 0.0148 lr: 0.02\n",
            "iteration: 42790 loss: 0.0106 lr: 0.02\n",
            "iteration: 42800 loss: 0.0146 lr: 0.02\n",
            "iteration: 42810 loss: 0.0123 lr: 0.02\n",
            "iteration: 42820 loss: 0.0156 lr: 0.02\n",
            "iteration: 42830 loss: 0.0124 lr: 0.02\n",
            "iteration: 42840 loss: 0.0151 lr: 0.02\n",
            "iteration: 42850 loss: 0.0077 lr: 0.02\n",
            "iteration: 42860 loss: 0.0131 lr: 0.02\n",
            "iteration: 42870 loss: 0.0195 lr: 0.02\n",
            "iteration: 42880 loss: 0.0098 lr: 0.02\n",
            "iteration: 42890 loss: 0.0177 lr: 0.02\n",
            "iteration: 42900 loss: 0.0135 lr: 0.02\n",
            "iteration: 42910 loss: 0.0132 lr: 0.02\n",
            "iteration: 42920 loss: 0.0115 lr: 0.02\n",
            "iteration: 42930 loss: 0.0122 lr: 0.02\n",
            "iteration: 42940 loss: 0.0133 lr: 0.02\n",
            "iteration: 42950 loss: 0.0255 lr: 0.02\n",
            "iteration: 42960 loss: 0.0133 lr: 0.02\n",
            "iteration: 42970 loss: 0.0188 lr: 0.02\n",
            "iteration: 42980 loss: 0.0138 lr: 0.02\n",
            "iteration: 42990 loss: 0.0102 lr: 0.02\n",
            "iteration: 43000 loss: 0.0155 lr: 0.02\n",
            "iteration: 43010 loss: 0.0145 lr: 0.02\n",
            "iteration: 43020 loss: 0.0148 lr: 0.02\n",
            "iteration: 43030 loss: 0.0120 lr: 0.02\n",
            "iteration: 43040 loss: 0.0157 lr: 0.02\n",
            "iteration: 43050 loss: 0.0113 lr: 0.02\n",
            "iteration: 43060 loss: 0.0126 lr: 0.02\n",
            "iteration: 43070 loss: 0.0153 lr: 0.02\n",
            "iteration: 43080 loss: 0.0127 lr: 0.02\n",
            "iteration: 43090 loss: 0.0137 lr: 0.02\n",
            "iteration: 43100 loss: 0.0149 lr: 0.02\n",
            "iteration: 43110 loss: 0.0108 lr: 0.02\n",
            "iteration: 43120 loss: 0.0142 lr: 0.02\n",
            "iteration: 43130 loss: 0.0210 lr: 0.02\n",
            "iteration: 43140 loss: 0.0149 lr: 0.02\n",
            "iteration: 43150 loss: 0.0228 lr: 0.02\n",
            "iteration: 43160 loss: 0.0099 lr: 0.02\n",
            "iteration: 43170 loss: 0.0124 lr: 0.02\n",
            "iteration: 43180 loss: 0.0126 lr: 0.02\n",
            "iteration: 43190 loss: 0.0134 lr: 0.02\n",
            "iteration: 43200 loss: 0.0183 lr: 0.02\n",
            "iteration: 43210 loss: 0.0155 lr: 0.02\n",
            "iteration: 43220 loss: 0.0125 lr: 0.02\n",
            "iteration: 43230 loss: 0.0148 lr: 0.02\n",
            "iteration: 43240 loss: 0.0156 lr: 0.02\n",
            "iteration: 43250 loss: 0.0169 lr: 0.02\n",
            "iteration: 43260 loss: 0.0150 lr: 0.02\n",
            "iteration: 43270 loss: 0.0140 lr: 0.02\n",
            "iteration: 43280 loss: 0.0181 lr: 0.02\n",
            "iteration: 43290 loss: 0.0143 lr: 0.02\n",
            "iteration: 43300 loss: 0.0154 lr: 0.02\n",
            "iteration: 43310 loss: 0.0200 lr: 0.02\n",
            "iteration: 43320 loss: 0.0108 lr: 0.02\n",
            "iteration: 43330 loss: 0.0257 lr: 0.02\n",
            "iteration: 43340 loss: 0.0133 lr: 0.02\n",
            "iteration: 43350 loss: 0.0174 lr: 0.02\n",
            "iteration: 43360 loss: 0.0111 lr: 0.02\n",
            "iteration: 43370 loss: 0.0100 lr: 0.02\n",
            "iteration: 43380 loss: 0.0138 lr: 0.02\n",
            "iteration: 43390 loss: 0.0111 lr: 0.02\n",
            "iteration: 43400 loss: 0.0097 lr: 0.02\n",
            "iteration: 43410 loss: 0.0156 lr: 0.02\n",
            "iteration: 43420 loss: 0.0184 lr: 0.02\n",
            "iteration: 43430 loss: 0.0145 lr: 0.02\n",
            "iteration: 43440 loss: 0.0173 lr: 0.02\n",
            "iteration: 43450 loss: 0.0210 lr: 0.02\n",
            "iteration: 43460 loss: 0.0151 lr: 0.02\n",
            "iteration: 43470 loss: 0.0103 lr: 0.02\n",
            "iteration: 43480 loss: 0.0184 lr: 0.02\n",
            "iteration: 43490 loss: 0.0116 lr: 0.02\n",
            "iteration: 43500 loss: 0.0191 lr: 0.02\n",
            "iteration: 43510 loss: 0.0174 lr: 0.02\n",
            "iteration: 43520 loss: 0.0173 lr: 0.02\n",
            "iteration: 43530 loss: 0.0250 lr: 0.02\n",
            "iteration: 43540 loss: 0.0237 lr: 0.02\n",
            "iteration: 43550 loss: 0.0175 lr: 0.02\n",
            "iteration: 43560 loss: 0.0209 lr: 0.02\n",
            "iteration: 43570 loss: 0.0186 lr: 0.02\n",
            "iteration: 43580 loss: 0.0173 lr: 0.02\n",
            "iteration: 43590 loss: 0.0127 lr: 0.02\n",
            "iteration: 43600 loss: 0.0175 lr: 0.02\n",
            "iteration: 43610 loss: 0.0102 lr: 0.02\n",
            "iteration: 43620 loss: 0.0143 lr: 0.02\n",
            "iteration: 43630 loss: 0.0140 lr: 0.02\n",
            "iteration: 43640 loss: 0.0154 lr: 0.02\n",
            "iteration: 43650 loss: 0.0136 lr: 0.02\n",
            "iteration: 43660 loss: 0.0143 lr: 0.02\n",
            "iteration: 43670 loss: 0.0150 lr: 0.02\n",
            "iteration: 43680 loss: 0.0132 lr: 0.02\n",
            "iteration: 43690 loss: 0.0117 lr: 0.02\n",
            "iteration: 43700 loss: 0.0147 lr: 0.02\n",
            "iteration: 43710 loss: 0.0144 lr: 0.02\n",
            "iteration: 43720 loss: 0.0115 lr: 0.02\n",
            "iteration: 43730 loss: 0.0142 lr: 0.02\n",
            "iteration: 43740 loss: 0.0119 lr: 0.02\n",
            "iteration: 43750 loss: 0.0140 lr: 0.02\n",
            "iteration: 43760 loss: 0.0203 lr: 0.02\n",
            "iteration: 43770 loss: 0.0122 lr: 0.02\n",
            "iteration: 43780 loss: 0.0189 lr: 0.02\n",
            "iteration: 43790 loss: 0.0177 lr: 0.02\n",
            "iteration: 43800 loss: 0.0146 lr: 0.02\n",
            "iteration: 43810 loss: 0.0136 lr: 0.02\n",
            "iteration: 43820 loss: 0.0173 lr: 0.02\n",
            "iteration: 43830 loss: 0.0185 lr: 0.02\n",
            "iteration: 43840 loss: 0.0177 lr: 0.02\n",
            "iteration: 43850 loss: 0.0157 lr: 0.02\n",
            "iteration: 43860 loss: 0.0165 lr: 0.02\n",
            "iteration: 43870 loss: 0.0186 lr: 0.02\n",
            "iteration: 43880 loss: 0.0154 lr: 0.02\n",
            "iteration: 43890 loss: 0.0100 lr: 0.02\n",
            "iteration: 43900 loss: 0.0159 lr: 0.02\n",
            "iteration: 43910 loss: 0.0139 lr: 0.02\n",
            "iteration: 43920 loss: 0.0105 lr: 0.02\n",
            "iteration: 43930 loss: 0.0134 lr: 0.02\n",
            "iteration: 43940 loss: 0.0113 lr: 0.02\n",
            "iteration: 43950 loss: 0.0212 lr: 0.02\n",
            "iteration: 43960 loss: 0.0174 lr: 0.02\n",
            "iteration: 43970 loss: 0.0164 lr: 0.02\n",
            "iteration: 43980 loss: 0.0185 lr: 0.02\n",
            "iteration: 43990 loss: 0.0118 lr: 0.02\n",
            "iteration: 44000 loss: 0.0125 lr: 0.02\n",
            "iteration: 44010 loss: 0.0137 lr: 0.02\n",
            "iteration: 44020 loss: 0.0145 lr: 0.02\n",
            "iteration: 44030 loss: 0.0124 lr: 0.02\n",
            "iteration: 44040 loss: 0.0102 lr: 0.02\n",
            "iteration: 44050 loss: 0.0148 lr: 0.02\n",
            "iteration: 44060 loss: 0.0133 lr: 0.02\n",
            "iteration: 44070 loss: 0.0184 lr: 0.02\n",
            "iteration: 44080 loss: 0.0132 lr: 0.02\n",
            "iteration: 44090 loss: 0.0138 lr: 0.02\n",
            "iteration: 44100 loss: 0.0247 lr: 0.02\n",
            "iteration: 44110 loss: 0.0180 lr: 0.02\n",
            "iteration: 44120 loss: 0.0166 lr: 0.02\n",
            "iteration: 44130 loss: 0.0239 lr: 0.02\n",
            "iteration: 44140 loss: 0.0144 lr: 0.02\n",
            "iteration: 44150 loss: 0.0167 lr: 0.02\n",
            "iteration: 44160 loss: 0.0186 lr: 0.02\n",
            "iteration: 44170 loss: 0.0120 lr: 0.02\n",
            "iteration: 44180 loss: 0.0118 lr: 0.02\n",
            "iteration: 44190 loss: 0.0179 lr: 0.02\n",
            "iteration: 44200 loss: 0.0151 lr: 0.02\n",
            "iteration: 44210 loss: 0.0125 lr: 0.02\n",
            "iteration: 44220 loss: 0.0126 lr: 0.02\n",
            "iteration: 44230 loss: 0.0204 lr: 0.02\n",
            "iteration: 44240 loss: 0.0153 lr: 0.02\n",
            "iteration: 44250 loss: 0.0116 lr: 0.02\n",
            "iteration: 44260 loss: 0.0107 lr: 0.02\n",
            "iteration: 44270 loss: 0.0127 lr: 0.02\n",
            "iteration: 44280 loss: 0.0191 lr: 0.02\n",
            "iteration: 44290 loss: 0.0169 lr: 0.02\n",
            "iteration: 44300 loss: 0.0236 lr: 0.02\n",
            "iteration: 44310 loss: 0.0206 lr: 0.02\n",
            "iteration: 44320 loss: 0.0109 lr: 0.02\n",
            "iteration: 44330 loss: 0.0157 lr: 0.02\n",
            "iteration: 44340 loss: 0.0117 lr: 0.02\n",
            "iteration: 44350 loss: 0.0287 lr: 0.02\n",
            "iteration: 44360 loss: 0.0150 lr: 0.02\n",
            "iteration: 44370 loss: 0.0169 lr: 0.02\n",
            "iteration: 44380 loss: 0.0256 lr: 0.02\n",
            "iteration: 44390 loss: 0.0141 lr: 0.02\n",
            "iteration: 44400 loss: 0.0157 lr: 0.02\n",
            "iteration: 44410 loss: 0.0135 lr: 0.02\n",
            "iteration: 44420 loss: 0.0150 lr: 0.02\n",
            "iteration: 44430 loss: 0.0130 lr: 0.02\n",
            "iteration: 44440 loss: 0.0135 lr: 0.02\n",
            "iteration: 44450 loss: 0.0110 lr: 0.02\n",
            "iteration: 44460 loss: 0.0196 lr: 0.02\n",
            "iteration: 44470 loss: 0.0118 lr: 0.02\n",
            "iteration: 44480 loss: 0.0157 lr: 0.02\n",
            "iteration: 44490 loss: 0.0210 lr: 0.02\n",
            "iteration: 44500 loss: 0.0118 lr: 0.02\n",
            "iteration: 44510 loss: 0.0162 lr: 0.02\n",
            "iteration: 44520 loss: 0.0127 lr: 0.02\n",
            "iteration: 44530 loss: 0.0169 lr: 0.02\n",
            "iteration: 44540 loss: 0.0155 lr: 0.02\n",
            "iteration: 44550 loss: 0.0141 lr: 0.02\n",
            "iteration: 44560 loss: 0.0172 lr: 0.02\n",
            "iteration: 44570 loss: 0.0101 lr: 0.02\n",
            "iteration: 44580 loss: 0.0086 lr: 0.02\n",
            "iteration: 44590 loss: 0.0194 lr: 0.02\n",
            "iteration: 44600 loss: 0.0159 lr: 0.02\n",
            "iteration: 44610 loss: 0.0155 lr: 0.02\n",
            "iteration: 44620 loss: 0.0113 lr: 0.02\n",
            "iteration: 44630 loss: 0.0098 lr: 0.02\n",
            "iteration: 44640 loss: 0.0136 lr: 0.02\n",
            "iteration: 44650 loss: 0.0217 lr: 0.02\n",
            "iteration: 44660 loss: 0.0121 lr: 0.02\n",
            "iteration: 44670 loss: 0.0254 lr: 0.02\n",
            "iteration: 44680 loss: 0.0123 lr: 0.02\n",
            "iteration: 44690 loss: 0.0170 lr: 0.02\n",
            "iteration: 44700 loss: 0.0155 lr: 0.02\n",
            "iteration: 44710 loss: 0.0153 lr: 0.02\n",
            "iteration: 44720 loss: 0.0196 lr: 0.02\n",
            "iteration: 44730 loss: 0.0174 lr: 0.02\n",
            "iteration: 44740 loss: 0.0190 lr: 0.02\n",
            "iteration: 44750 loss: 0.0138 lr: 0.02\n",
            "iteration: 44760 loss: 0.0121 lr: 0.02\n",
            "iteration: 44770 loss: 0.0108 lr: 0.02\n",
            "iteration: 44780 loss: 0.0246 lr: 0.02\n",
            "iteration: 44790 loss: 0.0204 lr: 0.02\n",
            "iteration: 44800 loss: 0.0135 lr: 0.02\n",
            "iteration: 44810 loss: 0.0128 lr: 0.02\n",
            "iteration: 44820 loss: 0.0156 lr: 0.02\n",
            "iteration: 44830 loss: 0.0121 lr: 0.02\n",
            "iteration: 44840 loss: 0.0141 lr: 0.02\n",
            "iteration: 44850 loss: 0.0149 lr: 0.02\n",
            "iteration: 44860 loss: 0.0126 lr: 0.02\n",
            "iteration: 44870 loss: 0.0180 lr: 0.02\n",
            "iteration: 44880 loss: 0.0146 lr: 0.02\n",
            "iteration: 44890 loss: 0.0163 lr: 0.02\n",
            "iteration: 44900 loss: 0.0127 lr: 0.02\n",
            "iteration: 44910 loss: 0.0116 lr: 0.02\n",
            "iteration: 44920 loss: 0.0153 lr: 0.02\n",
            "iteration: 44930 loss: 0.0152 lr: 0.02\n",
            "iteration: 44940 loss: 0.0117 lr: 0.02\n",
            "iteration: 44950 loss: 0.0169 lr: 0.02\n",
            "iteration: 44960 loss: 0.0141 lr: 0.02\n",
            "iteration: 44970 loss: 0.0112 lr: 0.02\n",
            "iteration: 44980 loss: 0.0195 lr: 0.02\n",
            "iteration: 44990 loss: 0.0181 lr: 0.02\n",
            "iteration: 45000 loss: 0.0119 lr: 0.02\n",
            "iteration: 45010 loss: 0.0150 lr: 0.02\n",
            "iteration: 45020 loss: 0.0170 lr: 0.02\n",
            "iteration: 45030 loss: 0.0148 lr: 0.02\n",
            "iteration: 45040 loss: 0.0112 lr: 0.02\n",
            "iteration: 45050 loss: 0.0168 lr: 0.02\n",
            "iteration: 45060 loss: 0.0126 lr: 0.02\n",
            "iteration: 45070 loss: 0.0106 lr: 0.02\n",
            "iteration: 45080 loss: 0.0152 lr: 0.02\n",
            "iteration: 45090 loss: 0.0248 lr: 0.02\n",
            "iteration: 45100 loss: 0.0120 lr: 0.02\n",
            "iteration: 45110 loss: 0.0190 lr: 0.02\n",
            "iteration: 45120 loss: 0.0135 lr: 0.02\n",
            "iteration: 45130 loss: 0.0145 lr: 0.02\n",
            "iteration: 45140 loss: 0.0148 lr: 0.02\n",
            "iteration: 45150 loss: 0.0155 lr: 0.02\n",
            "iteration: 45160 loss: 0.0162 lr: 0.02\n",
            "iteration: 45170 loss: 0.0140 lr: 0.02\n",
            "iteration: 45180 loss: 0.0142 lr: 0.02\n",
            "iteration: 45190 loss: 0.0188 lr: 0.02\n",
            "iteration: 45200 loss: 0.0152 lr: 0.02\n",
            "iteration: 45210 loss: 0.0162 lr: 0.02\n",
            "iteration: 45220 loss: 0.0169 lr: 0.02\n",
            "iteration: 45230 loss: 0.0178 lr: 0.02\n",
            "iteration: 45240 loss: 0.0123 lr: 0.02\n",
            "iteration: 45250 loss: 0.0152 lr: 0.02\n",
            "iteration: 45260 loss: 0.0182 lr: 0.02\n",
            "iteration: 45270 loss: 0.0117 lr: 0.02\n",
            "iteration: 45280 loss: 0.0160 lr: 0.02\n",
            "iteration: 45290 loss: 0.0097 lr: 0.02\n",
            "iteration: 45300 loss: 0.0136 lr: 0.02\n",
            "iteration: 45310 loss: 0.0146 lr: 0.02\n",
            "iteration: 45320 loss: 0.0106 lr: 0.02\n",
            "iteration: 45330 loss: 0.0110 lr: 0.02\n",
            "iteration: 45340 loss: 0.0120 lr: 0.02\n",
            "iteration: 45350 loss: 0.0123 lr: 0.02\n",
            "iteration: 45360 loss: 0.0160 lr: 0.02\n",
            "iteration: 45370 loss: 0.0146 lr: 0.02\n",
            "iteration: 45380 loss: 0.0240 lr: 0.02\n",
            "iteration: 45390 loss: 0.0135 lr: 0.02\n",
            "iteration: 45400 loss: 0.0118 lr: 0.02\n",
            "iteration: 45410 loss: 0.0116 lr: 0.02\n",
            "iteration: 45420 loss: 0.0140 lr: 0.02\n",
            "iteration: 45430 loss: 0.0134 lr: 0.02\n",
            "iteration: 45440 loss: 0.0145 lr: 0.02\n",
            "iteration: 45450 loss: 0.0121 lr: 0.02\n",
            "iteration: 45460 loss: 0.0132 lr: 0.02\n",
            "iteration: 45470 loss: 0.0134 lr: 0.02\n",
            "iteration: 45480 loss: 0.0177 lr: 0.02\n",
            "iteration: 45490 loss: 0.0162 lr: 0.02\n",
            "iteration: 45500 loss: 0.0194 lr: 0.02\n",
            "iteration: 45510 loss: 0.0152 lr: 0.02\n",
            "iteration: 45520 loss: 0.0177 lr: 0.02\n",
            "iteration: 45530 loss: 0.0164 lr: 0.02\n",
            "iteration: 45540 loss: 0.0144 lr: 0.02\n",
            "iteration: 45550 loss: 0.0121 lr: 0.02\n",
            "iteration: 45560 loss: 0.0113 lr: 0.02\n",
            "iteration: 45570 loss: 0.0174 lr: 0.02\n",
            "iteration: 45580 loss: 0.0226 lr: 0.02\n",
            "iteration: 45590 loss: 0.0205 lr: 0.02\n",
            "iteration: 45600 loss: 0.0143 lr: 0.02\n",
            "iteration: 45610 loss: 0.0152 lr: 0.02\n",
            "iteration: 45620 loss: 0.0098 lr: 0.02\n",
            "iteration: 45630 loss: 0.0145 lr: 0.02\n",
            "iteration: 45640 loss: 0.0242 lr: 0.02\n",
            "iteration: 45650 loss: 0.0183 lr: 0.02\n",
            "iteration: 45660 loss: 0.0151 lr: 0.02\n",
            "iteration: 45670 loss: 0.0115 lr: 0.02\n",
            "iteration: 45680 loss: 0.0135 lr: 0.02\n",
            "iteration: 45690 loss: 0.0142 lr: 0.02\n",
            "iteration: 45700 loss: 0.0081 lr: 0.02\n",
            "iteration: 45710 loss: 0.0128 lr: 0.02\n",
            "iteration: 45720 loss: 0.0110 lr: 0.02\n",
            "iteration: 45730 loss: 0.0129 lr: 0.02\n",
            "iteration: 45740 loss: 0.0131 lr: 0.02\n",
            "iteration: 45750 loss: 0.0125 lr: 0.02\n",
            "iteration: 45760 loss: 0.0141 lr: 0.02\n",
            "iteration: 45770 loss: 0.0088 lr: 0.02\n",
            "iteration: 45780 loss: 0.0106 lr: 0.02\n",
            "iteration: 45790 loss: 0.0168 lr: 0.02\n",
            "iteration: 45800 loss: 0.0122 lr: 0.02\n",
            "iteration: 45810 loss: 0.0136 lr: 0.02\n",
            "iteration: 45820 loss: 0.0111 lr: 0.02\n",
            "iteration: 45830 loss: 0.0145 lr: 0.02\n",
            "iteration: 45840 loss: 0.0112 lr: 0.02\n",
            "iteration: 45850 loss: 0.0133 lr: 0.02\n",
            "iteration: 45860 loss: 0.0175 lr: 0.02\n",
            "iteration: 45870 loss: 0.0190 lr: 0.02\n",
            "iteration: 45880 loss: 0.0161 lr: 0.02\n",
            "iteration: 45890 loss: 0.0122 lr: 0.02\n",
            "iteration: 45900 loss: 0.0119 lr: 0.02\n",
            "iteration: 45910 loss: 0.0113 lr: 0.02\n",
            "iteration: 45920 loss: 0.0218 lr: 0.02\n",
            "iteration: 45930 loss: 0.0149 lr: 0.02\n",
            "iteration: 45940 loss: 0.0215 lr: 0.02\n",
            "iteration: 45950 loss: 0.0135 lr: 0.02\n",
            "iteration: 45960 loss: 0.0192 lr: 0.02\n",
            "iteration: 45970 loss: 0.0138 lr: 0.02\n",
            "iteration: 45980 loss: 0.0152 lr: 0.02\n",
            "iteration: 45990 loss: 0.0145 lr: 0.02\n",
            "iteration: 46000 loss: 0.0225 lr: 0.02\n",
            "iteration: 46010 loss: 0.0113 lr: 0.02\n",
            "iteration: 46020 loss: 0.0129 lr: 0.02\n",
            "iteration: 46030 loss: 0.0184 lr: 0.02\n",
            "iteration: 46040 loss: 0.0107 lr: 0.02\n",
            "iteration: 46050 loss: 0.0136 lr: 0.02\n",
            "iteration: 46060 loss: 0.0165 lr: 0.02\n",
            "iteration: 46070 loss: 0.0138 lr: 0.02\n",
            "iteration: 46080 loss: 0.0125 lr: 0.02\n",
            "iteration: 46090 loss: 0.0120 lr: 0.02\n",
            "iteration: 46100 loss: 0.0155 lr: 0.02\n",
            "iteration: 46110 loss: 0.0126 lr: 0.02\n",
            "iteration: 46120 loss: 0.0195 lr: 0.02\n",
            "iteration: 46130 loss: 0.0120 lr: 0.02\n",
            "iteration: 46140 loss: 0.0148 lr: 0.02\n",
            "iteration: 46150 loss: 0.0199 lr: 0.02\n",
            "iteration: 46160 loss: 0.0222 lr: 0.02\n",
            "iteration: 46170 loss: 0.0163 lr: 0.02\n",
            "iteration: 46180 loss: 0.0225 lr: 0.02\n",
            "iteration: 46190 loss: 0.0157 lr: 0.02\n",
            "iteration: 46200 loss: 0.0113 lr: 0.02\n",
            "iteration: 46210 loss: 0.0242 lr: 0.02\n",
            "iteration: 46220 loss: 0.0275 lr: 0.02\n",
            "iteration: 46230 loss: 0.0099 lr: 0.02\n",
            "iteration: 46240 loss: 0.0151 lr: 0.02\n",
            "iteration: 46250 loss: 0.0147 lr: 0.02\n",
            "iteration: 46260 loss: 0.0148 lr: 0.02\n",
            "iteration: 46270 loss: 0.0136 lr: 0.02\n",
            "iteration: 46280 loss: 0.0151 lr: 0.02\n",
            "iteration: 46290 loss: 0.0156 lr: 0.02\n",
            "iteration: 46300 loss: 0.0139 lr: 0.02\n",
            "iteration: 46310 loss: 0.0144 lr: 0.02\n",
            "iteration: 46320 loss: 0.0138 lr: 0.02\n",
            "iteration: 46330 loss: 0.0148 lr: 0.02\n",
            "iteration: 46340 loss: 0.0144 lr: 0.02\n",
            "iteration: 46350 loss: 0.0140 lr: 0.02\n",
            "iteration: 46360 loss: 0.0135 lr: 0.02\n",
            "iteration: 46370 loss: 0.0140 lr: 0.02\n",
            "iteration: 46380 loss: 0.0155 lr: 0.02\n",
            "iteration: 46390 loss: 0.0159 lr: 0.02\n",
            "iteration: 46400 loss: 0.0124 lr: 0.02\n",
            "iteration: 46410 loss: 0.0095 lr: 0.02\n",
            "iteration: 46420 loss: 0.0148 lr: 0.02\n",
            "iteration: 46430 loss: 0.0200 lr: 0.02\n",
            "iteration: 46440 loss: 0.0152 lr: 0.02\n",
            "iteration: 46450 loss: 0.0175 lr: 0.02\n",
            "iteration: 46460 loss: 0.0131 lr: 0.02\n",
            "iteration: 46470 loss: 0.0154 lr: 0.02\n",
            "iteration: 46480 loss: 0.0106 lr: 0.02\n",
            "iteration: 46490 loss: 0.0097 lr: 0.02\n",
            "iteration: 46500 loss: 0.0141 lr: 0.02\n",
            "iteration: 46510 loss: 0.0150 lr: 0.02\n",
            "iteration: 46520 loss: 0.0122 lr: 0.02\n",
            "iteration: 46530 loss: 0.0137 lr: 0.02\n",
            "iteration: 46540 loss: 0.0141 lr: 0.02\n",
            "iteration: 46550 loss: 0.0130 lr: 0.02\n",
            "iteration: 46560 loss: 0.0133 lr: 0.02\n",
            "iteration: 46570 loss: 0.0136 lr: 0.02\n",
            "iteration: 46580 loss: 0.0152 lr: 0.02\n",
            "iteration: 46590 loss: 0.0088 lr: 0.02\n",
            "iteration: 46600 loss: 0.0143 lr: 0.02\n",
            "iteration: 46610 loss: 0.0101 lr: 0.02\n",
            "iteration: 46620 loss: 0.0134 lr: 0.02\n",
            "iteration: 46630 loss: 0.0097 lr: 0.02\n",
            "iteration: 46640 loss: 0.0209 lr: 0.02\n",
            "iteration: 46650 loss: 0.0179 lr: 0.02\n",
            "iteration: 46660 loss: 0.0165 lr: 0.02\n",
            "iteration: 46670 loss: 0.0185 lr: 0.02\n",
            "iteration: 46680 loss: 0.0135 lr: 0.02\n",
            "iteration: 46690 loss: 0.0169 lr: 0.02\n",
            "iteration: 46700 loss: 0.0267 lr: 0.02\n",
            "iteration: 46710 loss: 0.0129 lr: 0.02\n",
            "iteration: 46720 loss: 0.0258 lr: 0.02\n",
            "iteration: 46730 loss: 0.0178 lr: 0.02\n",
            "iteration: 46740 loss: 0.0154 lr: 0.02\n",
            "iteration: 46750 loss: 0.0083 lr: 0.02\n",
            "iteration: 46760 loss: 0.0179 lr: 0.02\n",
            "iteration: 46770 loss: 0.0125 lr: 0.02\n",
            "iteration: 46780 loss: 0.0170 lr: 0.02\n",
            "iteration: 46790 loss: 0.0143 lr: 0.02\n",
            "iteration: 46800 loss: 0.0183 lr: 0.02\n",
            "iteration: 46810 loss: 0.0137 lr: 0.02\n",
            "iteration: 46820 loss: 0.0130 lr: 0.02\n",
            "iteration: 46830 loss: 0.0118 lr: 0.02\n",
            "iteration: 46840 loss: 0.0139 lr: 0.02\n",
            "iteration: 46850 loss: 0.0132 lr: 0.02\n",
            "iteration: 46860 loss: 0.0108 lr: 0.02\n",
            "iteration: 46870 loss: 0.0111 lr: 0.02\n",
            "iteration: 46880 loss: 0.0133 lr: 0.02\n",
            "iteration: 46890 loss: 0.0099 lr: 0.02\n",
            "iteration: 46900 loss: 0.0219 lr: 0.02\n",
            "iteration: 46910 loss: 0.0138 lr: 0.02\n",
            "iteration: 46920 loss: 0.0157 lr: 0.02\n",
            "iteration: 46930 loss: 0.0099 lr: 0.02\n",
            "iteration: 46940 loss: 0.0149 lr: 0.02\n",
            "iteration: 46950 loss: 0.0182 lr: 0.02\n",
            "iteration: 46960 loss: 0.0158 lr: 0.02\n",
            "iteration: 46970 loss: 0.0157 lr: 0.02\n",
            "iteration: 46980 loss: 0.0159 lr: 0.02\n",
            "iteration: 46990 loss: 0.0227 lr: 0.02\n",
            "iteration: 47000 loss: 0.0168 lr: 0.02\n",
            "iteration: 47010 loss: 0.0095 lr: 0.02\n",
            "iteration: 47020 loss: 0.0151 lr: 0.02\n",
            "iteration: 47030 loss: 0.0100 lr: 0.02\n",
            "iteration: 47040 loss: 0.0146 lr: 0.02\n",
            "iteration: 47050 loss: 0.0202 lr: 0.02\n",
            "iteration: 47060 loss: 0.0131 lr: 0.02\n",
            "iteration: 47070 loss: 0.0209 lr: 0.02\n",
            "iteration: 47080 loss: 0.0103 lr: 0.02\n",
            "iteration: 47090 loss: 0.0139 lr: 0.02\n",
            "iteration: 47100 loss: 0.0111 lr: 0.02\n",
            "iteration: 47110 loss: 0.0116 lr: 0.02\n",
            "iteration: 47120 loss: 0.0184 lr: 0.02\n",
            "iteration: 47130 loss: 0.0150 lr: 0.02\n",
            "iteration: 47140 loss: 0.0134 lr: 0.02\n",
            "iteration: 47150 loss: 0.0119 lr: 0.02\n",
            "iteration: 47160 loss: 0.0139 lr: 0.02\n",
            "iteration: 47170 loss: 0.0212 lr: 0.02\n",
            "iteration: 47180 loss: 0.0145 lr: 0.02\n",
            "iteration: 47190 loss: 0.0122 lr: 0.02\n",
            "iteration: 47200 loss: 0.0103 lr: 0.02\n",
            "iteration: 47210 loss: 0.0156 lr: 0.02\n",
            "iteration: 47220 loss: 0.0161 lr: 0.02\n",
            "iteration: 47230 loss: 0.0271 lr: 0.02\n",
            "iteration: 47240 loss: 0.0148 lr: 0.02\n",
            "iteration: 47250 loss: 0.0108 lr: 0.02\n",
            "iteration: 47260 loss: 0.0218 lr: 0.02\n",
            "iteration: 47270 loss: 0.0091 lr: 0.02\n",
            "iteration: 47280 loss: 0.0142 lr: 0.02\n",
            "iteration: 47290 loss: 0.0120 lr: 0.02\n",
            "iteration: 47300 loss: 0.0113 lr: 0.02\n",
            "iteration: 47310 loss: 0.0144 lr: 0.02\n",
            "iteration: 47320 loss: 0.0162 lr: 0.02\n",
            "iteration: 47330 loss: 0.0117 lr: 0.02\n",
            "iteration: 47340 loss: 0.0161 lr: 0.02\n",
            "iteration: 47350 loss: 0.0140 lr: 0.02\n",
            "iteration: 47360 loss: 0.0204 lr: 0.02\n",
            "iteration: 47370 loss: 0.0163 lr: 0.02\n",
            "iteration: 47380 loss: 0.0139 lr: 0.02\n",
            "iteration: 47390 loss: 0.0172 lr: 0.02\n",
            "iteration: 47400 loss: 0.0129 lr: 0.02\n",
            "iteration: 47410 loss: 0.0132 lr: 0.02\n",
            "iteration: 47420 loss: 0.0145 lr: 0.02\n",
            "iteration: 47430 loss: 0.0145 lr: 0.02\n",
            "iteration: 47440 loss: 0.0161 lr: 0.02\n",
            "iteration: 47450 loss: 0.0130 lr: 0.02\n",
            "iteration: 47460 loss: 0.0166 lr: 0.02\n",
            "iteration: 47470 loss: 0.0144 lr: 0.02\n",
            "iteration: 47480 loss: 0.0124 lr: 0.02\n",
            "iteration: 47490 loss: 0.0103 lr: 0.02\n",
            "iteration: 47500 loss: 0.0188 lr: 0.02\n",
            "iteration: 47510 loss: 0.0102 lr: 0.02\n",
            "iteration: 47520 loss: 0.0133 lr: 0.02\n",
            "iteration: 47530 loss: 0.0132 lr: 0.02\n",
            "iteration: 47540 loss: 0.0169 lr: 0.02\n",
            "iteration: 47550 loss: 0.0167 lr: 0.02\n",
            "iteration: 47560 loss: 0.0129 lr: 0.02\n",
            "iteration: 47570 loss: 0.0210 lr: 0.02\n",
            "iteration: 47580 loss: 0.0133 lr: 0.02\n",
            "iteration: 47590 loss: 0.0082 lr: 0.02\n",
            "iteration: 47600 loss: 0.0112 lr: 0.02\n",
            "iteration: 47610 loss: 0.0153 lr: 0.02\n",
            "iteration: 47620 loss: 0.0151 lr: 0.02\n",
            "iteration: 47630 loss: 0.0165 lr: 0.02\n",
            "iteration: 47640 loss: 0.0170 lr: 0.02\n",
            "iteration: 47650 loss: 0.0165 lr: 0.02\n",
            "iteration: 47660 loss: 0.0125 lr: 0.02\n",
            "iteration: 47670 loss: 0.0166 lr: 0.02\n",
            "iteration: 47680 loss: 0.0146 lr: 0.02\n",
            "iteration: 47690 loss: 0.0216 lr: 0.02\n",
            "iteration: 47700 loss: 0.0207 lr: 0.02\n",
            "iteration: 47710 loss: 0.0161 lr: 0.02\n",
            "iteration: 47720 loss: 0.0178 lr: 0.02\n",
            "iteration: 47730 loss: 0.0207 lr: 0.02\n",
            "iteration: 47740 loss: 0.0101 lr: 0.02\n",
            "iteration: 47750 loss: 0.0157 lr: 0.02\n",
            "iteration: 47760 loss: 0.0140 lr: 0.02\n",
            "iteration: 47770 loss: 0.0164 lr: 0.02\n",
            "iteration: 47780 loss: 0.0129 lr: 0.02\n",
            "iteration: 47790 loss: 0.0154 lr: 0.02\n",
            "iteration: 47800 loss: 0.0186 lr: 0.02\n",
            "iteration: 47810 loss: 0.0143 lr: 0.02\n",
            "iteration: 47820 loss: 0.0130 lr: 0.02\n",
            "iteration: 47830 loss: 0.0208 lr: 0.02\n",
            "iteration: 47840 loss: 0.0115 lr: 0.02\n",
            "iteration: 47850 loss: 0.0162 lr: 0.02\n",
            "iteration: 47860 loss: 0.0104 lr: 0.02\n",
            "iteration: 47870 loss: 0.0228 lr: 0.02\n",
            "iteration: 47880 loss: 0.0167 lr: 0.02\n",
            "iteration: 47890 loss: 0.0178 lr: 0.02\n",
            "iteration: 47900 loss: 0.0123 lr: 0.02\n",
            "iteration: 47910 loss: 0.0206 lr: 0.02\n",
            "iteration: 47920 loss: 0.0168 lr: 0.02\n",
            "iteration: 47930 loss: 0.0241 lr: 0.02\n",
            "iteration: 47940 loss: 0.0195 lr: 0.02\n",
            "iteration: 47950 loss: 0.0125 lr: 0.02\n",
            "iteration: 47960 loss: 0.0132 lr: 0.02\n",
            "iteration: 47970 loss: 0.0133 lr: 0.02\n",
            "iteration: 47980 loss: 0.0141 lr: 0.02\n",
            "iteration: 47990 loss: 0.0131 lr: 0.02\n",
            "iteration: 48000 loss: 0.0106 lr: 0.02\n",
            "iteration: 48010 loss: 0.0130 lr: 0.02\n",
            "iteration: 48020 loss: 0.0132 lr: 0.02\n",
            "iteration: 48030 loss: 0.0159 lr: 0.02\n",
            "iteration: 48040 loss: 0.0119 lr: 0.02\n",
            "iteration: 48050 loss: 0.0180 lr: 0.02\n",
            "iteration: 48060 loss: 0.0191 lr: 0.02\n",
            "iteration: 48070 loss: 0.0121 lr: 0.02\n",
            "iteration: 48080 loss: 0.0083 lr: 0.02\n",
            "iteration: 48090 loss: 0.0178 lr: 0.02\n",
            "iteration: 48100 loss: 0.0138 lr: 0.02\n",
            "iteration: 48110 loss: 0.0152 lr: 0.02\n",
            "iteration: 48120 loss: 0.0120 lr: 0.02\n",
            "iteration: 48130 loss: 0.0134 lr: 0.02\n",
            "iteration: 48140 loss: 0.0125 lr: 0.02\n",
            "iteration: 48150 loss: 0.0172 lr: 0.02\n",
            "iteration: 48160 loss: 0.0131 lr: 0.02\n",
            "iteration: 48170 loss: 0.0155 lr: 0.02\n",
            "iteration: 48180 loss: 0.0108 lr: 0.02\n",
            "iteration: 48190 loss: 0.0150 lr: 0.02\n",
            "iteration: 48200 loss: 0.0184 lr: 0.02\n",
            "iteration: 48210 loss: 0.0160 lr: 0.02\n",
            "iteration: 48220 loss: 0.0087 lr: 0.02\n",
            "iteration: 48230 loss: 0.0131 lr: 0.02\n",
            "iteration: 48240 loss: 0.0142 lr: 0.02\n",
            "iteration: 48250 loss: 0.0143 lr: 0.02\n",
            "iteration: 48260 loss: 0.0101 lr: 0.02\n",
            "iteration: 48270 loss: 0.0189 lr: 0.02\n",
            "iteration: 48280 loss: 0.0140 lr: 0.02\n",
            "iteration: 48290 loss: 0.0140 lr: 0.02\n",
            "iteration: 48300 loss: 0.0151 lr: 0.02\n",
            "iteration: 48310 loss: 0.0224 lr: 0.02\n",
            "iteration: 48320 loss: 0.0139 lr: 0.02\n",
            "iteration: 48330 loss: 0.0168 lr: 0.02\n",
            "iteration: 48340 loss: 0.0139 lr: 0.02\n",
            "iteration: 48350 loss: 0.0144 lr: 0.02\n",
            "iteration: 48360 loss: 0.0190 lr: 0.02\n",
            "iteration: 48370 loss: 0.0152 lr: 0.02\n",
            "iteration: 48380 loss: 0.0133 lr: 0.02\n",
            "iteration: 48390 loss: 0.0144 lr: 0.02\n",
            "iteration: 48400 loss: 0.0130 lr: 0.02\n",
            "iteration: 48410 loss: 0.0116 lr: 0.02\n",
            "iteration: 48420 loss: 0.0143 lr: 0.02\n",
            "iteration: 48430 loss: 0.0147 lr: 0.02\n",
            "iteration: 48440 loss: 0.0123 lr: 0.02\n",
            "iteration: 48450 loss: 0.0122 lr: 0.02\n",
            "iteration: 48460 loss: 0.0169 lr: 0.02\n",
            "iteration: 48470 loss: 0.0145 lr: 0.02\n",
            "iteration: 48480 loss: 0.0150 lr: 0.02\n",
            "iteration: 48490 loss: 0.0142 lr: 0.02\n",
            "iteration: 48500 loss: 0.0115 lr: 0.02\n",
            "iteration: 48510 loss: 0.0105 lr: 0.02\n",
            "iteration: 48520 loss: 0.0118 lr: 0.02\n",
            "iteration: 48530 loss: 0.0110 lr: 0.02\n",
            "iteration: 48540 loss: 0.0097 lr: 0.02\n",
            "iteration: 48550 loss: 0.0182 lr: 0.02\n",
            "iteration: 48560 loss: 0.0160 lr: 0.02\n",
            "iteration: 48570 loss: 0.0223 lr: 0.02\n",
            "iteration: 48580 loss: 0.0139 lr: 0.02\n",
            "iteration: 48590 loss: 0.0120 lr: 0.02\n",
            "iteration: 48600 loss: 0.0128 lr: 0.02\n",
            "iteration: 48610 loss: 0.0129 lr: 0.02\n",
            "iteration: 48620 loss: 0.0137 lr: 0.02\n",
            "iteration: 48630 loss: 0.0164 lr: 0.02\n",
            "iteration: 48640 loss: 0.0146 lr: 0.02\n",
            "iteration: 48650 loss: 0.0110 lr: 0.02\n",
            "iteration: 48660 loss: 0.0154 lr: 0.02\n",
            "iteration: 48670 loss: 0.0141 lr: 0.02\n",
            "iteration: 48680 loss: 0.0134 lr: 0.02\n",
            "iteration: 48690 loss: 0.0138 lr: 0.02\n",
            "iteration: 48700 loss: 0.0188 lr: 0.02\n",
            "iteration: 48710 loss: 0.0138 lr: 0.02\n",
            "iteration: 48720 loss: 0.0184 lr: 0.02\n",
            "iteration: 48730 loss: 0.0129 lr: 0.02\n",
            "iteration: 48740 loss: 0.0091 lr: 0.02\n",
            "iteration: 48750 loss: 0.0100 lr: 0.02\n",
            "iteration: 48760 loss: 0.0117 lr: 0.02\n",
            "iteration: 48770 loss: 0.0116 lr: 0.02\n",
            "iteration: 48780 loss: 0.0163 lr: 0.02\n",
            "iteration: 48790 loss: 0.0143 lr: 0.02\n",
            "iteration: 48800 loss: 0.0124 lr: 0.02\n",
            "iteration: 48810 loss: 0.0155 lr: 0.02\n",
            "iteration: 48820 loss: 0.0161 lr: 0.02\n",
            "iteration: 48830 loss: 0.0170 lr: 0.02\n",
            "iteration: 48840 loss: 0.0192 lr: 0.02\n",
            "iteration: 48850 loss: 0.0172 lr: 0.02\n",
            "iteration: 48860 loss: 0.0094 lr: 0.02\n",
            "iteration: 48870 loss: 0.0113 lr: 0.02\n",
            "iteration: 48880 loss: 0.0142 lr: 0.02\n",
            "iteration: 48890 loss: 0.0114 lr: 0.02\n",
            "iteration: 48900 loss: 0.0174 lr: 0.02\n",
            "iteration: 48910 loss: 0.0135 lr: 0.02\n",
            "iteration: 48920 loss: 0.0130 lr: 0.02\n",
            "iteration: 48930 loss: 0.0163 lr: 0.02\n",
            "iteration: 48940 loss: 0.0199 lr: 0.02\n",
            "iteration: 48950 loss: 0.0164 lr: 0.02\n",
            "iteration: 48960 loss: 0.0122 lr: 0.02\n",
            "iteration: 48970 loss: 0.0118 lr: 0.02\n",
            "iteration: 48980 loss: 0.0157 lr: 0.02\n",
            "iteration: 48990 loss: 0.0143 lr: 0.02\n",
            "iteration: 49000 loss: 0.0159 lr: 0.02\n",
            "iteration: 49010 loss: 0.0149 lr: 0.02\n",
            "iteration: 49020 loss: 0.0140 lr: 0.02\n",
            "iteration: 49030 loss: 0.0201 lr: 0.02\n",
            "iteration: 49040 loss: 0.0108 lr: 0.02\n",
            "iteration: 49050 loss: 0.0152 lr: 0.02\n",
            "iteration: 49060 loss: 0.0125 lr: 0.02\n",
            "iteration: 49070 loss: 0.0158 lr: 0.02\n",
            "iteration: 49080 loss: 0.0141 lr: 0.02\n",
            "iteration: 49090 loss: 0.0099 lr: 0.02\n",
            "iteration: 49100 loss: 0.0154 lr: 0.02\n",
            "iteration: 49110 loss: 0.0163 lr: 0.02\n",
            "iteration: 49120 loss: 0.0137 lr: 0.02\n",
            "iteration: 49130 loss: 0.0123 lr: 0.02\n",
            "iteration: 49140 loss: 0.0150 lr: 0.02\n",
            "iteration: 49150 loss: 0.0162 lr: 0.02\n",
            "iteration: 49160 loss: 0.0142 lr: 0.02\n",
            "iteration: 49170 loss: 0.0123 lr: 0.02\n",
            "iteration: 49180 loss: 0.0158 lr: 0.02\n",
            "iteration: 49190 loss: 0.0141 lr: 0.02\n",
            "iteration: 49200 loss: 0.0131 lr: 0.02\n",
            "iteration: 49210 loss: 0.0184 lr: 0.02\n",
            "iteration: 49220 loss: 0.0100 lr: 0.02\n",
            "iteration: 49230 loss: 0.0138 lr: 0.02\n",
            "iteration: 49240 loss: 0.0133 lr: 0.02\n",
            "iteration: 49250 loss: 0.0186 lr: 0.02\n",
            "iteration: 49260 loss: 0.0136 lr: 0.02\n",
            "iteration: 49270 loss: 0.0126 lr: 0.02\n",
            "iteration: 49280 loss: 0.0131 lr: 0.02\n",
            "iteration: 49290 loss: 0.0138 lr: 0.02\n",
            "iteration: 49300 loss: 0.0127 lr: 0.02\n",
            "iteration: 49310 loss: 0.0115 lr: 0.02\n",
            "iteration: 49320 loss: 0.0218 lr: 0.02\n",
            "iteration: 49330 loss: 0.0171 lr: 0.02\n",
            "iteration: 49340 loss: 0.0243 lr: 0.02\n",
            "iteration: 49350 loss: 0.0126 lr: 0.02\n",
            "iteration: 49360 loss: 0.0168 lr: 0.02\n",
            "iteration: 49370 loss: 0.0135 lr: 0.02\n",
            "iteration: 49380 loss: 0.0154 lr: 0.02\n",
            "iteration: 49390 loss: 0.0151 lr: 0.02\n",
            "iteration: 49400 loss: 0.0136 lr: 0.02\n",
            "iteration: 49410 loss: 0.0124 lr: 0.02\n",
            "iteration: 49420 loss: 0.0131 lr: 0.02\n",
            "iteration: 49430 loss: 0.0137 lr: 0.02\n",
            "iteration: 49440 loss: 0.0116 lr: 0.02\n",
            "iteration: 49450 loss: 0.0210 lr: 0.02\n",
            "iteration: 49460 loss: 0.0142 lr: 0.02\n",
            "iteration: 49470 loss: 0.0138 lr: 0.02\n",
            "iteration: 49480 loss: 0.0155 lr: 0.02\n",
            "iteration: 49490 loss: 0.0148 lr: 0.02\n",
            "iteration: 49500 loss: 0.0176 lr: 0.02\n",
            "iteration: 49510 loss: 0.0132 lr: 0.02\n",
            "iteration: 49520 loss: 0.0127 lr: 0.02\n",
            "iteration: 49530 loss: 0.0129 lr: 0.02\n",
            "iteration: 49540 loss: 0.0144 lr: 0.02\n",
            "iteration: 49550 loss: 0.0152 lr: 0.02\n",
            "iteration: 49560 loss: 0.0123 lr: 0.02\n",
            "iteration: 49570 loss: 0.0154 lr: 0.02\n",
            "iteration: 49580 loss: 0.0175 lr: 0.02\n",
            "iteration: 49590 loss: 0.0153 lr: 0.02\n",
            "iteration: 49600 loss: 0.0136 lr: 0.02\n",
            "iteration: 49610 loss: 0.0142 lr: 0.02\n",
            "iteration: 49620 loss: 0.0131 lr: 0.02\n",
            "iteration: 49630 loss: 0.0214 lr: 0.02\n",
            "iteration: 49640 loss: 0.0170 lr: 0.02\n",
            "iteration: 49650 loss: 0.0136 lr: 0.02\n",
            "iteration: 49660 loss: 0.0125 lr: 0.02\n",
            "iteration: 49670 loss: 0.0134 lr: 0.02\n",
            "iteration: 49680 loss: 0.0142 lr: 0.02\n",
            "iteration: 49690 loss: 0.0187 lr: 0.02\n",
            "iteration: 49700 loss: 0.0195 lr: 0.02\n",
            "iteration: 49710 loss: 0.0123 lr: 0.02\n",
            "iteration: 49720 loss: 0.0089 lr: 0.02\n",
            "iteration: 49730 loss: 0.0131 lr: 0.02\n",
            "iteration: 49740 loss: 0.0132 lr: 0.02\n",
            "iteration: 49750 loss: 0.0127 lr: 0.02\n",
            "iteration: 49760 loss: 0.0126 lr: 0.02\n",
            "iteration: 49770 loss: 0.0141 lr: 0.02\n",
            "iteration: 49780 loss: 0.0163 lr: 0.02\n",
            "iteration: 49790 loss: 0.0109 lr: 0.02\n",
            "iteration: 49800 loss: 0.0137 lr: 0.02\n",
            "iteration: 49810 loss: 0.0143 lr: 0.02\n",
            "iteration: 49820 loss: 0.0150 lr: 0.02\n",
            "iteration: 49830 loss: 0.0174 lr: 0.02\n",
            "iteration: 49840 loss: 0.0132 lr: 0.02\n",
            "iteration: 49850 loss: 0.0130 lr: 0.02\n",
            "iteration: 49860 loss: 0.0126 lr: 0.02\n",
            "iteration: 49870 loss: 0.0215 lr: 0.02\n",
            "iteration: 49880 loss: 0.0175 lr: 0.02\n",
            "iteration: 49890 loss: 0.0135 lr: 0.02\n",
            "iteration: 49900 loss: 0.0224 lr: 0.02\n",
            "iteration: 49910 loss: 0.0138 lr: 0.02\n",
            "iteration: 49920 loss: 0.0142 lr: 0.02\n",
            "iteration: 49930 loss: 0.0121 lr: 0.02\n",
            "iteration: 49940 loss: 0.0095 lr: 0.02\n",
            "iteration: 49950 loss: 0.0158 lr: 0.02\n",
            "iteration: 49960 loss: 0.0104 lr: 0.02\n",
            "iteration: 49970 loss: 0.0169 lr: 0.02\n",
            "iteration: 49980 loss: 0.0173 lr: 0.02\n",
            "iteration: 49990 loss: 0.0144 lr: 0.02\n",
            "iteration: 50000 loss: 0.0229 lr: 0.02\n",
            "iteration: 50010 loss: 0.0110 lr: 0.02\n",
            "iteration: 50020 loss: 0.0148 lr: 0.02\n",
            "iteration: 50030 loss: 0.0143 lr: 0.02\n",
            "iteration: 50040 loss: 0.0136 lr: 0.02\n",
            "iteration: 50050 loss: 0.0116 lr: 0.02\n",
            "iteration: 50060 loss: 0.0119 lr: 0.02\n",
            "iteration: 50070 loss: 0.0144 lr: 0.02\n",
            "iteration: 50080 loss: 0.0143 lr: 0.02\n",
            "iteration: 50090 loss: 0.0119 lr: 0.02\n",
            "iteration: 50100 loss: 0.0130 lr: 0.02\n",
            "iteration: 50110 loss: 0.0162 lr: 0.02\n",
            "iteration: 50120 loss: 0.0121 lr: 0.02\n",
            "iteration: 50130 loss: 0.0155 lr: 0.02\n",
            "iteration: 50140 loss: 0.0126 lr: 0.02\n",
            "iteration: 50150 loss: 0.0168 lr: 0.02\n",
            "iteration: 50160 loss: 0.0095 lr: 0.02\n",
            "iteration: 50170 loss: 0.0108 lr: 0.02\n",
            "iteration: 50180 loss: 0.0138 lr: 0.02\n",
            "iteration: 50190 loss: 0.0146 lr: 0.02\n",
            "iteration: 50200 loss: 0.0138 lr: 0.02\n",
            "iteration: 50210 loss: 0.0209 lr: 0.02\n",
            "iteration: 50220 loss: 0.0076 lr: 0.02\n",
            "iteration: 50230 loss: 0.0133 lr: 0.02\n",
            "iteration: 50240 loss: 0.0127 lr: 0.02\n",
            "iteration: 50250 loss: 0.0104 lr: 0.02\n",
            "iteration: 50260 loss: 0.0132 lr: 0.02\n",
            "iteration: 50270 loss: 0.0151 lr: 0.02\n",
            "iteration: 50280 loss: 0.0148 lr: 0.02\n",
            "iteration: 50290 loss: 0.0167 lr: 0.02\n",
            "iteration: 50300 loss: 0.0155 lr: 0.02\n",
            "iteration: 50310 loss: 0.0123 lr: 0.02\n",
            "iteration: 50320 loss: 0.0151 lr: 0.02\n",
            "iteration: 50330 loss: 0.0143 lr: 0.02\n",
            "iteration: 50340 loss: 0.0095 lr: 0.02\n",
            "iteration: 50350 loss: 0.0134 lr: 0.02\n",
            "iteration: 50360 loss: 0.0168 lr: 0.02\n",
            "iteration: 50370 loss: 0.0106 lr: 0.02\n",
            "iteration: 50380 loss: 0.0117 lr: 0.02\n",
            "iteration: 50390 loss: 0.0119 lr: 0.02\n",
            "iteration: 50400 loss: 0.0215 lr: 0.02\n",
            "iteration: 50410 loss: 0.0189 lr: 0.02\n",
            "iteration: 50420 loss: 0.0171 lr: 0.02\n",
            "iteration: 50430 loss: 0.0137 lr: 0.02\n",
            "iteration: 50440 loss: 0.0099 lr: 0.02\n",
            "iteration: 50450 loss: 0.0167 lr: 0.02\n",
            "iteration: 50460 loss: 0.0135 lr: 0.02\n",
            "iteration: 50470 loss: 0.0151 lr: 0.02\n",
            "iteration: 50480 loss: 0.0153 lr: 0.02\n",
            "iteration: 50490 loss: 0.0112 lr: 0.02\n",
            "iteration: 50500 loss: 0.0102 lr: 0.02\n",
            "iteration: 50510 loss: 0.0134 lr: 0.02\n",
            "iteration: 50520 loss: 0.0165 lr: 0.02\n",
            "iteration: 50530 loss: 0.0085 lr: 0.02\n",
            "iteration: 50540 loss: 0.0175 lr: 0.02\n",
            "iteration: 50550 loss: 0.0115 lr: 0.02\n",
            "iteration: 50560 loss: 0.0145 lr: 0.02\n",
            "iteration: 50570 loss: 0.0113 lr: 0.02\n",
            "iteration: 50580 loss: 0.0141 lr: 0.02\n",
            "iteration: 50590 loss: 0.0168 lr: 0.02\n",
            "iteration: 50600 loss: 0.0160 lr: 0.02\n",
            "iteration: 50610 loss: 0.0143 lr: 0.02\n",
            "iteration: 50620 loss: 0.0258 lr: 0.02\n",
            "iteration: 50630 loss: 0.0172 lr: 0.02\n",
            "iteration: 50640 loss: 0.0166 lr: 0.02\n",
            "iteration: 50650 loss: 0.0118 lr: 0.02\n",
            "iteration: 50660 loss: 0.0133 lr: 0.02\n",
            "iteration: 50670 loss: 0.0224 lr: 0.02\n",
            "iteration: 50680 loss: 0.0131 lr: 0.02\n",
            "iteration: 50690 loss: 0.0137 lr: 0.02\n",
            "iteration: 50700 loss: 0.0170 lr: 0.02\n",
            "iteration: 50710 loss: 0.0132 lr: 0.02\n",
            "iteration: 50720 loss: 0.0121 lr: 0.02\n",
            "iteration: 50730 loss: 0.0189 lr: 0.02\n",
            "iteration: 50740 loss: 0.0097 lr: 0.02\n",
            "iteration: 50750 loss: 0.0130 lr: 0.02\n",
            "iteration: 50760 loss: 0.0143 lr: 0.02\n",
            "iteration: 50770 loss: 0.0143 lr: 0.02\n",
            "iteration: 50780 loss: 0.0136 lr: 0.02\n",
            "iteration: 50790 loss: 0.0117 lr: 0.02\n",
            "iteration: 50800 loss: 0.0143 lr: 0.02\n",
            "iteration: 50810 loss: 0.0157 lr: 0.02\n",
            "iteration: 50820 loss: 0.0129 lr: 0.02\n",
            "iteration: 50830 loss: 0.0136 lr: 0.02\n",
            "iteration: 50840 loss: 0.0239 lr: 0.02\n",
            "iteration: 50850 loss: 0.0095 lr: 0.02\n",
            "iteration: 50860 loss: 0.0104 lr: 0.02\n",
            "iteration: 50870 loss: 0.0130 lr: 0.02\n",
            "iteration: 50880 loss: 0.0157 lr: 0.02\n",
            "iteration: 50890 loss: 0.0122 lr: 0.02\n",
            "iteration: 50900 loss: 0.0141 lr: 0.02\n",
            "iteration: 50910 loss: 0.0154 lr: 0.02\n",
            "iteration: 50920 loss: 0.0140 lr: 0.02\n",
            "iteration: 50930 loss: 0.0185 lr: 0.02\n",
            "iteration: 50940 loss: 0.0121 lr: 0.02\n",
            "iteration: 50950 loss: 0.0145 lr: 0.02\n",
            "iteration: 50960 loss: 0.0169 lr: 0.02\n",
            "iteration: 50970 loss: 0.0188 lr: 0.02\n",
            "iteration: 50980 loss: 0.0189 lr: 0.02\n",
            "iteration: 50990 loss: 0.0230 lr: 0.02\n",
            "iteration: 51000 loss: 0.0120 lr: 0.02\n",
            "iteration: 51010 loss: 0.0167 lr: 0.02\n",
            "iteration: 51020 loss: 0.0130 lr: 0.02\n",
            "iteration: 51030 loss: 0.0104 lr: 0.02\n",
            "iteration: 51040 loss: 0.0190 lr: 0.02\n",
            "iteration: 51050 loss: 0.0160 lr: 0.02\n",
            "iteration: 51060 loss: 0.0157 lr: 0.02\n",
            "iteration: 51070 loss: 0.0178 lr: 0.02\n",
            "iteration: 51080 loss: 0.0127 lr: 0.02\n",
            "iteration: 51090 loss: 0.0159 lr: 0.02\n",
            "iteration: 51100 loss: 0.0136 lr: 0.02\n",
            "iteration: 51110 loss: 0.0098 lr: 0.02\n",
            "iteration: 51120 loss: 0.0114 lr: 0.02\n",
            "iteration: 51130 loss: 0.0150 lr: 0.02\n",
            "iteration: 51140 loss: 0.0158 lr: 0.02\n",
            "iteration: 51150 loss: 0.0223 lr: 0.02\n",
            "iteration: 51160 loss: 0.0173 lr: 0.02\n",
            "iteration: 51170 loss: 0.0125 lr: 0.02\n",
            "iteration: 51180 loss: 0.0163 lr: 0.02\n",
            "iteration: 51190 loss: 0.0160 lr: 0.02\n",
            "iteration: 51200 loss: 0.0258 lr: 0.02\n",
            "iteration: 51210 loss: 0.0152 lr: 0.02\n",
            "iteration: 51220 loss: 0.0128 lr: 0.02\n",
            "iteration: 51230 loss: 0.0202 lr: 0.02\n",
            "iteration: 51240 loss: 0.0131 lr: 0.02\n",
            "iteration: 51250 loss: 0.0181 lr: 0.02\n",
            "iteration: 51260 loss: 0.0149 lr: 0.02\n",
            "iteration: 51270 loss: 0.0227 lr: 0.02\n",
            "iteration: 51280 loss: 0.0115 lr: 0.02\n",
            "iteration: 51290 loss: 0.0122 lr: 0.02\n",
            "iteration: 51300 loss: 0.0189 lr: 0.02\n",
            "iteration: 51310 loss: 0.0200 lr: 0.02\n",
            "iteration: 51320 loss: 0.0121 lr: 0.02\n",
            "iteration: 51330 loss: 0.0109 lr: 0.02\n",
            "iteration: 51340 loss: 0.0125 lr: 0.02\n",
            "iteration: 51350 loss: 0.0162 lr: 0.02\n",
            "iteration: 51360 loss: 0.0151 lr: 0.02\n",
            "iteration: 51370 loss: 0.0189 lr: 0.02\n",
            "iteration: 51380 loss: 0.0159 lr: 0.02\n",
            "iteration: 51390 loss: 0.0150 lr: 0.02\n",
            "iteration: 51400 loss: 0.0140 lr: 0.02\n",
            "iteration: 51410 loss: 0.0151 lr: 0.02\n",
            "iteration: 51420 loss: 0.0134 lr: 0.02\n",
            "iteration: 51430 loss: 0.0260 lr: 0.02\n",
            "iteration: 51440 loss: 0.0121 lr: 0.02\n",
            "iteration: 51450 loss: 0.0159 lr: 0.02\n",
            "iteration: 51460 loss: 0.0164 lr: 0.02\n",
            "iteration: 51470 loss: 0.0133 lr: 0.02\n",
            "iteration: 51480 loss: 0.0195 lr: 0.02\n",
            "iteration: 51490 loss: 0.0125 lr: 0.02\n",
            "iteration: 51500 loss: 0.0134 lr: 0.02\n",
            "iteration: 51510 loss: 0.0098 lr: 0.02\n",
            "iteration: 51520 loss: 0.0170 lr: 0.02\n",
            "iteration: 51530 loss: 0.0133 lr: 0.02\n",
            "iteration: 51540 loss: 0.0096 lr: 0.02\n",
            "iteration: 51550 loss: 0.0120 lr: 0.02\n",
            "iteration: 51560 loss: 0.0106 lr: 0.02\n",
            "iteration: 51570 loss: 0.0182 lr: 0.02\n",
            "iteration: 51580 loss: 0.0142 lr: 0.02\n",
            "iteration: 51590 loss: 0.0131 lr: 0.02\n",
            "iteration: 51600 loss: 0.0132 lr: 0.02\n",
            "iteration: 51610 loss: 0.0170 lr: 0.02\n",
            "iteration: 51620 loss: 0.0101 lr: 0.02\n",
            "iteration: 51630 loss: 0.0235 lr: 0.02\n",
            "iteration: 51640 loss: 0.0137 lr: 0.02\n",
            "iteration: 51650 loss: 0.0129 lr: 0.02\n",
            "iteration: 51660 loss: 0.0135 lr: 0.02\n",
            "iteration: 51670 loss: 0.0188 lr: 0.02\n",
            "iteration: 51680 loss: 0.0116 lr: 0.02\n",
            "iteration: 51690 loss: 0.0149 lr: 0.02\n",
            "iteration: 51700 loss: 0.0179 lr: 0.02\n",
            "iteration: 51710 loss: 0.0092 lr: 0.02\n",
            "iteration: 51720 loss: 0.0129 lr: 0.02\n",
            "iteration: 51730 loss: 0.0146 lr: 0.02\n",
            "iteration: 51740 loss: 0.0147 lr: 0.02\n",
            "iteration: 51750 loss: 0.0093 lr: 0.02\n",
            "iteration: 51760 loss: 0.0095 lr: 0.02\n",
            "iteration: 51770 loss: 0.0224 lr: 0.02\n",
            "iteration: 51780 loss: 0.0124 lr: 0.02\n",
            "iteration: 51790 loss: 0.0178 lr: 0.02\n",
            "iteration: 51800 loss: 0.0117 lr: 0.02\n",
            "iteration: 51810 loss: 0.0182 lr: 0.02\n",
            "iteration: 51820 loss: 0.0084 lr: 0.02\n",
            "iteration: 51830 loss: 0.0142 lr: 0.02\n",
            "iteration: 51840 loss: 0.0120 lr: 0.02\n",
            "iteration: 51850 loss: 0.0165 lr: 0.02\n",
            "iteration: 51860 loss: 0.0137 lr: 0.02\n",
            "iteration: 51870 loss: 0.0133 lr: 0.02\n",
            "iteration: 51880 loss: 0.0173 lr: 0.02\n",
            "iteration: 51890 loss: 0.0134 lr: 0.02\n",
            "iteration: 51900 loss: 0.0141 lr: 0.02\n",
            "iteration: 51910 loss: 0.0168 lr: 0.02\n",
            "iteration: 51920 loss: 0.0131 lr: 0.02\n",
            "iteration: 51930 loss: 0.0148 lr: 0.02\n",
            "iteration: 51940 loss: 0.0119 lr: 0.02\n",
            "iteration: 51950 loss: 0.0120 lr: 0.02\n",
            "iteration: 51960 loss: 0.0184 lr: 0.02\n",
            "iteration: 51970 loss: 0.0116 lr: 0.02\n",
            "iteration: 51980 loss: 0.0167 lr: 0.02\n",
            "iteration: 51990 loss: 0.0151 lr: 0.02\n",
            "iteration: 52000 loss: 0.0125 lr: 0.02\n",
            "iteration: 52010 loss: 0.0119 lr: 0.02\n",
            "iteration: 52020 loss: 0.0207 lr: 0.02\n",
            "iteration: 52030 loss: 0.0125 lr: 0.02\n",
            "iteration: 52040 loss: 0.0174 lr: 0.02\n",
            "iteration: 52050 loss: 0.0136 lr: 0.02\n",
            "iteration: 52060 loss: 0.0155 lr: 0.02\n",
            "iteration: 52070 loss: 0.0098 lr: 0.02\n",
            "iteration: 52080 loss: 0.0119 lr: 0.02\n",
            "iteration: 52090 loss: 0.0135 lr: 0.02\n",
            "iteration: 52100 loss: 0.0092 lr: 0.02\n",
            "iteration: 52110 loss: 0.0158 lr: 0.02\n",
            "iteration: 52120 loss: 0.0154 lr: 0.02\n",
            "iteration: 52130 loss: 0.0146 lr: 0.02\n",
            "iteration: 52140 loss: 0.0169 lr: 0.02\n",
            "iteration: 52150 loss: 0.0129 lr: 0.02\n",
            "iteration: 52160 loss: 0.0186 lr: 0.02\n",
            "iteration: 52170 loss: 0.0143 lr: 0.02\n",
            "iteration: 52180 loss: 0.0175 lr: 0.02\n",
            "iteration: 52190 loss: 0.0099 lr: 0.02\n",
            "iteration: 52200 loss: 0.0188 lr: 0.02\n",
            "iteration: 52210 loss: 0.0163 lr: 0.02\n",
            "iteration: 52220 loss: 0.0131 lr: 0.02\n",
            "iteration: 52230 loss: 0.0136 lr: 0.02\n",
            "iteration: 52240 loss: 0.0167 lr: 0.02\n",
            "iteration: 52250 loss: 0.0112 lr: 0.02\n",
            "iteration: 52260 loss: 0.0117 lr: 0.02\n",
            "iteration: 52270 loss: 0.0152 lr: 0.02\n",
            "iteration: 52280 loss: 0.0101 lr: 0.02\n",
            "iteration: 52290 loss: 0.0139 lr: 0.02\n",
            "iteration: 52300 loss: 0.0142 lr: 0.02\n",
            "iteration: 52310 loss: 0.0131 lr: 0.02\n",
            "iteration: 52320 loss: 0.0159 lr: 0.02\n",
            "iteration: 52330 loss: 0.0141 lr: 0.02\n",
            "iteration: 52340 loss: 0.0169 lr: 0.02\n",
            "iteration: 52350 loss: 0.0216 lr: 0.02\n",
            "iteration: 52360 loss: 0.0162 lr: 0.02\n",
            "iteration: 52370 loss: 0.0116 lr: 0.02\n",
            "iteration: 52380 loss: 0.0133 lr: 0.02\n",
            "iteration: 52390 loss: 0.0128 lr: 0.02\n",
            "iteration: 52400 loss: 0.0166 lr: 0.02\n",
            "iteration: 52410 loss: 0.0209 lr: 0.02\n",
            "iteration: 52420 loss: 0.0146 lr: 0.02\n",
            "iteration: 52430 loss: 0.0107 lr: 0.02\n",
            "iteration: 52440 loss: 0.0167 lr: 0.02\n",
            "iteration: 52450 loss: 0.0185 lr: 0.02\n",
            "iteration: 52460 loss: 0.0142 lr: 0.02\n",
            "iteration: 52470 loss: 0.0124 lr: 0.02\n",
            "iteration: 52480 loss: 0.0162 lr: 0.02\n",
            "iteration: 52490 loss: 0.0169 lr: 0.02\n",
            "iteration: 52500 loss: 0.0111 lr: 0.02\n",
            "iteration: 52510 loss: 0.0138 lr: 0.02\n",
            "iteration: 52520 loss: 0.0152 lr: 0.02\n",
            "iteration: 52530 loss: 0.0159 lr: 0.02\n",
            "iteration: 52540 loss: 0.0127 lr: 0.02\n",
            "iteration: 52550 loss: 0.0173 lr: 0.02\n",
            "iteration: 52560 loss: 0.0136 lr: 0.02\n",
            "iteration: 52570 loss: 0.0143 lr: 0.02\n",
            "iteration: 52580 loss: 0.0151 lr: 0.02\n",
            "iteration: 52590 loss: 0.0095 lr: 0.02\n",
            "iteration: 52600 loss: 0.0152 lr: 0.02\n",
            "iteration: 52610 loss: 0.0136 lr: 0.02\n",
            "iteration: 52620 loss: 0.0097 lr: 0.02\n",
            "iteration: 52630 loss: 0.0119 lr: 0.02\n",
            "iteration: 52640 loss: 0.0126 lr: 0.02\n",
            "iteration: 52650 loss: 0.0096 lr: 0.02\n",
            "iteration: 52660 loss: 0.0139 lr: 0.02\n",
            "iteration: 52670 loss: 0.0188 lr: 0.02\n",
            "iteration: 52680 loss: 0.0151 lr: 0.02\n",
            "iteration: 52690 loss: 0.0135 lr: 0.02\n",
            "iteration: 52700 loss: 0.0084 lr: 0.02\n",
            "iteration: 52710 loss: 0.0157 lr: 0.02\n",
            "iteration: 52720 loss: 0.0236 lr: 0.02\n",
            "iteration: 52730 loss: 0.0104 lr: 0.02\n",
            "iteration: 52740 loss: 0.0163 lr: 0.02\n",
            "iteration: 52750 loss: 0.0134 lr: 0.02\n",
            "iteration: 52760 loss: 0.0193 lr: 0.02\n",
            "iteration: 52770 loss: 0.0141 lr: 0.02\n",
            "iteration: 52780 loss: 0.0110 lr: 0.02\n",
            "iteration: 52790 loss: 0.0136 lr: 0.02\n",
            "iteration: 52800 loss: 0.0115 lr: 0.02\n",
            "iteration: 52810 loss: 0.0143 lr: 0.02\n",
            "iteration: 52820 loss: 0.0124 lr: 0.02\n",
            "iteration: 52830 loss: 0.0211 lr: 0.02\n",
            "iteration: 52840 loss: 0.0104 lr: 0.02\n",
            "iteration: 52850 loss: 0.0084 lr: 0.02\n",
            "iteration: 52860 loss: 0.0175 lr: 0.02\n",
            "iteration: 52870 loss: 0.0120 lr: 0.02\n",
            "iteration: 52880 loss: 0.0123 lr: 0.02\n",
            "iteration: 52890 loss: 0.0121 lr: 0.02\n",
            "iteration: 52900 loss: 0.0159 lr: 0.02\n",
            "iteration: 52910 loss: 0.0121 lr: 0.02\n",
            "iteration: 52920 loss: 0.0137 lr: 0.02\n",
            "iteration: 52930 loss: 0.0127 lr: 0.02\n",
            "iteration: 52940 loss: 0.0139 lr: 0.02\n",
            "iteration: 52950 loss: 0.0101 lr: 0.02\n",
            "iteration: 52960 loss: 0.0137 lr: 0.02\n",
            "iteration: 52970 loss: 0.0154 lr: 0.02\n",
            "iteration: 52980 loss: 0.0159 lr: 0.02\n",
            "iteration: 52990 loss: 0.0155 lr: 0.02\n",
            "iteration: 53000 loss: 0.0087 lr: 0.02\n",
            "iteration: 53010 loss: 0.0123 lr: 0.02\n",
            "iteration: 53020 loss: 0.0136 lr: 0.02\n",
            "iteration: 53030 loss: 0.0125 lr: 0.02\n",
            "iteration: 53040 loss: 0.0130 lr: 0.02\n",
            "iteration: 53050 loss: 0.0115 lr: 0.02\n",
            "iteration: 53060 loss: 0.0132 lr: 0.02\n",
            "iteration: 53070 loss: 0.0136 lr: 0.02\n",
            "iteration: 53080 loss: 0.0121 lr: 0.02\n",
            "iteration: 53090 loss: 0.0123 lr: 0.02\n",
            "iteration: 53100 loss: 0.0146 lr: 0.02\n",
            "iteration: 53110 loss: 0.0150 lr: 0.02\n",
            "iteration: 53120 loss: 0.0151 lr: 0.02\n",
            "iteration: 53130 loss: 0.0196 lr: 0.02\n",
            "iteration: 53140 loss: 0.0111 lr: 0.02\n",
            "iteration: 53150 loss: 0.0150 lr: 0.02\n",
            "iteration: 53160 loss: 0.0120 lr: 0.02\n",
            "iteration: 53170 loss: 0.0128 lr: 0.02\n",
            "iteration: 53180 loss: 0.0130 lr: 0.02\n",
            "iteration: 53190 loss: 0.0155 lr: 0.02\n",
            "iteration: 53200 loss: 0.0127 lr: 0.02\n",
            "iteration: 53210 loss: 0.0166 lr: 0.02\n",
            "iteration: 53220 loss: 0.0117 lr: 0.02\n",
            "iteration: 53230 loss: 0.0194 lr: 0.02\n",
            "iteration: 53240 loss: 0.0130 lr: 0.02\n",
            "iteration: 53250 loss: 0.0116 lr: 0.02\n",
            "iteration: 53260 loss: 0.0137 lr: 0.02\n",
            "iteration: 53270 loss: 0.0166 lr: 0.02\n",
            "iteration: 53280 loss: 0.0081 lr: 0.02\n",
            "iteration: 53290 loss: 0.0166 lr: 0.02\n",
            "iteration: 53300 loss: 0.0132 lr: 0.02\n",
            "iteration: 53310 loss: 0.0125 lr: 0.02\n",
            "iteration: 53320 loss: 0.0139 lr: 0.02\n",
            "iteration: 53330 loss: 0.0126 lr: 0.02\n",
            "iteration: 53340 loss: 0.0174 lr: 0.02\n",
            "iteration: 53350 loss: 0.0187 lr: 0.02\n",
            "iteration: 53360 loss: 0.0161 lr: 0.02\n",
            "iteration: 53370 loss: 0.0127 lr: 0.02\n",
            "iteration: 53380 loss: 0.0110 lr: 0.02\n",
            "iteration: 53390 loss: 0.0169 lr: 0.02\n",
            "iteration: 53400 loss: 0.0140 lr: 0.02\n",
            "iteration: 53410 loss: 0.0167 lr: 0.02\n",
            "iteration: 53420 loss: 0.0181 lr: 0.02\n",
            "iteration: 53430 loss: 0.0117 lr: 0.02\n",
            "iteration: 53440 loss: 0.0163 lr: 0.02\n",
            "iteration: 53450 loss: 0.0128 lr: 0.02\n",
            "iteration: 53460 loss: 0.0090 lr: 0.02\n",
            "iteration: 53470 loss: 0.0177 lr: 0.02\n",
            "iteration: 53480 loss: 0.0128 lr: 0.02\n",
            "iteration: 53490 loss: 0.0168 lr: 0.02\n",
            "iteration: 53500 loss: 0.0158 lr: 0.02\n",
            "iteration: 53510 loss: 0.0142 lr: 0.02\n",
            "iteration: 53520 loss: 0.0087 lr: 0.02\n",
            "iteration: 53530 loss: 0.0164 lr: 0.02\n",
            "iteration: 53540 loss: 0.0111 lr: 0.02\n",
            "iteration: 53550 loss: 0.0103 lr: 0.02\n",
            "iteration: 53560 loss: 0.0141 lr: 0.02\n",
            "iteration: 53570 loss: 0.0169 lr: 0.02\n",
            "iteration: 53580 loss: 0.0132 lr: 0.02\n",
            "iteration: 53590 loss: 0.0149 lr: 0.02\n",
            "iteration: 53600 loss: 0.0129 lr: 0.02\n",
            "iteration: 53610 loss: 0.0143 lr: 0.02\n",
            "iteration: 53620 loss: 0.0140 lr: 0.02\n",
            "iteration: 53630 loss: 0.0115 lr: 0.02\n",
            "iteration: 53640 loss: 0.0093 lr: 0.02\n",
            "iteration: 53650 loss: 0.0110 lr: 0.02\n",
            "iteration: 53660 loss: 0.0154 lr: 0.02\n",
            "iteration: 53670 loss: 0.0119 lr: 0.02\n",
            "iteration: 53680 loss: 0.0121 lr: 0.02\n",
            "iteration: 53690 loss: 0.0188 lr: 0.02\n",
            "iteration: 53700 loss: 0.0135 lr: 0.02\n",
            "iteration: 53710 loss: 0.0093 lr: 0.02\n",
            "iteration: 53720 loss: 0.0148 lr: 0.02\n",
            "iteration: 53730 loss: 0.0142 lr: 0.02\n",
            "iteration: 53740 loss: 0.0150 lr: 0.02\n",
            "iteration: 53750 loss: 0.0128 lr: 0.02\n",
            "iteration: 53760 loss: 0.0159 lr: 0.02\n",
            "iteration: 53770 loss: 0.0120 lr: 0.02\n",
            "iteration: 53780 loss: 0.0121 lr: 0.02\n",
            "iteration: 53790 loss: 0.0154 lr: 0.02\n",
            "iteration: 53800 loss: 0.0128 lr: 0.02\n",
            "iteration: 53810 loss: 0.0117 lr: 0.02\n",
            "iteration: 53820 loss: 0.0146 lr: 0.02\n",
            "iteration: 53830 loss: 0.0096 lr: 0.02\n",
            "iteration: 53840 loss: 0.0158 lr: 0.02\n",
            "iteration: 53850 loss: 0.0117 lr: 0.02\n",
            "iteration: 53860 loss: 0.0136 lr: 0.02\n",
            "iteration: 53870 loss: 0.0143 lr: 0.02\n",
            "iteration: 53880 loss: 0.0094 lr: 0.02\n",
            "iteration: 53890 loss: 0.0140 lr: 0.02\n",
            "iteration: 53900 loss: 0.0109 lr: 0.02\n",
            "iteration: 53910 loss: 0.0122 lr: 0.02\n",
            "iteration: 53920 loss: 0.0115 lr: 0.02\n",
            "iteration: 53930 loss: 0.0158 lr: 0.02\n",
            "iteration: 53940 loss: 0.0198 lr: 0.02\n",
            "iteration: 53950 loss: 0.0119 lr: 0.02\n",
            "iteration: 53960 loss: 0.0174 lr: 0.02\n",
            "iteration: 53970 loss: 0.0151 lr: 0.02\n",
            "iteration: 53980 loss: 0.0152 lr: 0.02\n",
            "iteration: 53990 loss: 0.0129 lr: 0.02\n",
            "iteration: 54000 loss: 0.0130 lr: 0.02\n",
            "iteration: 54010 loss: 0.0113 lr: 0.02\n",
            "iteration: 54020 loss: 0.0128 lr: 0.02\n",
            "iteration: 54030 loss: 0.0122 lr: 0.02\n",
            "iteration: 54040 loss: 0.0160 lr: 0.02\n",
            "iteration: 54050 loss: 0.0113 lr: 0.02\n",
            "iteration: 54060 loss: 0.0124 lr: 0.02\n",
            "iteration: 54070 loss: 0.0218 lr: 0.02\n",
            "iteration: 54080 loss: 0.0127 lr: 0.02\n",
            "iteration: 54090 loss: 0.0097 lr: 0.02\n",
            "iteration: 54100 loss: 0.0159 lr: 0.02\n",
            "iteration: 54110 loss: 0.0132 lr: 0.02\n",
            "iteration: 54120 loss: 0.0157 lr: 0.02\n",
            "iteration: 54130 loss: 0.0174 lr: 0.02\n",
            "iteration: 54140 loss: 0.0109 lr: 0.02\n",
            "iteration: 54150 loss: 0.0112 lr: 0.02\n",
            "iteration: 54160 loss: 0.0203 lr: 0.02\n",
            "iteration: 54170 loss: 0.0118 lr: 0.02\n",
            "iteration: 54180 loss: 0.0124 lr: 0.02\n",
            "iteration: 54190 loss: 0.0145 lr: 0.02\n",
            "iteration: 54200 loss: 0.0126 lr: 0.02\n",
            "iteration: 54210 loss: 0.0096 lr: 0.02\n",
            "iteration: 54220 loss: 0.0114 lr: 0.02\n",
            "iteration: 54230 loss: 0.0179 lr: 0.02\n",
            "iteration: 54240 loss: 0.0174 lr: 0.02\n",
            "iteration: 54250 loss: 0.0163 lr: 0.02\n",
            "iteration: 54260 loss: 0.0182 lr: 0.02\n",
            "iteration: 54270 loss: 0.0155 lr: 0.02\n",
            "iteration: 54280 loss: 0.0107 lr: 0.02\n",
            "iteration: 54290 loss: 0.0151 lr: 0.02\n",
            "iteration: 54300 loss: 0.0121 lr: 0.02\n",
            "iteration: 54310 loss: 0.0134 lr: 0.02\n",
            "iteration: 54320 loss: 0.0173 lr: 0.02\n",
            "iteration: 54330 loss: 0.0192 lr: 0.02\n",
            "iteration: 54340 loss: 0.0152 lr: 0.02\n",
            "iteration: 54350 loss: 0.0168 lr: 0.02\n",
            "iteration: 54360 loss: 0.0143 lr: 0.02\n",
            "iteration: 54370 loss: 0.0152 lr: 0.02\n",
            "iteration: 54380 loss: 0.0159 lr: 0.02\n",
            "iteration: 54390 loss: 0.0133 lr: 0.02\n",
            "iteration: 54400 loss: 0.0115 lr: 0.02\n",
            "iteration: 54410 loss: 0.0135 lr: 0.02\n",
            "iteration: 54420 loss: 0.0148 lr: 0.02\n",
            "iteration: 54430 loss: 0.0154 lr: 0.02\n",
            "iteration: 54440 loss: 0.0131 lr: 0.02\n",
            "iteration: 54450 loss: 0.0113 lr: 0.02\n",
            "iteration: 54460 loss: 0.0135 lr: 0.02\n",
            "iteration: 54470 loss: 0.0194 lr: 0.02\n",
            "iteration: 54480 loss: 0.0093 lr: 0.02\n",
            "iteration: 54490 loss: 0.0144 lr: 0.02\n",
            "iteration: 54500 loss: 0.0133 lr: 0.02\n",
            "iteration: 54510 loss: 0.0097 lr: 0.02\n",
            "iteration: 54520 loss: 0.0140 lr: 0.02\n",
            "iteration: 54530 loss: 0.0150 lr: 0.02\n",
            "iteration: 54540 loss: 0.0141 lr: 0.02\n",
            "iteration: 54550 loss: 0.0176 lr: 0.02\n",
            "iteration: 54560 loss: 0.0184 lr: 0.02\n",
            "iteration: 54570 loss: 0.0196 lr: 0.02\n",
            "iteration: 54580 loss: 0.0125 lr: 0.02\n",
            "iteration: 54590 loss: 0.0234 lr: 0.02\n",
            "iteration: 54600 loss: 0.0096 lr: 0.02\n",
            "iteration: 54610 loss: 0.0166 lr: 0.02\n",
            "iteration: 54620 loss: 0.0140 lr: 0.02\n",
            "iteration: 54630 loss: 0.0130 lr: 0.02\n",
            "iteration: 54640 loss: 0.0134 lr: 0.02\n",
            "iteration: 54650 loss: 0.0109 lr: 0.02\n",
            "iteration: 54660 loss: 0.0206 lr: 0.02\n",
            "iteration: 54670 loss: 0.0133 lr: 0.02\n",
            "iteration: 54680 loss: 0.0129 lr: 0.02\n",
            "iteration: 54690 loss: 0.0093 lr: 0.02\n",
            "iteration: 54700 loss: 0.0129 lr: 0.02\n",
            "iteration: 54710 loss: 0.0164 lr: 0.02\n",
            "iteration: 54720 loss: 0.0137 lr: 0.02\n",
            "iteration: 54730 loss: 0.0127 lr: 0.02\n",
            "iteration: 54740 loss: 0.0173 lr: 0.02\n",
            "iteration: 54750 loss: 0.0111 lr: 0.02\n",
            "iteration: 54760 loss: 0.0099 lr: 0.02\n",
            "iteration: 54770 loss: 0.0098 lr: 0.02\n",
            "iteration: 54780 loss: 0.0147 lr: 0.02\n",
            "iteration: 54790 loss: 0.0120 lr: 0.02\n",
            "iteration: 54800 loss: 0.0152 lr: 0.02\n",
            "iteration: 54810 loss: 0.0108 lr: 0.02\n",
            "iteration: 54820 loss: 0.0129 lr: 0.02\n",
            "iteration: 54830 loss: 0.0102 lr: 0.02\n",
            "iteration: 54840 loss: 0.0106 lr: 0.02\n",
            "iteration: 54850 loss: 0.0089 lr: 0.02\n",
            "iteration: 54860 loss: 0.0110 lr: 0.02\n",
            "iteration: 54870 loss: 0.0111 lr: 0.02\n",
            "iteration: 54880 loss: 0.0091 lr: 0.02\n",
            "iteration: 54890 loss: 0.0100 lr: 0.02\n",
            "iteration: 54900 loss: 0.0130 lr: 0.02\n",
            "iteration: 54910 loss: 0.0120 lr: 0.02\n",
            "iteration: 54920 loss: 0.0145 lr: 0.02\n",
            "iteration: 54930 loss: 0.0125 lr: 0.02\n",
            "iteration: 54940 loss: 0.0128 lr: 0.02\n",
            "iteration: 54950 loss: 0.0116 lr: 0.02\n",
            "iteration: 54960 loss: 0.0191 lr: 0.02\n",
            "iteration: 54970 loss: 0.0150 lr: 0.02\n",
            "iteration: 54980 loss: 0.0108 lr: 0.02\n",
            "iteration: 54990 loss: 0.0151 lr: 0.02\n",
            "iteration: 55000 loss: 0.0109 lr: 0.02\n",
            "iteration: 55010 loss: 0.0159 lr: 0.02\n",
            "iteration: 55020 loss: 0.0110 lr: 0.02\n",
            "iteration: 55030 loss: 0.0185 lr: 0.02\n",
            "iteration: 55040 loss: 0.0156 lr: 0.02\n",
            "iteration: 55050 loss: 0.0118 lr: 0.02\n",
            "iteration: 55060 loss: 0.0147 lr: 0.02\n",
            "iteration: 55070 loss: 0.0135 lr: 0.02\n",
            "iteration: 55080 loss: 0.0152 lr: 0.02\n",
            "iteration: 55090 loss: 0.0108 lr: 0.02\n",
            "iteration: 55100 loss: 0.0129 lr: 0.02\n",
            "iteration: 55110 loss: 0.0117 lr: 0.02\n",
            "iteration: 55120 loss: 0.0124 lr: 0.02\n",
            "iteration: 55130 loss: 0.0186 lr: 0.02\n",
            "iteration: 55140 loss: 0.0148 lr: 0.02\n",
            "iteration: 55150 loss: 0.0119 lr: 0.02\n",
            "iteration: 55160 loss: 0.0127 lr: 0.02\n",
            "iteration: 55170 loss: 0.0148 lr: 0.02\n",
            "iteration: 55180 loss: 0.0171 lr: 0.02\n",
            "iteration: 55190 loss: 0.0124 lr: 0.02\n",
            "iteration: 55200 loss: 0.0158 lr: 0.02\n",
            "iteration: 55210 loss: 0.0121 lr: 0.02\n",
            "iteration: 55220 loss: 0.0133 lr: 0.02\n",
            "iteration: 55230 loss: 0.0139 lr: 0.02\n",
            "iteration: 55240 loss: 0.0114 lr: 0.02\n",
            "iteration: 55250 loss: 0.0131 lr: 0.02\n",
            "iteration: 55260 loss: 0.0117 lr: 0.02\n",
            "iteration: 55270 loss: 0.0120 lr: 0.02\n",
            "iteration: 55280 loss: 0.0141 lr: 0.02\n",
            "iteration: 55290 loss: 0.0108 lr: 0.02\n",
            "iteration: 55300 loss: 0.0110 lr: 0.02\n",
            "iteration: 55310 loss: 0.0130 lr: 0.02\n",
            "iteration: 55320 loss: 0.0192 lr: 0.02\n",
            "iteration: 55330 loss: 0.0100 lr: 0.02\n",
            "iteration: 55340 loss: 0.0113 lr: 0.02\n",
            "iteration: 55350 loss: 0.0150 lr: 0.02\n",
            "iteration: 55360 loss: 0.0162 lr: 0.02\n",
            "iteration: 55370 loss: 0.0197 lr: 0.02\n",
            "iteration: 55380 loss: 0.0161 lr: 0.02\n",
            "iteration: 55390 loss: 0.0140 lr: 0.02\n",
            "iteration: 55400 loss: 0.0134 lr: 0.02\n",
            "iteration: 55410 loss: 0.0123 lr: 0.02\n",
            "iteration: 55420 loss: 0.0115 lr: 0.02\n",
            "iteration: 55430 loss: 0.0186 lr: 0.02\n",
            "iteration: 55440 loss: 0.0159 lr: 0.02\n",
            "iteration: 55450 loss: 0.0099 lr: 0.02\n",
            "iteration: 55460 loss: 0.0204 lr: 0.02\n",
            "iteration: 55470 loss: 0.0176 lr: 0.02\n",
            "iteration: 55480 loss: 0.0154 lr: 0.02\n",
            "iteration: 55490 loss: 0.0138 lr: 0.02\n",
            "iteration: 55500 loss: 0.0108 lr: 0.02\n",
            "iteration: 55510 loss: 0.0077 lr: 0.02\n",
            "iteration: 55520 loss: 0.0209 lr: 0.02\n",
            "iteration: 55530 loss: 0.0155 lr: 0.02\n",
            "iteration: 55540 loss: 0.0145 lr: 0.02\n",
            "iteration: 55550 loss: 0.0157 lr: 0.02\n",
            "iteration: 55560 loss: 0.0127 lr: 0.02\n",
            "iteration: 55570 loss: 0.0145 lr: 0.02\n",
            "iteration: 55580 loss: 0.0144 lr: 0.02\n",
            "iteration: 55590 loss: 0.0120 lr: 0.02\n",
            "iteration: 55600 loss: 0.0154 lr: 0.02\n",
            "iteration: 55610 loss: 0.0118 lr: 0.02\n",
            "iteration: 55620 loss: 0.0143 lr: 0.02\n",
            "iteration: 55630 loss: 0.0183 lr: 0.02\n",
            "iteration: 55640 loss: 0.0124 lr: 0.02\n",
            "iteration: 55650 loss: 0.0154 lr: 0.02\n",
            "iteration: 55660 loss: 0.0188 lr: 0.02\n",
            "iteration: 55670 loss: 0.0230 lr: 0.02\n",
            "iteration: 55680 loss: 0.0152 lr: 0.02\n",
            "iteration: 55690 loss: 0.0116 lr: 0.02\n",
            "iteration: 55700 loss: 0.0283 lr: 0.02\n",
            "iteration: 55710 loss: 0.0148 lr: 0.02\n",
            "iteration: 55720 loss: 0.0165 lr: 0.02\n",
            "iteration: 55730 loss: 0.0121 lr: 0.02\n",
            "iteration: 55740 loss: 0.0143 lr: 0.02\n",
            "iteration: 55750 loss: 0.0229 lr: 0.02\n",
            "iteration: 55760 loss: 0.0179 lr: 0.02\n",
            "iteration: 55770 loss: 0.0115 lr: 0.02\n",
            "iteration: 55780 loss: 0.0140 lr: 0.02\n",
            "iteration: 55790 loss: 0.0124 lr: 0.02\n",
            "iteration: 55800 loss: 0.0120 lr: 0.02\n",
            "iteration: 55810 loss: 0.0082 lr: 0.02\n",
            "iteration: 55820 loss: 0.0134 lr: 0.02\n",
            "iteration: 55830 loss: 0.0139 lr: 0.02\n",
            "iteration: 55840 loss: 0.0127 lr: 0.02\n",
            "iteration: 55850 loss: 0.0144 lr: 0.02\n",
            "iteration: 55860 loss: 0.0202 lr: 0.02\n",
            "iteration: 55870 loss: 0.0108 lr: 0.02\n",
            "iteration: 55880 loss: 0.0112 lr: 0.02\n",
            "iteration: 55890 loss: 0.0097 lr: 0.02\n",
            "iteration: 55900 loss: 0.0153 lr: 0.02\n",
            "iteration: 55910 loss: 0.0202 lr: 0.02\n",
            "iteration: 55920 loss: 0.0208 lr: 0.02\n",
            "iteration: 55930 loss: 0.0139 lr: 0.02\n",
            "iteration: 55940 loss: 0.0149 lr: 0.02\n",
            "iteration: 55950 loss: 0.0167 lr: 0.02\n",
            "iteration: 55960 loss: 0.0137 lr: 0.02\n",
            "iteration: 55970 loss: 0.0161 lr: 0.02\n",
            "iteration: 55980 loss: 0.0154 lr: 0.02\n",
            "iteration: 55990 loss: 0.0266 lr: 0.02\n",
            "iteration: 56000 loss: 0.0159 lr: 0.02\n",
            "iteration: 56010 loss: 0.0212 lr: 0.02\n",
            "iteration: 56020 loss: 0.0162 lr: 0.02\n",
            "iteration: 56030 loss: 0.0144 lr: 0.02\n",
            "iteration: 56040 loss: 0.0134 lr: 0.02\n",
            "iteration: 56050 loss: 0.0104 lr: 0.02\n",
            "iteration: 56060 loss: 0.0130 lr: 0.02\n",
            "iteration: 56070 loss: 0.0120 lr: 0.02\n",
            "iteration: 56080 loss: 0.0109 lr: 0.02\n",
            "iteration: 56090 loss: 0.0122 lr: 0.02\n",
            "iteration: 56100 loss: 0.0139 lr: 0.02\n",
            "iteration: 56110 loss: 0.0150 lr: 0.02\n",
            "iteration: 56120 loss: 0.0128 lr: 0.02\n",
            "iteration: 56130 loss: 0.0166 lr: 0.02\n",
            "iteration: 56140 loss: 0.0122 lr: 0.02\n",
            "iteration: 56150 loss: 0.0136 lr: 0.02\n",
            "iteration: 56160 loss: 0.0139 lr: 0.02\n",
            "iteration: 56170 loss: 0.0114 lr: 0.02\n",
            "iteration: 56180 loss: 0.0121 lr: 0.02\n",
            "iteration: 56190 loss: 0.0099 lr: 0.02\n",
            "iteration: 56200 loss: 0.0128 lr: 0.02\n",
            "iteration: 56210 loss: 0.0127 lr: 0.02\n",
            "iteration: 56220 loss: 0.0157 lr: 0.02\n",
            "iteration: 56230 loss: 0.0166 lr: 0.02\n",
            "iteration: 56240 loss: 0.0119 lr: 0.02\n",
            "iteration: 56250 loss: 0.0098 lr: 0.02\n",
            "iteration: 56260 loss: 0.0131 lr: 0.02\n",
            "iteration: 56270 loss: 0.0215 lr: 0.02\n",
            "iteration: 56280 loss: 0.0121 lr: 0.02\n",
            "iteration: 56290 loss: 0.0086 lr: 0.02\n",
            "iteration: 56300 loss: 0.0165 lr: 0.02\n",
            "iteration: 56310 loss: 0.0140 lr: 0.02\n",
            "iteration: 56320 loss: 0.0119 lr: 0.02\n",
            "iteration: 56330 loss: 0.0169 lr: 0.02\n",
            "iteration: 56340 loss: 0.0172 lr: 0.02\n",
            "iteration: 56350 loss: 0.0115 lr: 0.02\n",
            "iteration: 56360 loss: 0.0121 lr: 0.02\n",
            "iteration: 56370 loss: 0.0109 lr: 0.02\n",
            "iteration: 56380 loss: 0.0107 lr: 0.02\n",
            "iteration: 56390 loss: 0.0108 lr: 0.02\n",
            "iteration: 56400 loss: 0.0128 lr: 0.02\n",
            "iteration: 56410 loss: 0.0131 lr: 0.02\n",
            "iteration: 56420 loss: 0.0111 lr: 0.02\n",
            "iteration: 56430 loss: 0.0092 lr: 0.02\n",
            "iteration: 56440 loss: 0.0125 lr: 0.02\n",
            "iteration: 56450 loss: 0.0096 lr: 0.02\n",
            "iteration: 56460 loss: 0.0117 lr: 0.02\n",
            "iteration: 56470 loss: 0.0111 lr: 0.02\n",
            "iteration: 56480 loss: 0.0144 lr: 0.02\n",
            "iteration: 56490 loss: 0.0114 lr: 0.02\n",
            "iteration: 56500 loss: 0.0123 lr: 0.02\n",
            "iteration: 56510 loss: 0.0171 lr: 0.02\n",
            "iteration: 56520 loss: 0.0110 lr: 0.02\n",
            "iteration: 56530 loss: 0.0115 lr: 0.02\n",
            "iteration: 56540 loss: 0.0134 lr: 0.02\n",
            "iteration: 56550 loss: 0.0166 lr: 0.02\n",
            "iteration: 56560 loss: 0.0235 lr: 0.02\n",
            "iteration: 56570 loss: 0.0241 lr: 0.02\n",
            "iteration: 56580 loss: 0.0099 lr: 0.02\n",
            "iteration: 56590 loss: 0.0111 lr: 0.02\n",
            "iteration: 56600 loss: 0.0141 lr: 0.02\n",
            "iteration: 56610 loss: 0.0143 lr: 0.02\n",
            "iteration: 56620 loss: 0.0134 lr: 0.02\n",
            "iteration: 56630 loss: 0.0136 lr: 0.02\n",
            "iteration: 56640 loss: 0.0136 lr: 0.02\n",
            "iteration: 56650 loss: 0.0091 lr: 0.02\n",
            "iteration: 56660 loss: 0.0095 lr: 0.02\n",
            "iteration: 56670 loss: 0.0098 lr: 0.02\n",
            "iteration: 56680 loss: 0.0134 lr: 0.02\n",
            "iteration: 56690 loss: 0.0143 lr: 0.02\n",
            "iteration: 56700 loss: 0.0130 lr: 0.02\n",
            "iteration: 56710 loss: 0.0107 lr: 0.02\n",
            "iteration: 56720 loss: 0.0123 lr: 0.02\n",
            "iteration: 56730 loss: 0.0113 lr: 0.02\n",
            "iteration: 56740 loss: 0.0116 lr: 0.02\n",
            "iteration: 56750 loss: 0.0158 lr: 0.02\n",
            "iteration: 56760 loss: 0.0115 lr: 0.02\n",
            "iteration: 56770 loss: 0.0128 lr: 0.02\n",
            "iteration: 56780 loss: 0.0135 lr: 0.02\n",
            "iteration: 56790 loss: 0.0137 lr: 0.02\n",
            "iteration: 56800 loss: 0.0154 lr: 0.02\n",
            "iteration: 56810 loss: 0.0148 lr: 0.02\n",
            "iteration: 56820 loss: 0.0195 lr: 0.02\n",
            "iteration: 56830 loss: 0.0177 lr: 0.02\n",
            "iteration: 56840 loss: 0.0102 lr: 0.02\n",
            "iteration: 56850 loss: 0.0173 lr: 0.02\n",
            "iteration: 56860 loss: 0.0137 lr: 0.02\n",
            "iteration: 56870 loss: 0.0161 lr: 0.02\n",
            "iteration: 56880 loss: 0.0111 lr: 0.02\n",
            "iteration: 56890 loss: 0.0121 lr: 0.02\n",
            "iteration: 56900 loss: 0.0138 lr: 0.02\n",
            "iteration: 56910 loss: 0.0165 lr: 0.02\n",
            "iteration: 56920 loss: 0.0159 lr: 0.02\n",
            "iteration: 56930 loss: 0.0095 lr: 0.02\n",
            "iteration: 56940 loss: 0.0103 lr: 0.02\n",
            "iteration: 56950 loss: 0.0137 lr: 0.02\n",
            "iteration: 56960 loss: 0.0158 lr: 0.02\n",
            "iteration: 56970 loss: 0.0147 lr: 0.02\n",
            "iteration: 56980 loss: 0.0225 lr: 0.02\n",
            "iteration: 56990 loss: 0.0186 lr: 0.02\n",
            "iteration: 57000 loss: 0.0145 lr: 0.02\n",
            "iteration: 57010 loss: 0.0133 lr: 0.02\n",
            "iteration: 57020 loss: 0.0142 lr: 0.02\n",
            "iteration: 57030 loss: 0.0191 lr: 0.02\n",
            "iteration: 57040 loss: 0.0135 lr: 0.02\n",
            "iteration: 57050 loss: 0.0196 lr: 0.02\n",
            "iteration: 57060 loss: 0.0146 lr: 0.02\n",
            "iteration: 57070 loss: 0.0095 lr: 0.02\n",
            "iteration: 57080 loss: 0.0148 lr: 0.02\n",
            "iteration: 57090 loss: 0.0153 lr: 0.02\n",
            "iteration: 57100 loss: 0.0122 lr: 0.02\n",
            "iteration: 57110 loss: 0.0106 lr: 0.02\n",
            "iteration: 57120 loss: 0.0109 lr: 0.02\n",
            "iteration: 57130 loss: 0.0132 lr: 0.02\n",
            "iteration: 57140 loss: 0.0094 lr: 0.02\n",
            "iteration: 57150 loss: 0.0151 lr: 0.02\n",
            "iteration: 57160 loss: 0.0116 lr: 0.02\n",
            "iteration: 57170 loss: 0.0130 lr: 0.02\n",
            "iteration: 57180 loss: 0.0180 lr: 0.02\n",
            "iteration: 57190 loss: 0.0161 lr: 0.02\n",
            "iteration: 57200 loss: 0.0136 lr: 0.02\n",
            "iteration: 57210 loss: 0.0109 lr: 0.02\n",
            "iteration: 57220 loss: 0.0136 lr: 0.02\n",
            "iteration: 57230 loss: 0.0174 lr: 0.02\n",
            "iteration: 57240 loss: 0.0175 lr: 0.02\n",
            "iteration: 57250 loss: 0.0177 lr: 0.02\n",
            "iteration: 57260 loss: 0.0151 lr: 0.02\n",
            "iteration: 57270 loss: 0.0109 lr: 0.02\n",
            "iteration: 57280 loss: 0.0113 lr: 0.02\n",
            "iteration: 57290 loss: 0.0122 lr: 0.02\n",
            "iteration: 57300 loss: 0.0109 lr: 0.02\n",
            "iteration: 57310 loss: 0.0113 lr: 0.02\n",
            "iteration: 57320 loss: 0.0199 lr: 0.02\n",
            "iteration: 57330 loss: 0.0235 lr: 0.02\n",
            "iteration: 57340 loss: 0.0119 lr: 0.02\n",
            "iteration: 57350 loss: 0.0134 lr: 0.02\n",
            "iteration: 57360 loss: 0.0128 lr: 0.02\n",
            "iteration: 57370 loss: 0.0116 lr: 0.02\n",
            "iteration: 57380 loss: 0.0150 lr: 0.02\n",
            "iteration: 57390 loss: 0.0160 lr: 0.02\n",
            "iteration: 57400 loss: 0.0122 lr: 0.02\n",
            "iteration: 57410 loss: 0.0119 lr: 0.02\n",
            "iteration: 57420 loss: 0.0134 lr: 0.02\n",
            "iteration: 57430 loss: 0.0102 lr: 0.02\n",
            "iteration: 57440 loss: 0.0099 lr: 0.02\n",
            "iteration: 57450 loss: 0.0128 lr: 0.02\n",
            "iteration: 57460 loss: 0.0106 lr: 0.02\n",
            "iteration: 57470 loss: 0.0162 lr: 0.02\n",
            "iteration: 57480 loss: 0.0168 lr: 0.02\n",
            "iteration: 57490 loss: 0.0179 lr: 0.02\n",
            "iteration: 57500 loss: 0.0157 lr: 0.02\n",
            "iteration: 57510 loss: 0.0269 lr: 0.02\n",
            "iteration: 57520 loss: 0.0103 lr: 0.02\n",
            "iteration: 57530 loss: 0.0140 lr: 0.02\n",
            "iteration: 57540 loss: 0.0131 lr: 0.02\n",
            "iteration: 57550 loss: 0.0156 lr: 0.02\n",
            "iteration: 57560 loss: 0.0205 lr: 0.02\n",
            "iteration: 57570 loss: 0.0132 lr: 0.02\n",
            "iteration: 57580 loss: 0.0106 lr: 0.02\n",
            "iteration: 57590 loss: 0.0141 lr: 0.02\n",
            "iteration: 57600 loss: 0.0116 lr: 0.02\n",
            "iteration: 57610 loss: 0.0122 lr: 0.02\n",
            "iteration: 57620 loss: 0.0205 lr: 0.02\n",
            "iteration: 57630 loss: 0.0140 lr: 0.02\n",
            "iteration: 57640 loss: 0.0107 lr: 0.02\n",
            "iteration: 57650 loss: 0.0146 lr: 0.02\n",
            "iteration: 57660 loss: 0.0124 lr: 0.02\n",
            "iteration: 57670 loss: 0.0128 lr: 0.02\n",
            "iteration: 57680 loss: 0.0091 lr: 0.02\n",
            "iteration: 57690 loss: 0.0155 lr: 0.02\n",
            "iteration: 57700 loss: 0.0128 lr: 0.02\n",
            "iteration: 57710 loss: 0.0149 lr: 0.02\n",
            "iteration: 57720 loss: 0.0151 lr: 0.02\n",
            "iteration: 57730 loss: 0.0115 lr: 0.02\n",
            "iteration: 57740 loss: 0.0169 lr: 0.02\n",
            "iteration: 57750 loss: 0.0169 lr: 0.02\n",
            "iteration: 57760 loss: 0.0183 lr: 0.02\n",
            "iteration: 57770 loss: 0.0130 lr: 0.02\n",
            "iteration: 57780 loss: 0.0124 lr: 0.02\n",
            "iteration: 57790 loss: 0.0147 lr: 0.02\n",
            "iteration: 57800 loss: 0.0147 lr: 0.02\n",
            "iteration: 57810 loss: 0.0148 lr: 0.02\n",
            "iteration: 57820 loss: 0.0119 lr: 0.02\n",
            "iteration: 57830 loss: 0.0126 lr: 0.02\n",
            "iteration: 57840 loss: 0.0152 lr: 0.02\n",
            "iteration: 57850 loss: 0.0151 lr: 0.02\n",
            "iteration: 57860 loss: 0.0106 lr: 0.02\n",
            "iteration: 57870 loss: 0.0140 lr: 0.02\n",
            "iteration: 57880 loss: 0.0107 lr: 0.02\n",
            "iteration: 57890 loss: 0.0183 lr: 0.02\n",
            "iteration: 57900 loss: 0.0120 lr: 0.02\n",
            "iteration: 57910 loss: 0.0124 lr: 0.02\n",
            "iteration: 57920 loss: 0.0094 lr: 0.02\n",
            "iteration: 57930 loss: 0.0122 lr: 0.02\n",
            "iteration: 57940 loss: 0.0130 lr: 0.02\n",
            "iteration: 57950 loss: 0.0149 lr: 0.02\n",
            "iteration: 57960 loss: 0.0159 lr: 0.02\n",
            "iteration: 57970 loss: 0.0178 lr: 0.02\n",
            "iteration: 57980 loss: 0.0122 lr: 0.02\n",
            "iteration: 57990 loss: 0.0121 lr: 0.02\n",
            "iteration: 58000 loss: 0.0121 lr: 0.02\n",
            "iteration: 58010 loss: 0.0123 lr: 0.02\n",
            "iteration: 58020 loss: 0.0098 lr: 0.02\n",
            "iteration: 58030 loss: 0.0170 lr: 0.02\n",
            "iteration: 58040 loss: 0.0132 lr: 0.02\n",
            "iteration: 58050 loss: 0.0123 lr: 0.02\n",
            "iteration: 58060 loss: 0.0164 lr: 0.02\n",
            "iteration: 58070 loss: 0.0087 lr: 0.02\n",
            "iteration: 58080 loss: 0.0102 lr: 0.02\n",
            "iteration: 58090 loss: 0.0111 lr: 0.02\n",
            "iteration: 58100 loss: 0.0151 lr: 0.02\n",
            "iteration: 58110 loss: 0.0125 lr: 0.02\n",
            "iteration: 58120 loss: 0.0076 lr: 0.02\n",
            "iteration: 58130 loss: 0.0139 lr: 0.02\n",
            "iteration: 58140 loss: 0.0159 lr: 0.02\n",
            "iteration: 58150 loss: 0.0126 lr: 0.02\n",
            "iteration: 58160 loss: 0.0135 lr: 0.02\n",
            "iteration: 58170 loss: 0.0184 lr: 0.02\n",
            "iteration: 58180 loss: 0.0166 lr: 0.02\n",
            "iteration: 58190 loss: 0.0133 lr: 0.02\n",
            "iteration: 58200 loss: 0.0113 lr: 0.02\n",
            "iteration: 58210 loss: 0.0123 lr: 0.02\n",
            "iteration: 58220 loss: 0.0198 lr: 0.02\n",
            "iteration: 58230 loss: 0.0124 lr: 0.02\n",
            "iteration: 58240 loss: 0.0163 lr: 0.02\n",
            "iteration: 58250 loss: 0.0099 lr: 0.02\n",
            "iteration: 58260 loss: 0.0137 lr: 0.02\n",
            "iteration: 58270 loss: 0.0093 lr: 0.02\n",
            "iteration: 58280 loss: 0.0116 lr: 0.02\n",
            "iteration: 58290 loss: 0.0131 lr: 0.02\n",
            "iteration: 58300 loss: 0.0122 lr: 0.02\n",
            "iteration: 58310 loss: 0.0102 lr: 0.02\n",
            "iteration: 58320 loss: 0.0114 lr: 0.02\n",
            "iteration: 58330 loss: 0.0167 lr: 0.02\n",
            "iteration: 58340 loss: 0.0134 lr: 0.02\n",
            "iteration: 58350 loss: 0.0138 lr: 0.02\n",
            "iteration: 58360 loss: 0.0117 lr: 0.02\n",
            "iteration: 58370 loss: 0.0128 lr: 0.02\n",
            "iteration: 58380 loss: 0.0126 lr: 0.02\n",
            "iteration: 58390 loss: 0.0157 lr: 0.02\n",
            "iteration: 58400 loss: 0.0146 lr: 0.02\n",
            "iteration: 58410 loss: 0.0125 lr: 0.02\n",
            "iteration: 58420 loss: 0.0123 lr: 0.02\n",
            "iteration: 58430 loss: 0.0180 lr: 0.02\n",
            "iteration: 58440 loss: 0.0130 lr: 0.02\n",
            "iteration: 58450 loss: 0.0095 lr: 0.02\n",
            "iteration: 58460 loss: 0.0174 lr: 0.02\n",
            "iteration: 58470 loss: 0.0206 lr: 0.02\n",
            "iteration: 58480 loss: 0.0132 lr: 0.02\n",
            "iteration: 58490 loss: 0.0112 lr: 0.02\n",
            "iteration: 58500 loss: 0.0142 lr: 0.02\n",
            "iteration: 58510 loss: 0.0148 lr: 0.02\n",
            "iteration: 58520 loss: 0.0118 lr: 0.02\n",
            "iteration: 58530 loss: 0.0176 lr: 0.02\n",
            "iteration: 58540 loss: 0.0070 lr: 0.02\n",
            "iteration: 58550 loss: 0.0093 lr: 0.02\n",
            "iteration: 58560 loss: 0.0124 lr: 0.02\n",
            "iteration: 58570 loss: 0.0157 lr: 0.02\n",
            "iteration: 58580 loss: 0.0138 lr: 0.02\n",
            "iteration: 58590 loss: 0.0118 lr: 0.02\n",
            "iteration: 58600 loss: 0.0144 lr: 0.02\n",
            "iteration: 58610 loss: 0.0112 lr: 0.02\n",
            "iteration: 58620 loss: 0.0108 lr: 0.02\n",
            "iteration: 58630 loss: 0.0101 lr: 0.02\n",
            "iteration: 58640 loss: 0.0111 lr: 0.02\n",
            "iteration: 58650 loss: 0.0136 lr: 0.02\n",
            "iteration: 58660 loss: 0.0126 lr: 0.02\n",
            "iteration: 58670 loss: 0.0095 lr: 0.02\n",
            "iteration: 58680 loss: 0.0104 lr: 0.02\n",
            "iteration: 58690 loss: 0.0093 lr: 0.02\n",
            "iteration: 58700 loss: 0.0116 lr: 0.02\n",
            "iteration: 58710 loss: 0.0132 lr: 0.02\n",
            "iteration: 58720 loss: 0.0109 lr: 0.02\n",
            "iteration: 58730 loss: 0.0094 lr: 0.02\n",
            "iteration: 58740 loss: 0.0151 lr: 0.02\n",
            "iteration: 58750 loss: 0.0128 lr: 0.02\n",
            "iteration: 58760 loss: 0.0146 lr: 0.02\n",
            "iteration: 58770 loss: 0.0098 lr: 0.02\n",
            "iteration: 58780 loss: 0.0180 lr: 0.02\n",
            "iteration: 58790 loss: 0.0143 lr: 0.02\n",
            "iteration: 58800 loss: 0.0127 lr: 0.02\n",
            "iteration: 58810 loss: 0.0152 lr: 0.02\n",
            "iteration: 58820 loss: 0.0159 lr: 0.02\n",
            "iteration: 58830 loss: 0.0150 lr: 0.02\n",
            "iteration: 58840 loss: 0.0139 lr: 0.02\n",
            "iteration: 58850 loss: 0.0136 lr: 0.02\n",
            "iteration: 58860 loss: 0.0196 lr: 0.02\n",
            "iteration: 58870 loss: 0.0116 lr: 0.02\n",
            "iteration: 58880 loss: 0.0106 lr: 0.02\n",
            "iteration: 58890 loss: 0.0149 lr: 0.02\n",
            "iteration: 58900 loss: 0.0106 lr: 0.02\n",
            "iteration: 58910 loss: 0.0108 lr: 0.02\n",
            "iteration: 58920 loss: 0.0184 lr: 0.02\n",
            "iteration: 58930 loss: 0.0105 lr: 0.02\n",
            "iteration: 58940 loss: 0.0165 lr: 0.02\n",
            "iteration: 58950 loss: 0.0139 lr: 0.02\n",
            "iteration: 58960 loss: 0.0100 lr: 0.02\n",
            "iteration: 58970 loss: 0.0153 lr: 0.02\n",
            "iteration: 58980 loss: 0.0174 lr: 0.02\n",
            "iteration: 58990 loss: 0.0114 lr: 0.02\n",
            "iteration: 59000 loss: 0.0105 lr: 0.02\n",
            "iteration: 59010 loss: 0.0144 lr: 0.02\n",
            "iteration: 59020 loss: 0.0137 lr: 0.02\n",
            "iteration: 59030 loss: 0.0137 lr: 0.02\n",
            "iteration: 59040 loss: 0.0103 lr: 0.02\n",
            "iteration: 59050 loss: 0.0152 lr: 0.02\n",
            "iteration: 59060 loss: 0.0151 lr: 0.02\n",
            "iteration: 59070 loss: 0.0186 lr: 0.02\n",
            "iteration: 59080 loss: 0.0101 lr: 0.02\n",
            "iteration: 59090 loss: 0.0131 lr: 0.02\n",
            "iteration: 59100 loss: 0.0199 lr: 0.02\n",
            "iteration: 59110 loss: 0.0099 lr: 0.02\n",
            "iteration: 59120 loss: 0.0128 lr: 0.02\n",
            "iteration: 59130 loss: 0.0128 lr: 0.02\n",
            "iteration: 59140 loss: 0.0159 lr: 0.02\n",
            "iteration: 59150 loss: 0.0145 lr: 0.02\n",
            "iteration: 59160 loss: 0.0121 lr: 0.02\n",
            "iteration: 59170 loss: 0.0117 lr: 0.02\n",
            "iteration: 59180 loss: 0.0141 lr: 0.02\n",
            "iteration: 59190 loss: 0.0116 lr: 0.02\n",
            "iteration: 59200 loss: 0.0137 lr: 0.02\n",
            "iteration: 59210 loss: 0.0207 lr: 0.02\n",
            "iteration: 59220 loss: 0.0214 lr: 0.02\n",
            "iteration: 59230 loss: 0.0099 lr: 0.02\n",
            "iteration: 59240 loss: 0.0158 lr: 0.02\n",
            "iteration: 59250 loss: 0.0144 lr: 0.02\n",
            "iteration: 59260 loss: 0.0143 lr: 0.02\n",
            "iteration: 59270 loss: 0.0136 lr: 0.02\n",
            "iteration: 59280 loss: 0.0112 lr: 0.02\n",
            "iteration: 59290 loss: 0.0184 lr: 0.02\n",
            "iteration: 59300 loss: 0.0240 lr: 0.02\n",
            "iteration: 59310 loss: 0.0097 lr: 0.02\n",
            "iteration: 59320 loss: 0.0168 lr: 0.02\n",
            "iteration: 59330 loss: 0.0165 lr: 0.02\n",
            "iteration: 59340 loss: 0.0163 lr: 0.02\n",
            "iteration: 59350 loss: 0.0118 lr: 0.02\n",
            "iteration: 59360 loss: 0.0142 lr: 0.02\n",
            "iteration: 59370 loss: 0.0100 lr: 0.02\n",
            "iteration: 59380 loss: 0.0112 lr: 0.02\n",
            "iteration: 59390 loss: 0.0132 lr: 0.02\n",
            "iteration: 59400 loss: 0.0123 lr: 0.02\n",
            "iteration: 59410 loss: 0.0131 lr: 0.02\n",
            "iteration: 59420 loss: 0.0147 lr: 0.02\n",
            "iteration: 59430 loss: 0.0127 lr: 0.02\n",
            "iteration: 59440 loss: 0.0123 lr: 0.02\n",
            "iteration: 59450 loss: 0.0139 lr: 0.02\n",
            "iteration: 59460 loss: 0.0186 lr: 0.02\n",
            "iteration: 59470 loss: 0.0103 lr: 0.02\n",
            "iteration: 59480 loss: 0.0135 lr: 0.02\n",
            "iteration: 59490 loss: 0.0198 lr: 0.02\n",
            "iteration: 59500 loss: 0.0095 lr: 0.02\n",
            "iteration: 59510 loss: 0.0124 lr: 0.02\n",
            "iteration: 59520 loss: 0.0156 lr: 0.02\n",
            "iteration: 59530 loss: 0.0107 lr: 0.02\n",
            "iteration: 59540 loss: 0.0130 lr: 0.02\n",
            "iteration: 59550 loss: 0.0084 lr: 0.02\n",
            "iteration: 59560 loss: 0.0186 lr: 0.02\n",
            "iteration: 59570 loss: 0.0159 lr: 0.02\n",
            "iteration: 59580 loss: 0.0150 lr: 0.02\n",
            "iteration: 59590 loss: 0.0160 lr: 0.02\n",
            "iteration: 59600 loss: 0.0121 lr: 0.02\n",
            "iteration: 59610 loss: 0.0122 lr: 0.02\n",
            "iteration: 59620 loss: 0.0173 lr: 0.02\n",
            "iteration: 59630 loss: 0.0114 lr: 0.02\n",
            "iteration: 59640 loss: 0.0150 lr: 0.02\n",
            "iteration: 59650 loss: 0.0192 lr: 0.02\n",
            "iteration: 59660 loss: 0.0126 lr: 0.02\n",
            "iteration: 59670 loss: 0.0153 lr: 0.02\n",
            "iteration: 59680 loss: 0.0156 lr: 0.02\n",
            "iteration: 59690 loss: 0.0153 lr: 0.02\n",
            "iteration: 59700 loss: 0.0163 lr: 0.02\n",
            "iteration: 59710 loss: 0.0166 lr: 0.02\n",
            "iteration: 59720 loss: 0.0127 lr: 0.02\n",
            "iteration: 59730 loss: 0.0121 lr: 0.02\n",
            "iteration: 59740 loss: 0.0139 lr: 0.02\n",
            "iteration: 59750 loss: 0.0151 lr: 0.02\n",
            "iteration: 59760 loss: 0.0133 lr: 0.02\n",
            "iteration: 59770 loss: 0.0115 lr: 0.02\n",
            "iteration: 59780 loss: 0.0146 lr: 0.02\n",
            "iteration: 59790 loss: 0.0149 lr: 0.02\n",
            "iteration: 59800 loss: 0.0119 lr: 0.02\n",
            "iteration: 59810 loss: 0.0122 lr: 0.02\n",
            "iteration: 59820 loss: 0.0120 lr: 0.02\n",
            "iteration: 59830 loss: 0.0155 lr: 0.02\n",
            "iteration: 59840 loss: 0.0114 lr: 0.02\n",
            "iteration: 59850 loss: 0.0135 lr: 0.02\n",
            "iteration: 59860 loss: 0.0180 lr: 0.02\n",
            "iteration: 59870 loss: 0.0151 lr: 0.02\n",
            "iteration: 59880 loss: 0.0108 lr: 0.02\n",
            "iteration: 59890 loss: 0.0132 lr: 0.02\n",
            "iteration: 59900 loss: 0.0095 lr: 0.02\n",
            "iteration: 59910 loss: 0.0113 lr: 0.02\n",
            "iteration: 59920 loss: 0.0132 lr: 0.02\n",
            "iteration: 59930 loss: 0.0111 lr: 0.02\n",
            "iteration: 59940 loss: 0.0151 lr: 0.02\n",
            "iteration: 59950 loss: 0.0116 lr: 0.02\n",
            "iteration: 59960 loss: 0.0111 lr: 0.02\n",
            "iteration: 59970 loss: 0.0138 lr: 0.02\n",
            "iteration: 59980 loss: 0.0150 lr: 0.02\n",
            "iteration: 59990 loss: 0.0163 lr: 0.02\n",
            "iteration: 60000 loss: 0.0129 lr: 0.02\n",
            "iteration: 60010 loss: 0.0111 lr: 0.02\n",
            "iteration: 60020 loss: 0.0144 lr: 0.02\n",
            "iteration: 60030 loss: 0.0133 lr: 0.02\n",
            "iteration: 60040 loss: 0.0146 lr: 0.02\n",
            "iteration: 60050 loss: 0.0139 lr: 0.02\n",
            "iteration: 60060 loss: 0.0109 lr: 0.02\n",
            "iteration: 60070 loss: 0.0186 lr: 0.02\n",
            "iteration: 60080 loss: 0.0143 lr: 0.02\n",
            "iteration: 60090 loss: 0.0115 lr: 0.02\n",
            "iteration: 60100 loss: 0.0102 lr: 0.02\n",
            "iteration: 60110 loss: 0.0187 lr: 0.02\n",
            "iteration: 60120 loss: 0.0106 lr: 0.02\n",
            "iteration: 60130 loss: 0.0227 lr: 0.02\n",
            "iteration: 60140 loss: 0.0173 lr: 0.02\n",
            "iteration: 60150 loss: 0.0131 lr: 0.02\n",
            "iteration: 60160 loss: 0.0188 lr: 0.02\n",
            "iteration: 60170 loss: 0.0109 lr: 0.02\n",
            "iteration: 60180 loss: 0.0116 lr: 0.02\n",
            "iteration: 60190 loss: 0.0111 lr: 0.02\n",
            "iteration: 60200 loss: 0.0119 lr: 0.02\n",
            "iteration: 60210 loss: 0.0135 lr: 0.02\n",
            "iteration: 60220 loss: 0.0160 lr: 0.02\n",
            "iteration: 60230 loss: 0.0151 lr: 0.02\n",
            "iteration: 60240 loss: 0.0098 lr: 0.02\n",
            "iteration: 60250 loss: 0.0101 lr: 0.02\n",
            "iteration: 60260 loss: 0.0100 lr: 0.02\n",
            "iteration: 60270 loss: 0.0123 lr: 0.02\n",
            "iteration: 60280 loss: 0.0143 lr: 0.02\n",
            "iteration: 60290 loss: 0.0160 lr: 0.02\n",
            "iteration: 60300 loss: 0.0192 lr: 0.02\n",
            "iteration: 60310 loss: 0.0125 lr: 0.02\n",
            "iteration: 60320 loss: 0.0110 lr: 0.02\n",
            "iteration: 60330 loss: 0.0165 lr: 0.02\n",
            "iteration: 60340 loss: 0.0107 lr: 0.02\n",
            "iteration: 60350 loss: 0.0130 lr: 0.02\n",
            "iteration: 60360 loss: 0.0123 lr: 0.02\n",
            "iteration: 60370 loss: 0.0150 lr: 0.02\n",
            "iteration: 60380 loss: 0.0123 lr: 0.02\n",
            "iteration: 60390 loss: 0.0145 lr: 0.02\n",
            "iteration: 60400 loss: 0.0198 lr: 0.02\n",
            "iteration: 60410 loss: 0.0105 lr: 0.02\n",
            "iteration: 60420 loss: 0.0150 lr: 0.02\n",
            "iteration: 60430 loss: 0.0086 lr: 0.02\n",
            "iteration: 60440 loss: 0.0208 lr: 0.02\n",
            "iteration: 60450 loss: 0.0165 lr: 0.02\n",
            "iteration: 60460 loss: 0.0130 lr: 0.02\n",
            "iteration: 60470 loss: 0.0190 lr: 0.02\n",
            "iteration: 60480 loss: 0.0111 lr: 0.02\n",
            "iteration: 60490 loss: 0.0103 lr: 0.02\n",
            "iteration: 60500 loss: 0.0159 lr: 0.02\n",
            "iteration: 60510 loss: 0.0123 lr: 0.02\n",
            "iteration: 60520 loss: 0.0135 lr: 0.02\n",
            "iteration: 60530 loss: 0.0220 lr: 0.02\n",
            "iteration: 60540 loss: 0.0103 lr: 0.02\n",
            "iteration: 60550 loss: 0.0113 lr: 0.02\n",
            "iteration: 60560 loss: 0.0133 lr: 0.02\n",
            "iteration: 60570 loss: 0.0177 lr: 0.02\n",
            "iteration: 60580 loss: 0.0116 lr: 0.02\n",
            "iteration: 60590 loss: 0.0132 lr: 0.02\n",
            "iteration: 60600 loss: 0.0168 lr: 0.02\n",
            "iteration: 60610 loss: 0.0119 lr: 0.02\n",
            "iteration: 60620 loss: 0.0159 lr: 0.02\n",
            "iteration: 60630 loss: 0.0136 lr: 0.02\n",
            "iteration: 60640 loss: 0.0119 lr: 0.02\n",
            "iteration: 60650 loss: 0.0088 lr: 0.02\n",
            "iteration: 60660 loss: 0.0112 lr: 0.02\n",
            "iteration: 60670 loss: 0.0238 lr: 0.02\n",
            "iteration: 60680 loss: 0.0129 lr: 0.02\n",
            "iteration: 60690 loss: 0.0107 lr: 0.02\n",
            "iteration: 60700 loss: 0.0101 lr: 0.02\n",
            "iteration: 60710 loss: 0.0140 lr: 0.02\n",
            "iteration: 60720 loss: 0.0123 lr: 0.02\n",
            "iteration: 60730 loss: 0.0177 lr: 0.02\n",
            "iteration: 60740 loss: 0.0104 lr: 0.02\n",
            "iteration: 60750 loss: 0.0139 lr: 0.02\n",
            "iteration: 60760 loss: 0.0132 lr: 0.02\n",
            "iteration: 60770 loss: 0.0172 lr: 0.02\n",
            "iteration: 60780 loss: 0.0110 lr: 0.02\n",
            "iteration: 60790 loss: 0.0134 lr: 0.02\n",
            "iteration: 60800 loss: 0.0141 lr: 0.02\n",
            "iteration: 60810 loss: 0.0133 lr: 0.02\n",
            "iteration: 60820 loss: 0.0111 lr: 0.02\n",
            "iteration: 60830 loss: 0.0137 lr: 0.02\n",
            "iteration: 60840 loss: 0.0138 lr: 0.02\n",
            "iteration: 60850 loss: 0.0164 lr: 0.02\n",
            "iteration: 60860 loss: 0.0140 lr: 0.02\n",
            "iteration: 60870 loss: 0.0139 lr: 0.02\n",
            "iteration: 60880 loss: 0.0151 lr: 0.02\n",
            "iteration: 60890 loss: 0.0084 lr: 0.02\n",
            "iteration: 60900 loss: 0.0154 lr: 0.02\n",
            "iteration: 60910 loss: 0.0094 lr: 0.02\n",
            "iteration: 60920 loss: 0.0173 lr: 0.02\n",
            "iteration: 60930 loss: 0.0158 lr: 0.02\n",
            "iteration: 60940 loss: 0.0211 lr: 0.02\n",
            "iteration: 60950 loss: 0.0234 lr: 0.02\n",
            "iteration: 60960 loss: 0.0132 lr: 0.02\n",
            "iteration: 60970 loss: 0.0163 lr: 0.02\n",
            "iteration: 60980 loss: 0.0146 lr: 0.02\n",
            "iteration: 60990 loss: 0.0154 lr: 0.02\n",
            "iteration: 61000 loss: 0.0120 lr: 0.02\n",
            "iteration: 61010 loss: 0.0172 lr: 0.02\n",
            "iteration: 61020 loss: 0.0096 lr: 0.02\n",
            "iteration: 61030 loss: 0.0139 lr: 0.02\n",
            "iteration: 61040 loss: 0.0114 lr: 0.02\n",
            "iteration: 61050 loss: 0.0171 lr: 0.02\n",
            "iteration: 61060 loss: 0.0157 lr: 0.02\n",
            "iteration: 61070 loss: 0.0076 lr: 0.02\n",
            "iteration: 61080 loss: 0.0182 lr: 0.02\n",
            "iteration: 61090 loss: 0.0109 lr: 0.02\n",
            "iteration: 61100 loss: 0.0133 lr: 0.02\n",
            "iteration: 61110 loss: 0.0195 lr: 0.02\n",
            "iteration: 61120 loss: 0.0114 lr: 0.02\n",
            "iteration: 61130 loss: 0.0128 lr: 0.02\n",
            "iteration: 61140 loss: 0.0132 lr: 0.02\n",
            "iteration: 61150 loss: 0.0101 lr: 0.02\n",
            "iteration: 61160 loss: 0.0138 lr: 0.02\n",
            "iteration: 61170 loss: 0.0129 lr: 0.02\n",
            "iteration: 61180 loss: 0.0143 lr: 0.02\n",
            "iteration: 61190 loss: 0.0148 lr: 0.02\n",
            "iteration: 61200 loss: 0.0126 lr: 0.02\n",
            "iteration: 61210 loss: 0.0112 lr: 0.02\n",
            "iteration: 61220 loss: 0.0108 lr: 0.02\n",
            "iteration: 61230 loss: 0.0148 lr: 0.02\n",
            "iteration: 61240 loss: 0.0133 lr: 0.02\n",
            "iteration: 61250 loss: 0.0100 lr: 0.02\n",
            "iteration: 61260 loss: 0.0144 lr: 0.02\n",
            "iteration: 61270 loss: 0.0181 lr: 0.02\n",
            "iteration: 61280 loss: 0.0131 lr: 0.02\n",
            "iteration: 61290 loss: 0.0131 lr: 0.02\n",
            "iteration: 61300 loss: 0.0154 lr: 0.02\n",
            "iteration: 61310 loss: 0.0131 lr: 0.02\n",
            "iteration: 61320 loss: 0.0115 lr: 0.02\n",
            "iteration: 61330 loss: 0.0149 lr: 0.02\n",
            "iteration: 61340 loss: 0.0131 lr: 0.02\n",
            "iteration: 61350 loss: 0.0147 lr: 0.02\n",
            "iteration: 61360 loss: 0.0148 lr: 0.02\n",
            "iteration: 61370 loss: 0.0104 lr: 0.02\n",
            "iteration: 61380 loss: 0.0106 lr: 0.02\n",
            "iteration: 61390 loss: 0.0172 lr: 0.02\n",
            "iteration: 61400 loss: 0.0127 lr: 0.02\n",
            "iteration: 61410 loss: 0.0087 lr: 0.02\n",
            "iteration: 61420 loss: 0.0152 lr: 0.02\n",
            "iteration: 61430 loss: 0.0128 lr: 0.02\n",
            "iteration: 61440 loss: 0.0178 lr: 0.02\n",
            "iteration: 61450 loss: 0.0099 lr: 0.02\n",
            "iteration: 61460 loss: 0.0136 lr: 0.02\n",
            "iteration: 61470 loss: 0.0084 lr: 0.02\n",
            "iteration: 61480 loss: 0.0158 lr: 0.02\n",
            "iteration: 61490 loss: 0.0156 lr: 0.02\n",
            "iteration: 61500 loss: 0.0159 lr: 0.02\n",
            "iteration: 61510 loss: 0.0185 lr: 0.02\n",
            "iteration: 61520 loss: 0.0097 lr: 0.02\n",
            "iteration: 61530 loss: 0.0118 lr: 0.02\n",
            "iteration: 61540 loss: 0.0091 lr: 0.02\n",
            "iteration: 61550 loss: 0.0096 lr: 0.02\n",
            "iteration: 61560 loss: 0.0194 lr: 0.02\n",
            "iteration: 61570 loss: 0.0138 lr: 0.02\n",
            "iteration: 61580 loss: 0.0097 lr: 0.02\n",
            "iteration: 61590 loss: 0.0135 lr: 0.02\n",
            "iteration: 61600 loss: 0.0123 lr: 0.02\n",
            "iteration: 61610 loss: 0.0122 lr: 0.02\n",
            "iteration: 61620 loss: 0.0130 lr: 0.02\n",
            "iteration: 61630 loss: 0.0106 lr: 0.02\n",
            "iteration: 61640 loss: 0.0125 lr: 0.02\n",
            "iteration: 61650 loss: 0.0112 lr: 0.02\n",
            "iteration: 61660 loss: 0.0114 lr: 0.02\n",
            "iteration: 61670 loss: 0.0109 lr: 0.02\n",
            "iteration: 61680 loss: 0.0134 lr: 0.02\n",
            "iteration: 61690 loss: 0.0135 lr: 0.02\n",
            "iteration: 61700 loss: 0.0122 lr: 0.02\n",
            "iteration: 61710 loss: 0.0108 lr: 0.02\n",
            "iteration: 61720 loss: 0.0131 lr: 0.02\n",
            "iteration: 61730 loss: 0.0117 lr: 0.02\n",
            "iteration: 61740 loss: 0.0139 lr: 0.02\n",
            "iteration: 61750 loss: 0.0111 lr: 0.02\n",
            "iteration: 61760 loss: 0.0109 lr: 0.02\n",
            "iteration: 61770 loss: 0.0088 lr: 0.02\n",
            "iteration: 61780 loss: 0.0121 lr: 0.02\n",
            "iteration: 61790 loss: 0.0097 lr: 0.02\n",
            "iteration: 61800 loss: 0.0193 lr: 0.02\n",
            "iteration: 61810 loss: 0.0125 lr: 0.02\n",
            "iteration: 61820 loss: 0.0143 lr: 0.02\n",
            "iteration: 61830 loss: 0.0191 lr: 0.02\n",
            "iteration: 61840 loss: 0.0184 lr: 0.02\n",
            "iteration: 61850 loss: 0.0153 lr: 0.02\n",
            "iteration: 61860 loss: 0.0140 lr: 0.02\n",
            "iteration: 61870 loss: 0.0169 lr: 0.02\n",
            "iteration: 61880 loss: 0.0126 lr: 0.02\n",
            "iteration: 61890 loss: 0.0163 lr: 0.02\n",
            "iteration: 61900 loss: 0.0095 lr: 0.02\n",
            "iteration: 61910 loss: 0.0173 lr: 0.02\n",
            "iteration: 61920 loss: 0.0105 lr: 0.02\n",
            "iteration: 61930 loss: 0.0111 lr: 0.02\n",
            "iteration: 61940 loss: 0.0130 lr: 0.02\n",
            "iteration: 61950 loss: 0.0090 lr: 0.02\n",
            "iteration: 61960 loss: 0.0156 lr: 0.02\n",
            "iteration: 61970 loss: 0.0094 lr: 0.02\n",
            "iteration: 61980 loss: 0.0141 lr: 0.02\n",
            "iteration: 61990 loss: 0.0111 lr: 0.02\n",
            "iteration: 62000 loss: 0.0107 lr: 0.02\n",
            "iteration: 62010 loss: 0.0131 lr: 0.02\n",
            "iteration: 62020 loss: 0.0157 lr: 0.02\n",
            "iteration: 62030 loss: 0.0124 lr: 0.02\n",
            "iteration: 62040 loss: 0.0141 lr: 0.02\n",
            "iteration: 62050 loss: 0.0147 lr: 0.02\n",
            "iteration: 62060 loss: 0.0113 lr: 0.02\n",
            "iteration: 62070 loss: 0.0176 lr: 0.02\n",
            "iteration: 62080 loss: 0.0128 lr: 0.02\n",
            "iteration: 62090 loss: 0.0107 lr: 0.02\n",
            "iteration: 62100 loss: 0.0114 lr: 0.02\n",
            "iteration: 62110 loss: 0.0126 lr: 0.02\n",
            "iteration: 62120 loss: 0.0133 lr: 0.02\n",
            "iteration: 62130 loss: 0.0112 lr: 0.02\n",
            "iteration: 62140 loss: 0.0097 lr: 0.02\n",
            "iteration: 62150 loss: 0.0137 lr: 0.02\n",
            "iteration: 62160 loss: 0.0089 lr: 0.02\n",
            "iteration: 62170 loss: 0.0145 lr: 0.02\n",
            "iteration: 62180 loss: 0.0126 lr: 0.02\n",
            "iteration: 62190 loss: 0.0170 lr: 0.02\n",
            "iteration: 62200 loss: 0.0142 lr: 0.02\n",
            "iteration: 62210 loss: 0.0130 lr: 0.02\n",
            "iteration: 62220 loss: 0.0184 lr: 0.02\n",
            "iteration: 62230 loss: 0.0170 lr: 0.02\n",
            "iteration: 62240 loss: 0.0141 lr: 0.02\n",
            "iteration: 62250 loss: 0.0165 lr: 0.02\n",
            "iteration: 62260 loss: 0.0150 lr: 0.02\n",
            "iteration: 62270 loss: 0.0114 lr: 0.02\n",
            "iteration: 62280 loss: 0.0115 lr: 0.02\n",
            "iteration: 62290 loss: 0.0144 lr: 0.02\n",
            "iteration: 62300 loss: 0.0131 lr: 0.02\n",
            "iteration: 62310 loss: 0.0147 lr: 0.02\n",
            "iteration: 62320 loss: 0.0256 lr: 0.02\n",
            "iteration: 62330 loss: 0.0158 lr: 0.02\n",
            "iteration: 62340 loss: 0.0116 lr: 0.02\n",
            "iteration: 62350 loss: 0.0197 lr: 0.02\n",
            "iteration: 62360 loss: 0.0129 lr: 0.02\n",
            "iteration: 62370 loss: 0.0114 lr: 0.02\n",
            "iteration: 62380 loss: 0.0174 lr: 0.02\n",
            "iteration: 62390 loss: 0.0080 lr: 0.02\n",
            "iteration: 62400 loss: 0.0110 lr: 0.02\n",
            "iteration: 62410 loss: 0.0143 lr: 0.02\n",
            "iteration: 62420 loss: 0.0124 lr: 0.02\n",
            "iteration: 62430 loss: 0.0117 lr: 0.02\n",
            "iteration: 62440 loss: 0.0133 lr: 0.02\n",
            "iteration: 62450 loss: 0.0103 lr: 0.02\n",
            "iteration: 62460 loss: 0.0135 lr: 0.02\n",
            "iteration: 62470 loss: 0.0110 lr: 0.02\n",
            "iteration: 62480 loss: 0.0132 lr: 0.02\n",
            "iteration: 62490 loss: 0.0209 lr: 0.02\n",
            "iteration: 62500 loss: 0.0146 lr: 0.02\n",
            "iteration: 62510 loss: 0.0119 lr: 0.02\n",
            "iteration: 62520 loss: 0.0094 lr: 0.02\n",
            "iteration: 62530 loss: 0.0167 lr: 0.02\n",
            "iteration: 62540 loss: 0.0128 lr: 0.02\n",
            "iteration: 62550 loss: 0.0138 lr: 0.02\n",
            "iteration: 62560 loss: 0.0207 lr: 0.02\n",
            "iteration: 62570 loss: 0.0144 lr: 0.02\n",
            "iteration: 62580 loss: 0.0135 lr: 0.02\n",
            "iteration: 62590 loss: 0.0133 lr: 0.02\n",
            "iteration: 62600 loss: 0.0124 lr: 0.02\n",
            "iteration: 62610 loss: 0.0119 lr: 0.02\n",
            "iteration: 62620 loss: 0.0129 lr: 0.02\n",
            "iteration: 62630 loss: 0.0125 lr: 0.02\n",
            "iteration: 62640 loss: 0.0125 lr: 0.02\n",
            "iteration: 62650 loss: 0.0105 lr: 0.02\n",
            "iteration: 62660 loss: 0.0200 lr: 0.02\n",
            "iteration: 62670 loss: 0.0087 lr: 0.02\n",
            "iteration: 62680 loss: 0.0106 lr: 0.02\n",
            "iteration: 62690 loss: 0.0095 lr: 0.02\n",
            "iteration: 62700 loss: 0.0103 lr: 0.02\n",
            "iteration: 62710 loss: 0.0116 lr: 0.02\n",
            "iteration: 62720 loss: 0.0116 lr: 0.02\n",
            "iteration: 62730 loss: 0.0131 lr: 0.02\n",
            "iteration: 62740 loss: 0.0127 lr: 0.02\n",
            "iteration: 62750 loss: 0.0115 lr: 0.02\n",
            "iteration: 62760 loss: 0.0119 lr: 0.02\n",
            "iteration: 62770 loss: 0.0136 lr: 0.02\n",
            "iteration: 62780 loss: 0.0160 lr: 0.02\n",
            "iteration: 62790 loss: 0.0103 lr: 0.02\n",
            "iteration: 62800 loss: 0.0117 lr: 0.02\n",
            "iteration: 62810 loss: 0.0144 lr: 0.02\n",
            "iteration: 62820 loss: 0.0092 lr: 0.02\n",
            "iteration: 62830 loss: 0.0149 lr: 0.02\n",
            "iteration: 62840 loss: 0.0187 lr: 0.02\n",
            "iteration: 62850 loss: 0.0118 lr: 0.02\n",
            "iteration: 62860 loss: 0.0139 lr: 0.02\n",
            "iteration: 62870 loss: 0.0122 lr: 0.02\n",
            "iteration: 62880 loss: 0.0110 lr: 0.02\n",
            "iteration: 62890 loss: 0.0093 lr: 0.02\n",
            "iteration: 62900 loss: 0.0172 lr: 0.02\n",
            "iteration: 62910 loss: 0.0146 lr: 0.02\n",
            "iteration: 62920 loss: 0.0116 lr: 0.02\n",
            "iteration: 62930 loss: 0.0145 lr: 0.02\n",
            "iteration: 62940 loss: 0.0130 lr: 0.02\n",
            "iteration: 62950 loss: 0.0108 lr: 0.02\n",
            "iteration: 62960 loss: 0.0137 lr: 0.02\n",
            "iteration: 62970 loss: 0.0134 lr: 0.02\n",
            "iteration: 62980 loss: 0.0141 lr: 0.02\n",
            "iteration: 62990 loss: 0.0135 lr: 0.02\n",
            "iteration: 63000 loss: 0.0169 lr: 0.02\n",
            "iteration: 63010 loss: 0.0091 lr: 0.02\n",
            "iteration: 63020 loss: 0.0127 lr: 0.02\n",
            "iteration: 63030 loss: 0.0109 lr: 0.02\n",
            "iteration: 63040 loss: 0.0126 lr: 0.02\n",
            "iteration: 63050 loss: 0.0140 lr: 0.02\n",
            "iteration: 63060 loss: 0.0131 lr: 0.02\n",
            "iteration: 63070 loss: 0.0136 lr: 0.02\n",
            "iteration: 63080 loss: 0.0097 lr: 0.02\n",
            "iteration: 63090 loss: 0.0097 lr: 0.02\n",
            "iteration: 63100 loss: 0.0143 lr: 0.02\n",
            "iteration: 63110 loss: 0.0141 lr: 0.02\n",
            "iteration: 63120 loss: 0.0143 lr: 0.02\n",
            "iteration: 63130 loss: 0.0107 lr: 0.02\n",
            "iteration: 63140 loss: 0.0103 lr: 0.02\n",
            "iteration: 63150 loss: 0.0122 lr: 0.02\n",
            "iteration: 63160 loss: 0.0088 lr: 0.02\n",
            "iteration: 63170 loss: 0.0147 lr: 0.02\n",
            "iteration: 63180 loss: 0.0124 lr: 0.02\n",
            "iteration: 63190 loss: 0.0105 lr: 0.02\n",
            "iteration: 63200 loss: 0.0160 lr: 0.02\n",
            "iteration: 63210 loss: 0.0127 lr: 0.02\n",
            "iteration: 63220 loss: 0.0129 lr: 0.02\n",
            "iteration: 63230 loss: 0.0112 lr: 0.02\n",
            "iteration: 63240 loss: 0.0115 lr: 0.02\n",
            "iteration: 63250 loss: 0.0131 lr: 0.02\n",
            "iteration: 63260 loss: 0.0118 lr: 0.02\n",
            "iteration: 63270 loss: 0.0113 lr: 0.02\n",
            "iteration: 63280 loss: 0.0111 lr: 0.02\n",
            "iteration: 63290 loss: 0.0158 lr: 0.02\n",
            "iteration: 63300 loss: 0.0144 lr: 0.02\n",
            "iteration: 63310 loss: 0.0151 lr: 0.02\n",
            "iteration: 63320 loss: 0.0088 lr: 0.02\n",
            "iteration: 63330 loss: 0.0157 lr: 0.02\n",
            "iteration: 63340 loss: 0.0113 lr: 0.02\n",
            "iteration: 63350 loss: 0.0103 lr: 0.02\n",
            "iteration: 63360 loss: 0.0099 lr: 0.02\n",
            "iteration: 63370 loss: 0.0113 lr: 0.02\n",
            "iteration: 63380 loss: 0.0128 lr: 0.02\n",
            "iteration: 63390 loss: 0.0091 lr: 0.02\n",
            "iteration: 63400 loss: 0.0097 lr: 0.02\n",
            "iteration: 63410 loss: 0.0126 lr: 0.02\n",
            "iteration: 63420 loss: 0.0129 lr: 0.02\n",
            "iteration: 63430 loss: 0.0154 lr: 0.02\n",
            "iteration: 63440 loss: 0.0098 lr: 0.02\n",
            "iteration: 63450 loss: 0.0183 lr: 0.02\n",
            "iteration: 63460 loss: 0.0130 lr: 0.02\n",
            "iteration: 63470 loss: 0.0154 lr: 0.02\n",
            "iteration: 63480 loss: 0.0161 lr: 0.02\n",
            "iteration: 63490 loss: 0.0109 lr: 0.02\n",
            "iteration: 63500 loss: 0.0170 lr: 0.02\n",
            "iteration: 63510 loss: 0.0105 lr: 0.02\n",
            "iteration: 63520 loss: 0.0095 lr: 0.02\n",
            "iteration: 63530 loss: 0.0153 lr: 0.02\n",
            "iteration: 63540 loss: 0.0137 lr: 0.02\n",
            "iteration: 63550 loss: 0.0128 lr: 0.02\n",
            "iteration: 63560 loss: 0.0161 lr: 0.02\n",
            "iteration: 63570 loss: 0.0128 lr: 0.02\n",
            "iteration: 63580 loss: 0.0111 lr: 0.02\n",
            "iteration: 63590 loss: 0.0105 lr: 0.02\n",
            "iteration: 63600 loss: 0.0205 lr: 0.02\n",
            "iteration: 63610 loss: 0.0104 lr: 0.02\n",
            "iteration: 63620 loss: 0.0138 lr: 0.02\n",
            "iteration: 63630 loss: 0.0102 lr: 0.02\n",
            "iteration: 63640 loss: 0.0138 lr: 0.02\n",
            "iteration: 63650 loss: 0.0148 lr: 0.02\n",
            "iteration: 63660 loss: 0.0150 lr: 0.02\n",
            "iteration: 63670 loss: 0.0085 lr: 0.02\n",
            "iteration: 63680 loss: 0.0094 lr: 0.02\n",
            "iteration: 63690 loss: 0.0098 lr: 0.02\n",
            "iteration: 63700 loss: 0.0167 lr: 0.02\n",
            "iteration: 63710 loss: 0.0086 lr: 0.02\n",
            "iteration: 63720 loss: 0.0123 lr: 0.02\n",
            "iteration: 63730 loss: 0.0129 lr: 0.02\n",
            "iteration: 63740 loss: 0.0143 lr: 0.02\n",
            "iteration: 63750 loss: 0.0102 lr: 0.02\n",
            "iteration: 63760 loss: 0.0135 lr: 0.02\n",
            "iteration: 63770 loss: 0.0098 lr: 0.02\n",
            "iteration: 63780 loss: 0.0104 lr: 0.02\n",
            "iteration: 63790 loss: 0.0097 lr: 0.02\n",
            "iteration: 63800 loss: 0.0150 lr: 0.02\n",
            "iteration: 63810 loss: 0.0108 lr: 0.02\n",
            "iteration: 63820 loss: 0.0129 lr: 0.02\n",
            "iteration: 63830 loss: 0.0111 lr: 0.02\n",
            "iteration: 63840 loss: 0.0097 lr: 0.02\n",
            "iteration: 63850 loss: 0.0122 lr: 0.02\n",
            "iteration: 63860 loss: 0.0105 lr: 0.02\n",
            "iteration: 63870 loss: 0.0140 lr: 0.02\n",
            "iteration: 63880 loss: 0.0153 lr: 0.02\n",
            "iteration: 63890 loss: 0.0141 lr: 0.02\n",
            "iteration: 63900 loss: 0.0102 lr: 0.02\n",
            "iteration: 63910 loss: 0.0122 lr: 0.02\n",
            "iteration: 63920 loss: 0.0120 lr: 0.02\n",
            "iteration: 63930 loss: 0.0112 lr: 0.02\n",
            "iteration: 63940 loss: 0.0111 lr: 0.02\n",
            "iteration: 63950 loss: 0.0127 lr: 0.02\n",
            "iteration: 63960 loss: 0.0082 lr: 0.02\n",
            "iteration: 63970 loss: 0.0129 lr: 0.02\n",
            "iteration: 63980 loss: 0.0134 lr: 0.02\n",
            "iteration: 63990 loss: 0.0094 lr: 0.02\n",
            "iteration: 64000 loss: 0.0208 lr: 0.02\n",
            "iteration: 64010 loss: 0.0106 lr: 0.02\n",
            "iteration: 64020 loss: 0.0098 lr: 0.02\n",
            "iteration: 64030 loss: 0.0130 lr: 0.02\n",
            "iteration: 64040 loss: 0.0124 lr: 0.02\n",
            "iteration: 64050 loss: 0.0093 lr: 0.02\n",
            "iteration: 64060 loss: 0.0146 lr: 0.02\n",
            "iteration: 64070 loss: 0.0108 lr: 0.02\n",
            "iteration: 64080 loss: 0.0099 lr: 0.02\n",
            "iteration: 64090 loss: 0.0104 lr: 0.02\n",
            "iteration: 64100 loss: 0.0112 lr: 0.02\n",
            "iteration: 64110 loss: 0.0138 lr: 0.02\n",
            "iteration: 64120 loss: 0.0155 lr: 0.02\n",
            "iteration: 64130 loss: 0.0113 lr: 0.02\n",
            "iteration: 64140 loss: 0.0107 lr: 0.02\n",
            "iteration: 64150 loss: 0.0100 lr: 0.02\n",
            "iteration: 64160 loss: 0.0117 lr: 0.02\n",
            "iteration: 64170 loss: 0.0105 lr: 0.02\n",
            "iteration: 64180 loss: 0.0100 lr: 0.02\n",
            "iteration: 64190 loss: 0.0089 lr: 0.02\n",
            "iteration: 64200 loss: 0.0133 lr: 0.02\n",
            "iteration: 64210 loss: 0.0123 lr: 0.02\n",
            "iteration: 64220 loss: 0.0081 lr: 0.02\n",
            "iteration: 64230 loss: 0.0124 lr: 0.02\n",
            "iteration: 64240 loss: 0.0106 lr: 0.02\n",
            "iteration: 64250 loss: 0.0108 lr: 0.02\n",
            "iteration: 64260 loss: 0.0152 lr: 0.02\n",
            "iteration: 64270 loss: 0.0095 lr: 0.02\n",
            "iteration: 64280 loss: 0.0151 lr: 0.02\n",
            "iteration: 64290 loss: 0.0146 lr: 0.02\n",
            "iteration: 64300 loss: 0.0119 lr: 0.02\n",
            "iteration: 64310 loss: 0.0127 lr: 0.02\n",
            "iteration: 64320 loss: 0.0178 lr: 0.02\n",
            "iteration: 64330 loss: 0.0180 lr: 0.02\n",
            "iteration: 64340 loss: 0.0245 lr: 0.02\n",
            "iteration: 64350 loss: 0.0140 lr: 0.02\n",
            "iteration: 64360 loss: 0.0218 lr: 0.02\n",
            "iteration: 64370 loss: 0.0119 lr: 0.02\n",
            "iteration: 64380 loss: 0.0124 lr: 0.02\n",
            "iteration: 64390 loss: 0.0158 lr: 0.02\n",
            "iteration: 64400 loss: 0.0096 lr: 0.02\n",
            "iteration: 64410 loss: 0.0125 lr: 0.02\n",
            "iteration: 64420 loss: 0.0127 lr: 0.02\n",
            "iteration: 64430 loss: 0.0109 lr: 0.02\n",
            "iteration: 64440 loss: 0.0159 lr: 0.02\n",
            "iteration: 64450 loss: 0.0124 lr: 0.02\n",
            "iteration: 64460 loss: 0.0154 lr: 0.02\n",
            "iteration: 64470 loss: 0.0126 lr: 0.02\n",
            "iteration: 64480 loss: 0.0149 lr: 0.02\n",
            "iteration: 64490 loss: 0.0121 lr: 0.02\n",
            "iteration: 64500 loss: 0.0105 lr: 0.02\n",
            "iteration: 64510 loss: 0.0120 lr: 0.02\n",
            "iteration: 64520 loss: 0.0108 lr: 0.02\n",
            "iteration: 64530 loss: 0.0106 lr: 0.02\n",
            "iteration: 64540 loss: 0.0150 lr: 0.02\n",
            "iteration: 64550 loss: 0.0121 lr: 0.02\n",
            "iteration: 64560 loss: 0.0130 lr: 0.02\n",
            "iteration: 64570 loss: 0.0126 lr: 0.02\n",
            "iteration: 64580 loss: 0.0174 lr: 0.02\n",
            "iteration: 64590 loss: 0.0161 lr: 0.02\n",
            "iteration: 64600 loss: 0.0105 lr: 0.02\n",
            "iteration: 64610 loss: 0.0096 lr: 0.02\n",
            "iteration: 64620 loss: 0.0091 lr: 0.02\n",
            "iteration: 64630 loss: 0.0152 lr: 0.02\n",
            "iteration: 64640 loss: 0.0134 lr: 0.02\n",
            "iteration: 64650 loss: 0.0121 lr: 0.02\n",
            "iteration: 64660 loss: 0.0116 lr: 0.02\n",
            "iteration: 64670 loss: 0.0121 lr: 0.02\n",
            "iteration: 64680 loss: 0.0100 lr: 0.02\n",
            "iteration: 64690 loss: 0.0156 lr: 0.02\n",
            "iteration: 64700 loss: 0.0143 lr: 0.02\n",
            "iteration: 64710 loss: 0.0121 lr: 0.02\n",
            "iteration: 64720 loss: 0.0119 lr: 0.02\n",
            "iteration: 64730 loss: 0.0199 lr: 0.02\n",
            "iteration: 64740 loss: 0.0132 lr: 0.02\n",
            "iteration: 64750 loss: 0.0108 lr: 0.02\n",
            "iteration: 64760 loss: 0.0141 lr: 0.02\n",
            "iteration: 64770 loss: 0.0148 lr: 0.02\n",
            "iteration: 64780 loss: 0.0134 lr: 0.02\n",
            "iteration: 64790 loss: 0.0106 lr: 0.02\n",
            "iteration: 64800 loss: 0.0166 lr: 0.02\n",
            "iteration: 64810 loss: 0.0146 lr: 0.02\n",
            "iteration: 64820 loss: 0.0163 lr: 0.02\n",
            "iteration: 64830 loss: 0.0108 lr: 0.02\n",
            "iteration: 64840 loss: 0.0113 lr: 0.02\n",
            "iteration: 64850 loss: 0.0141 lr: 0.02\n",
            "iteration: 64860 loss: 0.0136 lr: 0.02\n",
            "iteration: 64870 loss: 0.0141 lr: 0.02\n",
            "iteration: 64880 loss: 0.0088 lr: 0.02\n",
            "iteration: 64890 loss: 0.0106 lr: 0.02\n",
            "iteration: 64900 loss: 0.0137 lr: 0.02\n",
            "iteration: 64910 loss: 0.0115 lr: 0.02\n",
            "iteration: 64920 loss: 0.0103 lr: 0.02\n",
            "iteration: 64930 loss: 0.0141 lr: 0.02\n",
            "iteration: 64940 loss: 0.0160 lr: 0.02\n",
            "iteration: 64950 loss: 0.0118 lr: 0.02\n",
            "iteration: 64960 loss: 0.0138 lr: 0.02\n",
            "iteration: 64970 loss: 0.0101 lr: 0.02\n",
            "iteration: 64980 loss: 0.0099 lr: 0.02\n",
            "iteration: 64990 loss: 0.0154 lr: 0.02\n",
            "iteration: 65000 loss: 0.0133 lr: 0.02\n",
            "iteration: 65010 loss: 0.0103 lr: 0.02\n",
            "iteration: 65020 loss: 0.0140 lr: 0.02\n",
            "iteration: 65030 loss: 0.0125 lr: 0.02\n",
            "iteration: 65040 loss: 0.0152 lr: 0.02\n",
            "iteration: 65050 loss: 0.0174 lr: 0.02\n",
            "iteration: 65060 loss: 0.0150 lr: 0.02\n",
            "iteration: 65070 loss: 0.0097 lr: 0.02\n",
            "iteration: 65080 loss: 0.0170 lr: 0.02\n",
            "iteration: 65090 loss: 0.0107 lr: 0.02\n",
            "iteration: 65100 loss: 0.0095 lr: 0.02\n",
            "iteration: 65110 loss: 0.0100 lr: 0.02\n",
            "iteration: 65120 loss: 0.0098 lr: 0.02\n",
            "iteration: 65130 loss: 0.0106 lr: 0.02\n",
            "iteration: 65140 loss: 0.0135 lr: 0.02\n",
            "iteration: 65150 loss: 0.0187 lr: 0.02\n",
            "iteration: 65160 loss: 0.0160 lr: 0.02\n",
            "iteration: 65170 loss: 0.0136 lr: 0.02\n",
            "iteration: 65180 loss: 0.0140 lr: 0.02\n",
            "iteration: 65190 loss: 0.0153 lr: 0.02\n",
            "iteration: 65200 loss: 0.0145 lr: 0.02\n",
            "iteration: 65210 loss: 0.0137 lr: 0.02\n",
            "iteration: 65220 loss: 0.0097 lr: 0.02\n",
            "iteration: 65230 loss: 0.0100 lr: 0.02\n",
            "iteration: 65240 loss: 0.0145 lr: 0.02\n",
            "iteration: 65250 loss: 0.0126 lr: 0.02\n",
            "iteration: 65260 loss: 0.0092 lr: 0.02\n",
            "iteration: 65270 loss: 0.0127 lr: 0.02\n",
            "iteration: 65280 loss: 0.0138 lr: 0.02\n",
            "iteration: 65290 loss: 0.0101 lr: 0.02\n",
            "iteration: 65300 loss: 0.0125 lr: 0.02\n",
            "iteration: 65310 loss: 0.0183 lr: 0.02\n",
            "iteration: 65320 loss: 0.0173 lr: 0.02\n",
            "iteration: 65330 loss: 0.0189 lr: 0.02\n",
            "iteration: 65340 loss: 0.0153 lr: 0.02\n",
            "iteration: 65350 loss: 0.0126 lr: 0.02\n",
            "iteration: 65360 loss: 0.0136 lr: 0.02\n",
            "iteration: 65370 loss: 0.0088 lr: 0.02\n",
            "iteration: 65380 loss: 0.0206 lr: 0.02\n",
            "iteration: 65390 loss: 0.0119 lr: 0.02\n",
            "iteration: 65400 loss: 0.0161 lr: 0.02\n",
            "iteration: 65410 loss: 0.0152 lr: 0.02\n",
            "iteration: 65420 loss: 0.0147 lr: 0.02\n",
            "iteration: 65430 loss: 0.0095 lr: 0.02\n",
            "iteration: 65440 loss: 0.0089 lr: 0.02\n",
            "iteration: 65450 loss: 0.0124 lr: 0.02\n",
            "iteration: 65460 loss: 0.0110 lr: 0.02\n",
            "iteration: 65470 loss: 0.0101 lr: 0.02\n",
            "iteration: 65480 loss: 0.0077 lr: 0.02\n",
            "iteration: 65490 loss: 0.0091 lr: 0.02\n",
            "iteration: 65500 loss: 0.0140 lr: 0.02\n",
            "iteration: 65510 loss: 0.0123 lr: 0.02\n",
            "iteration: 65520 loss: 0.0130 lr: 0.02\n",
            "iteration: 65530 loss: 0.0074 lr: 0.02\n",
            "iteration: 65540 loss: 0.0124 lr: 0.02\n",
            "iteration: 65550 loss: 0.0103 lr: 0.02\n",
            "iteration: 65560 loss: 0.0139 lr: 0.02\n",
            "iteration: 65570 loss: 0.0102 lr: 0.02\n",
            "iteration: 65580 loss: 0.0096 lr: 0.02\n",
            "iteration: 65590 loss: 0.0144 lr: 0.02\n",
            "iteration: 65600 loss: 0.0130 lr: 0.02\n",
            "iteration: 65610 loss: 0.0115 lr: 0.02\n",
            "iteration: 65620 loss: 0.0105 lr: 0.02\n",
            "iteration: 65630 loss: 0.0145 lr: 0.02\n",
            "iteration: 65640 loss: 0.0103 lr: 0.02\n",
            "iteration: 65650 loss: 0.0115 lr: 0.02\n",
            "iteration: 65660 loss: 0.0173 lr: 0.02\n",
            "iteration: 65670 loss: 0.0172 lr: 0.02\n",
            "iteration: 65680 loss: 0.0096 lr: 0.02\n",
            "iteration: 65690 loss: 0.0135 lr: 0.02\n",
            "iteration: 65700 loss: 0.0164 lr: 0.02\n",
            "iteration: 65710 loss: 0.0120 lr: 0.02\n",
            "iteration: 65720 loss: 0.0135 lr: 0.02\n",
            "iteration: 65730 loss: 0.0133 lr: 0.02\n",
            "iteration: 65740 loss: 0.0091 lr: 0.02\n",
            "iteration: 65750 loss: 0.0102 lr: 0.02\n",
            "iteration: 65760 loss: 0.0102 lr: 0.02\n",
            "iteration: 65770 loss: 0.0126 lr: 0.02\n",
            "iteration: 65780 loss: 0.0115 lr: 0.02\n",
            "iteration: 65790 loss: 0.0155 lr: 0.02\n",
            "iteration: 65800 loss: 0.0125 lr: 0.02\n",
            "iteration: 65810 loss: 0.0123 lr: 0.02\n",
            "iteration: 65820 loss: 0.0101 lr: 0.02\n",
            "iteration: 65830 loss: 0.0115 lr: 0.02\n",
            "iteration: 65840 loss: 0.0112 lr: 0.02\n",
            "iteration: 65850 loss: 0.0111 lr: 0.02\n",
            "iteration: 65860 loss: 0.0114 lr: 0.02\n",
            "iteration: 65870 loss: 0.0126 lr: 0.02\n",
            "iteration: 65880 loss: 0.0070 lr: 0.02\n",
            "iteration: 65890 loss: 0.0123 lr: 0.02\n",
            "iteration: 65900 loss: 0.0138 lr: 0.02\n",
            "iteration: 65910 loss: 0.0165 lr: 0.02\n",
            "iteration: 65920 loss: 0.0141 lr: 0.02\n",
            "iteration: 65930 loss: 0.0165 lr: 0.02\n",
            "iteration: 65940 loss: 0.0151 lr: 0.02\n",
            "iteration: 65950 loss: 0.0136 lr: 0.02\n",
            "iteration: 65960 loss: 0.0089 lr: 0.02\n",
            "iteration: 65970 loss: 0.0140 lr: 0.02\n",
            "iteration: 65980 loss: 0.0132 lr: 0.02\n",
            "iteration: 65990 loss: 0.0093 lr: 0.02\n",
            "iteration: 66000 loss: 0.0133 lr: 0.02\n",
            "iteration: 66010 loss: 0.0155 lr: 0.02\n",
            "iteration: 66020 loss: 0.0084 lr: 0.02\n",
            "iteration: 66030 loss: 0.0169 lr: 0.02\n",
            "iteration: 66040 loss: 0.0135 lr: 0.02\n",
            "iteration: 66050 loss: 0.0077 lr: 0.02\n",
            "iteration: 66060 loss: 0.0110 lr: 0.02\n",
            "iteration: 66070 loss: 0.0090 lr: 0.02\n",
            "iteration: 66080 loss: 0.0155 lr: 0.02\n",
            "iteration: 66090 loss: 0.0128 lr: 0.02\n",
            "iteration: 66100 loss: 0.0131 lr: 0.02\n",
            "iteration: 66110 loss: 0.0177 lr: 0.02\n",
            "iteration: 66120 loss: 0.0099 lr: 0.02\n",
            "iteration: 66130 loss: 0.0118 lr: 0.02\n",
            "iteration: 66140 loss: 0.0138 lr: 0.02\n",
            "iteration: 66150 loss: 0.0112 lr: 0.02\n",
            "iteration: 66160 loss: 0.0098 lr: 0.02\n",
            "iteration: 66170 loss: 0.0134 lr: 0.02\n",
            "iteration: 66180 loss: 0.0107 lr: 0.02\n",
            "iteration: 66190 loss: 0.0144 lr: 0.02\n",
            "iteration: 66200 loss: 0.0096 lr: 0.02\n",
            "iteration: 66210 loss: 0.0127 lr: 0.02\n",
            "iteration: 66220 loss: 0.0165 lr: 0.02\n",
            "iteration: 66230 loss: 0.0119 lr: 0.02\n",
            "iteration: 66240 loss: 0.0160 lr: 0.02\n",
            "iteration: 66250 loss: 0.0120 lr: 0.02\n",
            "iteration: 66260 loss: 0.0112 lr: 0.02\n",
            "iteration: 66270 loss: 0.0151 lr: 0.02\n",
            "iteration: 66280 loss: 0.0118 lr: 0.02\n",
            "iteration: 66290 loss: 0.0144 lr: 0.02\n",
            "iteration: 66300 loss: 0.0087 lr: 0.02\n",
            "iteration: 66310 loss: 0.0115 lr: 0.02\n",
            "iteration: 66320 loss: 0.0091 lr: 0.02\n",
            "iteration: 66330 loss: 0.0120 lr: 0.02\n",
            "iteration: 66340 loss: 0.0135 lr: 0.02\n",
            "iteration: 66350 loss: 0.0155 lr: 0.02\n",
            "iteration: 66360 loss: 0.0150 lr: 0.02\n",
            "iteration: 66370 loss: 0.0141 lr: 0.02\n",
            "iteration: 66380 loss: 0.0127 lr: 0.02\n",
            "iteration: 66390 loss: 0.0081 lr: 0.02\n",
            "iteration: 66400 loss: 0.0135 lr: 0.02\n",
            "iteration: 66410 loss: 0.0168 lr: 0.02\n",
            "iteration: 66420 loss: 0.0105 lr: 0.02\n",
            "iteration: 66430 loss: 0.0124 lr: 0.02\n",
            "iteration: 66440 loss: 0.0079 lr: 0.02\n",
            "iteration: 66450 loss: 0.0082 lr: 0.02\n",
            "iteration: 66460 loss: 0.0100 lr: 0.02\n",
            "iteration: 66470 loss: 0.0134 lr: 0.02\n",
            "iteration: 66480 loss: 0.0082 lr: 0.02\n",
            "iteration: 66490 loss: 0.0099 lr: 0.02\n",
            "iteration: 66500 loss: 0.0101 lr: 0.02\n",
            "iteration: 66510 loss: 0.0113 lr: 0.02\n",
            "iteration: 66520 loss: 0.0117 lr: 0.02\n",
            "iteration: 66530 loss: 0.0125 lr: 0.02\n",
            "iteration: 66540 loss: 0.0108 lr: 0.02\n",
            "iteration: 66550 loss: 0.0118 lr: 0.02\n",
            "iteration: 66560 loss: 0.0121 lr: 0.02\n",
            "iteration: 66570 loss: 0.0136 lr: 0.02\n",
            "iteration: 66580 loss: 0.0130 lr: 0.02\n",
            "iteration: 66590 loss: 0.0125 lr: 0.02\n",
            "iteration: 66600 loss: 0.0130 lr: 0.02\n",
            "iteration: 66610 loss: 0.0145 lr: 0.02\n",
            "iteration: 66620 loss: 0.0127 lr: 0.02\n",
            "iteration: 66630 loss: 0.0172 lr: 0.02\n",
            "iteration: 66640 loss: 0.0102 lr: 0.02\n",
            "iteration: 66650 loss: 0.0109 lr: 0.02\n",
            "iteration: 66660 loss: 0.0124 lr: 0.02\n",
            "iteration: 66670 loss: 0.0102 lr: 0.02\n",
            "iteration: 66680 loss: 0.0090 lr: 0.02\n",
            "iteration: 66690 loss: 0.0094 lr: 0.02\n",
            "iteration: 66700 loss: 0.0151 lr: 0.02\n",
            "iteration: 66710 loss: 0.0092 lr: 0.02\n",
            "iteration: 66720 loss: 0.0116 lr: 0.02\n",
            "iteration: 66730 loss: 0.0098 lr: 0.02\n",
            "iteration: 66740 loss: 0.0145 lr: 0.02\n",
            "iteration: 66750 loss: 0.0139 lr: 0.02\n",
            "iteration: 66760 loss: 0.0112 lr: 0.02\n",
            "iteration: 66770 loss: 0.0137 lr: 0.02\n",
            "iteration: 66780 loss: 0.0098 lr: 0.02\n",
            "iteration: 66790 loss: 0.0080 lr: 0.02\n",
            "iteration: 66800 loss: 0.0116 lr: 0.02\n",
            "iteration: 66810 loss: 0.0102 lr: 0.02\n",
            "iteration: 66820 loss: 0.0120 lr: 0.02\n",
            "iteration: 66830 loss: 0.0100 lr: 0.02\n",
            "iteration: 66840 loss: 0.0096 lr: 0.02\n",
            "iteration: 66850 loss: 0.0120 lr: 0.02\n",
            "iteration: 66860 loss: 0.0132 lr: 0.02\n",
            "iteration: 66870 loss: 0.0136 lr: 0.02\n",
            "iteration: 66880 loss: 0.0146 lr: 0.02\n",
            "iteration: 66890 loss: 0.0130 lr: 0.02\n",
            "iteration: 66900 loss: 0.0131 lr: 0.02\n",
            "iteration: 66910 loss: 0.0125 lr: 0.02\n",
            "iteration: 66920 loss: 0.0164 lr: 0.02\n",
            "iteration: 66930 loss: 0.0108 lr: 0.02\n",
            "iteration: 66940 loss: 0.0139 lr: 0.02\n",
            "iteration: 66950 loss: 0.0111 lr: 0.02\n",
            "iteration: 66960 loss: 0.0194 lr: 0.02\n",
            "iteration: 66970 loss: 0.0145 lr: 0.02\n",
            "iteration: 66980 loss: 0.0117 lr: 0.02\n",
            "iteration: 66990 loss: 0.0123 lr: 0.02\n",
            "iteration: 67000 loss: 0.0099 lr: 0.02\n",
            "iteration: 67010 loss: 0.0138 lr: 0.02\n",
            "iteration: 67020 loss: 0.0135 lr: 0.02\n",
            "iteration: 67030 loss: 0.0142 lr: 0.02\n",
            "iteration: 67040 loss: 0.0127 lr: 0.02\n",
            "iteration: 67050 loss: 0.0122 lr: 0.02\n",
            "iteration: 67060 loss: 0.0101 lr: 0.02\n",
            "iteration: 67070 loss: 0.0115 lr: 0.02\n",
            "iteration: 67080 loss: 0.0104 lr: 0.02\n",
            "iteration: 67090 loss: 0.0097 lr: 0.02\n",
            "iteration: 67100 loss: 0.0108 lr: 0.02\n",
            "iteration: 67110 loss: 0.0116 lr: 0.02\n",
            "iteration: 67120 loss: 0.0111 lr: 0.02\n",
            "iteration: 67130 loss: 0.0214 lr: 0.02\n",
            "iteration: 67140 loss: 0.0161 lr: 0.02\n",
            "iteration: 67150 loss: 0.0131 lr: 0.02\n",
            "iteration: 67160 loss: 0.0114 lr: 0.02\n",
            "iteration: 67170 loss: 0.0116 lr: 0.02\n",
            "iteration: 67180 loss: 0.0179 lr: 0.02\n",
            "iteration: 67190 loss: 0.0168 lr: 0.02\n",
            "iteration: 67200 loss: 0.0142 lr: 0.02\n",
            "iteration: 67210 loss: 0.0081 lr: 0.02\n",
            "iteration: 67220 loss: 0.0118 lr: 0.02\n",
            "iteration: 67230 loss: 0.0126 lr: 0.02\n",
            "iteration: 67240 loss: 0.0086 lr: 0.02\n",
            "iteration: 67250 loss: 0.0222 lr: 0.02\n",
            "iteration: 67260 loss: 0.0137 lr: 0.02\n",
            "iteration: 67270 loss: 0.0131 lr: 0.02\n",
            "iteration: 67280 loss: 0.0123 lr: 0.02\n",
            "iteration: 67290 loss: 0.0092 lr: 0.02\n",
            "iteration: 67300 loss: 0.0155 lr: 0.02\n",
            "iteration: 67310 loss: 0.0122 lr: 0.02\n",
            "iteration: 67320 loss: 0.0088 lr: 0.02\n",
            "iteration: 67330 loss: 0.0127 lr: 0.02\n",
            "iteration: 67340 loss: 0.0132 lr: 0.02\n",
            "iteration: 67350 loss: 0.0147 lr: 0.02\n",
            "iteration: 67360 loss: 0.0146 lr: 0.02\n",
            "iteration: 67370 loss: 0.0084 lr: 0.02\n",
            "iteration: 67380 loss: 0.0098 lr: 0.02\n",
            "iteration: 67390 loss: 0.0111 lr: 0.02\n",
            "iteration: 67400 loss: 0.0135 lr: 0.02\n",
            "iteration: 67410 loss: 0.0142 lr: 0.02\n",
            "iteration: 67420 loss: 0.0180 lr: 0.02\n",
            "iteration: 67430 loss: 0.0115 lr: 0.02\n",
            "iteration: 67440 loss: 0.0248 lr: 0.02\n",
            "iteration: 67450 loss: 0.0097 lr: 0.02\n",
            "iteration: 67460 loss: 0.0104 lr: 0.02\n",
            "iteration: 67470 loss: 0.0152 lr: 0.02\n",
            "iteration: 67480 loss: 0.0170 lr: 0.02\n",
            "iteration: 67490 loss: 0.0161 lr: 0.02\n",
            "iteration: 67500 loss: 0.0114 lr: 0.02\n",
            "iteration: 67510 loss: 0.0106 lr: 0.02\n",
            "iteration: 67520 loss: 0.0127 lr: 0.02\n",
            "iteration: 67530 loss: 0.0106 lr: 0.02\n",
            "iteration: 67540 loss: 0.0149 lr: 0.02\n",
            "iteration: 67550 loss: 0.0144 lr: 0.02\n",
            "iteration: 67560 loss: 0.0153 lr: 0.02\n",
            "iteration: 67570 loss: 0.0105 lr: 0.02\n",
            "iteration: 67580 loss: 0.0159 lr: 0.02\n",
            "iteration: 67590 loss: 0.0139 lr: 0.02\n",
            "iteration: 67600 loss: 0.0098 lr: 0.02\n",
            "iteration: 67610 loss: 0.0165 lr: 0.02\n",
            "iteration: 67620 loss: 0.0107 lr: 0.02\n",
            "iteration: 67630 loss: 0.0132 lr: 0.02\n",
            "iteration: 67640 loss: 0.0076 lr: 0.02\n",
            "iteration: 67650 loss: 0.0184 lr: 0.02\n",
            "iteration: 67660 loss: 0.0115 lr: 0.02\n",
            "iteration: 67670 loss: 0.0117 lr: 0.02\n",
            "iteration: 67680 loss: 0.0087 lr: 0.02\n",
            "iteration: 67690 loss: 0.0116 lr: 0.02\n",
            "iteration: 67700 loss: 0.0082 lr: 0.02\n",
            "iteration: 67710 loss: 0.0141 lr: 0.02\n",
            "iteration: 67720 loss: 0.0097 lr: 0.02\n",
            "iteration: 67730 loss: 0.0137 lr: 0.02\n",
            "iteration: 67740 loss: 0.0154 lr: 0.02\n",
            "iteration: 67750 loss: 0.0083 lr: 0.02\n",
            "iteration: 67760 loss: 0.0094 lr: 0.02\n",
            "iteration: 67770 loss: 0.0088 lr: 0.02\n",
            "iteration: 67780 loss: 0.0087 lr: 0.02\n",
            "iteration: 67790 loss: 0.0085 lr: 0.02\n",
            "iteration: 67800 loss: 0.0102 lr: 0.02\n",
            "iteration: 67810 loss: 0.0119 lr: 0.02\n",
            "iteration: 67820 loss: 0.0092 lr: 0.02\n",
            "iteration: 67830 loss: 0.0116 lr: 0.02\n",
            "iteration: 67840 loss: 0.0097 lr: 0.02\n",
            "iteration: 67850 loss: 0.0113 lr: 0.02\n",
            "iteration: 67860 loss: 0.0154 lr: 0.02\n",
            "iteration: 67870 loss: 0.0109 lr: 0.02\n",
            "iteration: 67880 loss: 0.0133 lr: 0.02\n",
            "iteration: 67890 loss: 0.0109 lr: 0.02\n",
            "iteration: 67900 loss: 0.0128 lr: 0.02\n",
            "iteration: 67910 loss: 0.0137 lr: 0.02\n",
            "iteration: 67920 loss: 0.0151 lr: 0.02\n",
            "iteration: 67930 loss: 0.0124 lr: 0.02\n",
            "iteration: 67940 loss: 0.0106 lr: 0.02\n",
            "iteration: 67950 loss: 0.0115 lr: 0.02\n",
            "iteration: 67960 loss: 0.0100 lr: 0.02\n",
            "iteration: 67970 loss: 0.0152 lr: 0.02\n",
            "iteration: 67980 loss: 0.0115 lr: 0.02\n",
            "iteration: 67990 loss: 0.0090 lr: 0.02\n",
            "iteration: 68000 loss: 0.0106 lr: 0.02\n",
            "iteration: 68010 loss: 0.0114 lr: 0.02\n",
            "iteration: 68020 loss: 0.0128 lr: 0.02\n",
            "iteration: 68030 loss: 0.0165 lr: 0.02\n",
            "iteration: 68040 loss: 0.0143 lr: 0.02\n",
            "iteration: 68050 loss: 0.0134 lr: 0.02\n",
            "iteration: 68060 loss: 0.0082 lr: 0.02\n",
            "iteration: 68070 loss: 0.0095 lr: 0.02\n",
            "iteration: 68080 loss: 0.0104 lr: 0.02\n",
            "iteration: 68090 loss: 0.0129 lr: 0.02\n",
            "iteration: 68100 loss: 0.0099 lr: 0.02\n",
            "iteration: 68110 loss: 0.0090 lr: 0.02\n",
            "iteration: 68120 loss: 0.0089 lr: 0.02\n",
            "iteration: 68130 loss: 0.0192 lr: 0.02\n",
            "iteration: 68140 loss: 0.0141 lr: 0.02\n",
            "iteration: 68150 loss: 0.0148 lr: 0.02\n",
            "iteration: 68160 loss: 0.0102 lr: 0.02\n",
            "iteration: 68170 loss: 0.0157 lr: 0.02\n",
            "iteration: 68180 loss: 0.0119 lr: 0.02\n",
            "iteration: 68190 loss: 0.0126 lr: 0.02\n",
            "iteration: 68200 loss: 0.0105 lr: 0.02\n",
            "iteration: 68210 loss: 0.0119 lr: 0.02\n",
            "iteration: 68220 loss: 0.0089 lr: 0.02\n",
            "iteration: 68230 loss: 0.0076 lr: 0.02\n",
            "iteration: 68240 loss: 0.0112 lr: 0.02\n",
            "iteration: 68250 loss: 0.0141 lr: 0.02\n",
            "iteration: 68260 loss: 0.0131 lr: 0.02\n",
            "iteration: 68270 loss: 0.0129 lr: 0.02\n",
            "iteration: 68280 loss: 0.0152 lr: 0.02\n",
            "iteration: 68290 loss: 0.0125 lr: 0.02\n",
            "iteration: 68300 loss: 0.0127 lr: 0.02\n",
            "iteration: 68310 loss: 0.0142 lr: 0.02\n",
            "iteration: 68320 loss: 0.0142 lr: 0.02\n",
            "iteration: 68330 loss: 0.0103 lr: 0.02\n",
            "iteration: 68340 loss: 0.0143 lr: 0.02\n",
            "iteration: 68350 loss: 0.0137 lr: 0.02\n",
            "iteration: 68360 loss: 0.0106 lr: 0.02\n",
            "iteration: 68370 loss: 0.0103 lr: 0.02\n",
            "iteration: 68380 loss: 0.0132 lr: 0.02\n",
            "iteration: 68390 loss: 0.0109 lr: 0.02\n",
            "iteration: 68400 loss: 0.0113 lr: 0.02\n",
            "iteration: 68410 loss: 0.0117 lr: 0.02\n",
            "iteration: 68420 loss: 0.0099 lr: 0.02\n",
            "iteration: 68430 loss: 0.0103 lr: 0.02\n",
            "iteration: 68440 loss: 0.0130 lr: 0.02\n",
            "iteration: 68450 loss: 0.0133 lr: 0.02\n",
            "iteration: 68460 loss: 0.0121 lr: 0.02\n",
            "iteration: 68470 loss: 0.0143 lr: 0.02\n",
            "iteration: 68480 loss: 0.0104 lr: 0.02\n",
            "iteration: 68490 loss: 0.0118 lr: 0.02\n",
            "iteration: 68500 loss: 0.0112 lr: 0.02\n",
            "iteration: 68510 loss: 0.0143 lr: 0.02\n",
            "iteration: 68520 loss: 0.0121 lr: 0.02\n",
            "iteration: 68530 loss: 0.0099 lr: 0.02\n",
            "iteration: 68540 loss: 0.0104 lr: 0.02\n",
            "iteration: 68550 loss: 0.0117 lr: 0.02\n",
            "iteration: 68560 loss: 0.0105 lr: 0.02\n",
            "iteration: 68570 loss: 0.0166 lr: 0.02\n",
            "iteration: 68580 loss: 0.0137 lr: 0.02\n",
            "iteration: 68590 loss: 0.0132 lr: 0.02\n",
            "iteration: 68600 loss: 0.0158 lr: 0.02\n",
            "iteration: 68610 loss: 0.0153 lr: 0.02\n",
            "iteration: 68620 loss: 0.0158 lr: 0.02\n",
            "iteration: 68630 loss: 0.0111 lr: 0.02\n",
            "iteration: 68640 loss: 0.0095 lr: 0.02\n",
            "iteration: 68650 loss: 0.0095 lr: 0.02\n",
            "iteration: 68660 loss: 0.0140 lr: 0.02\n",
            "iteration: 68670 loss: 0.0109 lr: 0.02\n",
            "iteration: 68680 loss: 0.0109 lr: 0.02\n",
            "iteration: 68690 loss: 0.0124 lr: 0.02\n",
            "iteration: 68700 loss: 0.0133 lr: 0.02\n",
            "iteration: 68710 loss: 0.0116 lr: 0.02\n",
            "iteration: 68720 loss: 0.0096 lr: 0.02\n",
            "iteration: 68730 loss: 0.0116 lr: 0.02\n",
            "iteration: 68740 loss: 0.0123 lr: 0.02\n",
            "iteration: 68750 loss: 0.0153 lr: 0.02\n",
            "iteration: 68760 loss: 0.0129 lr: 0.02\n",
            "iteration: 68770 loss: 0.0096 lr: 0.02\n",
            "iteration: 68780 loss: 0.0169 lr: 0.02\n",
            "iteration: 68790 loss: 0.0147 lr: 0.02\n",
            "iteration: 68800 loss: 0.0110 lr: 0.02\n",
            "iteration: 68810 loss: 0.0141 lr: 0.02\n",
            "iteration: 68820 loss: 0.0191 lr: 0.02\n",
            "iteration: 68830 loss: 0.0186 lr: 0.02\n",
            "iteration: 68840 loss: 0.0161 lr: 0.02\n",
            "iteration: 68850 loss: 0.0114 lr: 0.02\n",
            "iteration: 68860 loss: 0.0112 lr: 0.02\n",
            "iteration: 68870 loss: 0.0120 lr: 0.02\n",
            "iteration: 68880 loss: 0.0132 lr: 0.02\n",
            "iteration: 68890 loss: 0.0149 lr: 0.02\n",
            "iteration: 68900 loss: 0.0124 lr: 0.02\n",
            "iteration: 68910 loss: 0.0119 lr: 0.02\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-9a4759324dea>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#more info and there are more things you can set: https://github.com/DeepLabCut/DeepLabCut/wiki/DOCSTRINGS#train_network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_config_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplayiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msaveiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#this will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end (default, 1.03M iterations).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Selecting single-animal trainer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             train(\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposeconfigfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mdisplayiters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deeplabcut/pose_estimation_tensorflow/core/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config_yaml, displayiters, saveiters, maxiters, max_to_keep, keepdeconvweights, allow_growth)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mlr_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcurrent_lr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         [_, loss_val, summary] = sess.run(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_summaries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    969\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    970\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1192\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1372\u001b[0m                            run_metadata)\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1359\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1362\u001b[0m                                       target_list, run_metadata)\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1452\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1453\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1454\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1455\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m                                             run_metadata)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#let's also change the display and save_iters just in case Colab takes away the GPU... \n",
        "#if that happens, you can reload from a saved point. Typically, you want to train to 200,000 + iterations.\n",
        "#more info and there are more things you can set: https://github.com/DeepLabCut/DeepLabCut/wiki/DOCSTRINGS#train_network\n",
        "\n",
        "deeplabcut.train_network(path_config_file, shuffle=1, displayiters=10,saveiters=500)\n",
        "\n",
        "#this will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end (default, 1.03M iterations). \n",
        "#Whichever you chose, you will see what looks like an error message, but it's not an error - don't worry...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiDwIVf5-3H_"
      },
      "source": [
        "**When you hit \"STOP\" you will get a KeyInterrupt \"error\"! No worries! :)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZygsb2DoEJc"
      },
      "source": [
        "## Start evaluating:\n",
        "This function evaluates a trained model for a specific shuffle/shuffles at a particular state or all the states on the data set (images)\n",
        "and stores the results as .csv file in a subdirectory under **evaluation-results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv4zlbrnoEJg"
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook\n",
        "deeplabcut.evaluate_network(path_config_file,plotting=True)\n",
        "\n",
        "# Here you want to see a low pixel error! Of course, it can only be as good as the labeler, \n",
        "#so be sure your labels are good! (And you have trained enough ;)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaLBl3TQtrfB"
      },
      "source": [
        "## There is an optional refinement step you can do outside of Colab:\n",
        "- if your pixel errors are not low enough, please check out the protocol guide on how to refine your network!\n",
        "- You will need to adjust the labels **outside of Colab!** We recommend coming back to train and analyze videos... \n",
        "- Please see the repo and protocol instructions on how to refine your data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVFLSKKfoEJk"
      },
      "source": [
        "## Start Analyzing videos: \n",
        "This function analyzes the new video. The user can choose the best model from the evaluation results and specify the correct snapshot index for the variable **snapshotindex** in the **config.yaml** file. Otherwise, by default the most recent snapshot is used to analyse the video.\n",
        "\n",
        "The results are stored in hd5 file in the same directory where the video resides. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_LZiS_0oEJl"
      },
      "outputs": [],
      "source": [
        "deeplabcut.analyze_videos(path_config_file,videofile_path, videotype=VideoType)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GTiuJESoEKH"
      },
      "source": [
        "## Plot the trajectories of the analyzed videos:\n",
        "This function plots the trajectories of all the body parts across the entire video. Each body part is identified by a unique color."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX21zZbXoEKJ"
      },
      "outputs": [],
      "source": [
        "deeplabcut.plot_trajectories(path_config_file,videofile_path, videotype=VideoType)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqaCw15v8EmB"
      },
      "source": [
        "Now you can look at the plot-poses file and check the \"plot-likelihood.png\" might want to change the \"p-cutoff\" in the config.yaml file so that you have only high confidnece points plotted in the video. i.e. ~0.8 or 0.9. The current default is 0.4. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCrUvQIvoEKD"
      },
      "source": [
        "## Create labeled video:\n",
        "This function is for visualiztion purpose and can be used to create a video in .mp4 format with labels predicted by the network. This video is saved in the same directory where the original video resides. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aDF7Q7KoEKE"
      },
      "outputs": [],
      "source": [
        "deeplabcut.create_labeled_video(path_config_file,videofile_path, videotype=VideoType)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of latest_Colab_TrainNetwork_VideoAnalysis.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('dlc')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "70cad038f2bddb56e8a0ba66c48b76ebce20579892bf83e71733a81977e3ceea"
      }
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}